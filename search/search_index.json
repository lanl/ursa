{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Universal Research and Scientific Agent (URSA)","text":"<p>The flexible agentic workflow for accelerating scientific tasks. Composes information flow between agents for planning, code writing and execution, and online research to solve complex problems.</p>"},{"location":"Metrics-Guide/","title":"URSA Metrics CLI \u2014 Plotting &amp; Aggregation Guide","text":"<p>This guide covers how to use <code>metrics_cli.py</code> to generate per-run, per-thread, and cross-thread (SUPER) charts from Telemetry JSON. It includes the new model- and agent-level aggregations and the interactive timeline.</p>"},{"location":"Metrics-Guide/#quickstart","title":"Quickstart","text":"<pre><code># Generate thread-level + per-run charts for every thread under a directory\npython ursa/scripts/metrics_cli.py --dir /path/to/metrics --chart all\n</code></pre> <pre><code># Walk all subdirectories, run `all` in each, then build a SUPER rollup at the root\npython ursa/scripts/metrics_cli.py --dir /path/to/workspaces --chart all-recursive\n</code></pre> <pre><code># Single JSON: make a time lollipop, token totals, KDE, or tokens/sec\npython ursa/scripts/metrics_cli.py path/to/agent_metrics.json --chart lollipop\npython ursa/scripts/metrics_cli.py path/to/agent_metrics.json --chart tokens-bar\npython ursa/scripts/metrics_cli.py path/to/agent_metrics.json --chart tokens-kde\npython ursa/scripts/metrics_cli.py path/to/agent_metrics.json --chart tokens-rate\n</code></pre>"},{"location":"Metrics-Guide/#what-the-cli-reads-what-it-makes","title":"What the CLI reads &amp; what it makes","text":"<p>Input (per file): - <code>context.agent</code>, <code>context.thread_id</code>, <code>context.run_id</code>, <code>context.started_at</code>, <code>context.ended_at</code> - <code>tables.llm[]</code>, <code>tables.tool[]</code>, <code>tables.runnable[]</code> - <code>llm_events[][].metrics.usage_rollup</code> (for token counts &amp; samples)</p> <p>Outputs (PNG/HTML), depending on mode: - Time lollipop: <code>*_lollipop.png</code> (or <code>thread_&lt;id&gt;_lollipop.png</code>, <code>super_lollipop.png</code>) - Token totals bar: <code>*_tokens_bar.png</code> - Token KDE overlay: <code>*_tokens_kde.png</code> - Tokens per second (two baselines): <code>*_tokens_rate.png</code> - Interactive timeline: <code>thread_&lt;id&gt;_timeline.html</code> - SUPER by-model: <code>super_tokens_bar_by_model.png</code>, <code>super_tokens_rate_by_model.png</code> - Agent rollups (thread or SUPER):   - tokens: <code>thread_&lt;id&gt;_agents_tokens.png</code>, <code>super_agents_tokens.png</code>   - tokens/sec: <code>thread_&lt;id&gt;_agents_tps.png</code>, <code>super_agents_tps.png</code></p> <p>Filenames for per-run charts are derived from the JSON path: <code>path/to/run.json</code> \u2192 <code>path/to/run_breakdown_&lt;chart&gt;.png</code>.</p>"},{"location":"Metrics-Guide/#modes","title":"Modes","text":""},{"location":"Metrics-Guide/#1-single-json-targeted-charts","title":"1) Single JSON (targeted charts)","text":"<pre><code>python ursa/scripts/metrics_cli.py path/to/run.json --chart lollipop\npython ursa/scripts/metrics_cli.py path/to/run.json --chart tokens-bar\npython ursa/scripts/metrics_cli.py path/to/run.json --chart tokens-kde\npython ursa/scripts/metrics_cli.py path/to/run.json --chart tokens-rate\n</code></pre> <p>Use <code>--title</code> and <code>--out</code> to customize:</p> <pre><code>python ursa/scripts/metrics_cli.py run.json \\\n  --chart tokens-rate \\\n  --title \"Model TPS (build #814)\" \\\n  --out out/my_rate.png\n</code></pre>"},{"location":"Metrics-Guide/#2-thread-level-aggregate-all-runs-of-one-thread_id","title":"2) Thread-level (aggregate all runs of one <code>thread_id</code>)","text":"<p>List threads the CLI can see in a directory:</p> <pre><code>python ursa/scripts/metrics_cli.py --dir /metrics --list-threads\n</code></pre> <p>Generate a specific thread\u2019s charts:</p> <pre><code># Time lollipop\npython ursa/scripts/metrics_cli.py --dir /metrics --chart thread-lollipop --thread &lt;thread_id&gt;\n\n# Token totals / KDE / TPS\npython ursa/scripts/metrics_cli.py --dir /metrics --chart thread-tokens-bar  --thread &lt;thread_id&gt;\npython ursa/scripts/metrics_cli.py --dir /metrics --chart thread-tokens-kde  --thread &lt;thread_id&gt; --log-x\npython ursa/scripts/metrics_cli.py --dir /metrics --chart thread-tokens-rate --thread &lt;thread_id&gt;\n\n# Interactive timeline (HTML); y-axis grouped by agent (default) or one row per run\npython ursa/scripts/metrics_cli.py --dir /metrics --chart timeline-html --thread &lt;thread_id&gt; --group-by agent\npython ursa/scripts/metrics_cli.py --dir /metrics --chart timeline-html --thread &lt;thread_id&gt; --group-by run\n</code></pre> <p>Agent rollups for a thread:</p> <pre><code># Tokens stacked by agent\npython ursa/scripts/metrics_cli.py --dir /metrics --chart thread-agents-tokens --thread &lt;thread_id&gt;\n\n# Tokens/sec by agent (two baselines)\npython ursa/scripts/metrics_cli.py --dir /metrics --chart thread-agents-tps --thread &lt;thread_id&gt;\n</code></pre> <p>Thread-level TPS uses two denominators: - per LLM-sec (sum): sum of <code>tables.llm[].total_s</code> across all runs in the thread - per thread-sec: <code>max(ended_at) - min(started_at)</code> across the thread\u2019s runs</p>"},{"location":"Metrics-Guide/#3-all-non-recursive-for-a-directory","title":"3) \u201cAll\u201d (non-recursive) for a directory","text":"<p>Run all thread-level charts and per-run charts inside a directory:</p> <pre><code>python ursa/scripts/metrics_cli.py --dir /metrics --chart all\n</code></pre> <p>What it produces: - For each thread: lollipop, tokens-bar, tokens-kde, tokens-rate, timeline HTML - For each JSON: lollipop, tokens-bar, tokens-kde, tokens-rate</p> <p>(Use <code>--log-x</code> to log-scale the lollipops &amp; KDE.)</p>"},{"location":"Metrics-Guide/#4-all-recursive-super-cross-thread-rollups","title":"4) \u201cAll-recursive\u201d + SUPER (cross-thread rollups)","text":"<p>Walk subdirectories, run <code>all</code> in each, then build a rollup at the root:</p> <pre><code>python ursa/scripts/metrics_cli.py --dir /workspaces --chart all-recursive\n</code></pre> <p>SUPER artifacts at <code>--dir</code>: - <code>super_lollipop.png</code> (time by component across all threads) - <code>super_tokens_bar.png</code> &amp; <code>super_tokens_kde.png</code> (totals &amp; distribution) - <code>super_tokens_rate.png</code> (TPS using \u03a3 thread windows &amp; \u03a3 LLM-sec) - By model: <code>super_tokens_bar_by_model.png</code>, <code>super_tokens_rate_by_model.png</code> - By agent: <code>super_agents_tokens.png</code>, <code>super_agents_tps.png</code> (Use the explicit charts below if you want just the agent rollups without re-running everything.)</p> <p>Build only the SUPER agent rollups for a directory you\u2019ve already processed:</p> <pre><code>python ursa/scripts/metrics_cli.py --dir /workspaces --chart super-agents-tokens\npython ursa/scripts/metrics_cli.py --dir /workspaces --chart super-agents-tps\n</code></pre> <p>In SUPER charts, the bottom footer does not show a single start\u2192end \u201cwindow\u201d, since different threads can overlap or run on different machines. Where relevant, the footer shows \u03a3 thread windows and \u03a3 LLM-active seconds instead.</p>"},{"location":"Metrics-Guide/#understanding-the-denominators-for-tps","title":"Understanding the denominators (for TPS)","text":"<ul> <li> <p>LLM-active seconds (sum)   From <code>tables.llm[].total_s</code> (or via event intervals for single runs). If multiple LLM calls overlap, the sum can exceed the wall window; this indicates parallelism.</p> </li> <li> <p>Thread window seconds   For a thread: <code>max(ended_at) - min(started_at)</code>.   For SUPER: sum of per-thread windows (not a single global wall window).</p> </li> </ul> <p>The TPS chart shows both denominators side-by-side to make parallelism visible.</p>"},{"location":"Metrics-Guide/#useful-options","title":"Useful options","text":"Flag Meaning Notes <code>--dir PATH</code> Directory to scan for metrics JSONs. Required for <code>all</code>, thread-level, and SUPER modes. <code>--chart</code> Which artifact(s) to generate. See lists above; default is <code>all</code>. <code>--thread ID</code> Limit to one thread for thread-level charts. Use with <code>--chart thread-*</code> or <code>timeline-html</code>. <code>--list-threads</code> Print discovered threads in <code>--dir</code>. Great to copy/paste a <code>--thread</code> ID. <code>--group-llm</code> Group all LLM rows into <code>llm:total</code> in time charts. Affects lollipop/pie/bar (time). <code>--group-by {agent,run}</code> Timeline y-axis grouping. <code>agent</code> is compact; <code>run</code> gives one lane per run. <code>--log-x</code> Log-scale for lollipop &amp; KDE. Helpful when components vary by orders of magnitude. <code>--min-label-pct FLOAT</code> Hide dot labels below this percent in lollipop. Default <code>0.0</code> (show all). <code>--title TEXT</code> Custom chart title. For targeted modes. <code>--out PATH</code> Custom output file path. For targeted modes. <code>--check</code> Print attribution totals for a single JSON and exit. Verifies <code>llm+tool+other \u2248 graph:graph</code>. <code>--epsilon FLOAT</code> Tolerance for <code>--check</code>. Default <code>0.050</code> seconds."},{"location":"Metrics-Guide/#examples-copypaste","title":"Examples (copy/paste)","text":"<pre><code># See what threads are in a directory\npython ursa/scripts/metrics_cli.py --dir ./workspaces/myrun --list-threads\n</code></pre> <pre><code># Thread-level bundle for a single thread (PNG + HTML)\npython ursa/scripts/metrics_cli.py --dir ./workspaces/myrun \\\n  --chart thread-tokens-rate --thread modsim_predict_final_mild-orange\npython ursa/scripts/metrics_cli.py --dir ./workspaces/myrun \\\n  --chart timeline-html --thread modsim_predict_final_mild-orange --group-by agent\n</code></pre> <pre><code># Agent breakdowns for one thread (stacked tokens + TPS)\npython ursa/scripts/metrics_cli.py --dir ./workspaces/myrun \\\n  --chart thread-agents-tokens --thread &lt;thread_id&gt;\npython ursa/scripts/metrics_cli.py --dir ./workspaces/myrun \\\n  --chart thread-agents-tps --thread &lt;thread_id&gt;\n</code></pre> <pre><code># Directory-wide (non-recursive) batch\npython ursa/scripts/metrics_cli.py --dir ./workspaces/myrun --chart all\n</code></pre> <pre><code># Recursive batch + SUPER rollups at the root\npython ursa/scripts/metrics_cli.py --dir ./workspaces --chart all-recursive\n</code></pre> <pre><code># Only the SUPER agent charts (when you already have per-thread results)\npython ursa/scripts/metrics_cli.py --dir ./workspaces --chart super-agents-tokens\npython ursa/scripts/metrics_cli.py --dir ./workspaces --chart super-agents-tps\n</code></pre> <pre><code># Single JSON \u2013 compare denominators in TPS\npython ursa/scripts/metrics_cli.py ./workspaces/t1/run_0007.json --chart tokens-rate --title \"step 7 TPS\"\n</code></pre>"},{"location":"Metrics-Guide/#output-naming-where-to-find-things","title":"Output naming &amp; where to find things","text":"<p>Per run (single JSON):</p> <pre><code>&lt;path&gt;/run_breakdown_lollipop.png\n&lt;path&gt;/run_breakdown_tokens_bar.png\n&lt;path&gt;/run_breakdown_tokens_kde.png\n&lt;path&gt;/run_breakdown_tokens_rate.png\n</code></pre> <p>Thread-level (in --dir):</p> <pre><code>thread_&lt;thread_id&gt;_lollipop.png\nthread_&lt;thread_id&gt;_tokens_bar.png\nthread_&lt;thread_id&gt;_tokens_kde.png\nthread_&lt;thread_id&gt;_tokens_rate.png\nthread_&lt;thread_id&gt;_timeline.html\nthread_&lt;thread_id&gt;_agents_tokens.png\nthread_&lt;thread_id&gt;_agents_tps.png\n</code></pre> <p>SUPER (at the root --dir for all-recursive):</p> <pre><code>super_lollipop.png\nsuper_tokens_bar.png\nsuper_tokens_kde.png\nsuper_tokens_rate.png\nsuper_tokens_bar_by_model.png\nsuper_tokens_rate_by_model.png\nsuper_agents_tokens.png\nsuper_agents_tps.png\n</code></pre>"},{"location":"Metrics-Guide/#tips","title":"Tips","text":"<ul> <li>Use <code>--log-x</code> for lollipop &amp; KDE when a few components dominate.</li> <li>Use <code>--group-llm</code> to collapse many LLM rows into a single \u201cllm:total\u201d bar for readability.</li> <li>The interactive timeline (<code>timeline-html</code>) is ideal for human inspection of overlaps; set <code>--group-by run</code> to see one lane per run.</li> <li>In SUPER TPS, the footer reports \u03a3 thread windows and \u03a3 LLM-active seconds rather than a single start\u2192end time.</li> </ul>"},{"location":"Metrics-Guide/#troubleshooting","title":"Troubleshooting","text":"<p>No thread IDs found - Ensure <code>--dir</code> points at a directory containing Telemetry JSON files. - JSON must include <code>context.thread_id</code>, <code>context.agent</code>, <code>context.run_id</code>, <code>context.started_at</code>, <code>context.ended_at</code>.</p> <p>Tokens charts look empty (all zeros) - Check that <code>llm_events[].metrics.usage_rollup</code> has <code>input_tokens</code> / <code>output_tokens</code> (or <code>prompt_tokens</code> / <code>completion_tokens</code>) fields. - If a provider omits <code>total_tokens</code>, the CLI computes <code>max(total, input+output)</code>.</p> <p>TPS \u201cLLM sum exceeds window \u2192 parallel LLM work\u201d - Expected when multiple LLM calls overlap. The note is helpful, not an error.</p> <p>Attribution check fails (<code>--check</code>) - The CLI prints: <code>graph:graph</code>, <code>LLM total_s</code>, <code>Tool total_s</code>, <code>Unattributed</code>, and any overage.   Small residuals under <code>--epsilon</code> are tolerated.</p> <p>Agent plots still zero - The agent aggregators depend on <code>context.agent</code> and <code>llm_events</code> being present per run. If your pipeline writes tokens only at the model level without <code>llm_events</code>, the totals per agent will be zero.</p>"},{"location":"Metrics-Guide/#version-notes","title":"Version notes","text":"<ul> <li>SUPER \u201cby model\u201d and \u201cby agent\u201d charts are additive across all discovered threads.  </li> <li>SUPER footers avoid a single run-window timestamp (threads can overlap and run elsewhere); they report sums instead.</li> </ul> <p>Happy charting!</p>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/","title":"Plan\u2013Execute Runner \u2014 Checkpointing &amp; Resume Guide","text":"<p>This guide shows how to run <code>plan_execute_from_yaml.py</code>, what files it creates, and how to resume from exactly the point you want \u2014 in both single and hierarchical planning modes.</p> <p>Example config to try: <code>examples/two_agent_examples/plan_execute/pi_multiple_ways.yaml</code></p>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#quickstart","title":"Quickstart","text":"<pre><code># Fresh run (prompts auto-default after the countdown)\npython examples/two_agent_examples/plan_execute/plan_execute_from_yaml.py \\\n  --config examples/two_agent_examples/plan_execute/pi_multiple_ways.yaml\n</code></pre> <p>Resume from a specific checkpoint: This example assumes the randomly generated workspace name is <code>FOOBAR</code>.  Change it to the one generated when you ran it yourself.  It also assumes a checkpoint number you want to resume.</p> <p>It's important to understand that what this does is look in <code>workspace</code> for a checkpoint to resume from.  You can set that <code>workspace</code> yourself by name or use the name generated by URSA if you didn't choose one yourself.</p> <pre><code>python examples/two_agent_examples/plan_execute/plan_execute_from_yaml.py \\\n  --config examples/two_agent_examples/plan_execute/pi_multiple_ways.yaml \\\n  --workspace pi_multiple_ways_FOOBAR \\\n  --resume-from executor_checkpoint_5.db\n</code></pre> <p>Headless / HPC-friendly (no waiting at prompts):</p> <pre><code>python examples/two_agent_examples/plan_execute/plan_execute_from_yaml.py \\\n  --config examples/two_agent_examples/plan_execute/pi_multiple_ways.yaml \\\n  --workspace pi_multiple_ways_batchrun \\\n  --interactive-timeout 0\n</code></pre>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#what-the-script-does","title":"What the script does","text":"<ul> <li>Plans steps from your YAML \u201cproblem.\u201d</li> <li>Executes those steps while checkpointing to SQLite.</li> <li>Lets you pause/resume from either:</li> <li>the live checkpoint (<code>executor_checkpoint.db</code>), or</li> <li>any snapshot (<code>executor_checkpoint_*.db</code>) created after each step/sub-step.</li> </ul> <p>All interactive prompts include a countdown and then pick a safe default. Use <code>--interactive-timeout 0</code> to default immediately (great for clusters).</p>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#cli-options","title":"CLI Options","text":"Flag What it does When to use it <code>--config PATH</code> Path to your YAML problem config. Required. Try the example <code>pi_multiple_ways.yaml</code>. <code>--workspace NAME</code> Directory for checkpoints &amp; outputs. If omitted, a new name is generated and printed. Use a stable name to resume later. <code>--planning-mode {single,hierarchical}</code> Force planning mode. If omitted, you\u2019ll be prompted. First run locks the workspace to that mode. <code>single</code> = plan once, run top-level steps. <code>hierarchical</code> = plan top-level, then re-plan sub-steps per main step. <code>--stepwise-exit</code> Demo mode: exits after each plan/step checkpoint. Good for demonstrating checkpointing; not needed normally. <code>--resume-from FILE</code> Restore executor state from a checkpoint file (e.g., <code>executor_checkpoint_5.db</code> or <code>executor_checkpoint_3_2.db</code>). Jump back to an earlier step or to a precise sub-step. <code>--interactive-timeout SECONDS</code> How long prompts (model/mode/checkpoint) wait before defaulting. <code>0</code> = default immediately. Set to <code>0</code> for HPC/headless. Otherwise default is <code>60</code>."},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#files-youll-see-in-a-workspace","title":"Files you\u2019ll see in a workspace","text":"<pre><code>&lt;workspace&gt;/\n\u251c\u2500\u2500 executor_checkpoint.db          # live executor DB (current run)\n\u251c\u2500\u2500 executor_checkpoint_1.db        # snapshot after step 1 (single mode)\n\u251c\u2500\u2500 executor_checkpoint_2.db        # snapshot after step 2\n\u251c\u2500\u2500 executor_checkpoint_3_2.db      # snapshot after MAIN=3, SUB=2 (hierarchical)\n\u251c\u2500\u2500 executor_progress.json          # single-mode progress (next_index, plan_hash, last_summary)\n\u251c\u2500\u2500 hier_progress.json              # hierarchical progress (main index + per-step sub-progress)\n\u251c\u2500\u2500 planner_checkpoint.db           # planner\u2019s DB\n\u251c\u2500\u2500 run_meta.json                   # metadata (planning_mode lock, plan hash, model, etc.)\n\u2514\u2500\u2500 ...                             # artifacts produced by steps (csv, plots, code, html, etc.)\n</code></pre> <p>Snapshot naming: - Single mode: <code>executor_checkpoint_&lt;STEP&gt;.db</code>   Example for 6 steps: <code>executor_checkpoint_1.db</code> . . . <code>executor_checkpoint_6.db</code>. - Hierarchical mode: <code>executor_checkpoint_&lt;MAIN&gt;_&lt;SUB&gt;.db</code> <code>&lt;MAIN&gt;</code> is the 1-based top-level step; <code>&lt;SUB&gt;</code> is the 1-based sub-step just finished.</p> <p>You can resume from any of these snapshot files.</p>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#single-vs-hierarchical","title":"Single vs. Hierarchical","text":""},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#single-mode","title":"Single mode","text":"<ul> <li>Planner creates one list of top-level steps.</li> <li>Execution proceeds linearly.</li> <li>After each top-level step completes, a snapshot is saved:</li> <li><code>executor_checkpoint_1.db</code>, <code>executor_checkpoint_2.db</code>, . . . </li> </ul> <p>Resuming (single): - <code>--resume-from executor_checkpoint_5.db</code> sets <code>executor_progress.json</code> so the next step to run is step 6. - Without <code>--resume-from</code>, you\u2019ll get an interactive chooser (with countdown). Default is <code>executor_checkpoint.db</code> (live).</p> <p>Example tree (6 steps planned):</p> <pre><code>workspace/\n\u251c\u2500\u2500 executor_checkpoint.db\n\u251c\u2500\u2500 executor_checkpoint_1.db\n\u251c\u2500\u2500 executor_checkpoint_2.db\n\u251c\u2500\u2500 executor_checkpoint_3.db\n\u251c\u2500\u2500 executor_checkpoint_4.db\n\u251c\u2500\u2500 executor_checkpoint_5.db\n\u251c\u2500\u2500 executor_checkpoint_6.db\n\u2514\u2500\u2500 executor_progress.json\n</code></pre> <p>To redo from step 3, run with: <code>--resume-from executor_checkpoint_2.db</code>.</p>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#hierarchical-mode","title":"Hierarchical mode","text":"<ul> <li>Planner creates top-level steps.</li> <li>For each main step, it re-plans concrete sub-steps.</li> <li>A snapshot is saved after each sub-step:</li> <li><code>executor_checkpoint_3_2.db</code> = finished main step 3, sub-step 2.</li> </ul> <p>Resuming (hierarchical): - <code>--resume-from executor_checkpoint_3_2.db</code> resumes within main step 3 at sub-step 3 (if it exists). - <code>executor_checkpoint_4_1.db</code> would continue with main 4, sub-step 2, etc.</p>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#typical-flows","title":"Typical flows","text":""},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#fresh-run-default-everything","title":"Fresh run, default everything","text":"<pre><code>python examples/two_agent_examples/plan_execute/plan_execute_from_yaml.py \\\n  --config examples/two_agent_examples/plan_execute/pi_multiple_ways.yaml\n</code></pre> <ul> <li>Prompts choose defaults after the countdown.</li> <li>A new workspace is created if <code>--workspace</code> isn\u2019t supplied.</li> </ul>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#resume-from-the-latest-run-no-prompts","title":"Resume from the latest run (no prompts)","text":"<pre><code>python examples/two_agent_examples/plan_execute/plan_execute_from_yaml.py \\\n  --config examples/two_agent_examples/plan_execute/pi_multiple_ways.yaml \\\n  --workspace pi_multiple_ways_myws \\\n  --interactive-timeout 0\n</code></pre> <ul> <li>Uses the default <code>executor_checkpoint.db</code>.  </li> <li>If no checkpoints exist yet, it starts fresh.</li> </ul>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#jump-back-to-an-earlier-point","title":"Jump back to an earlier point","text":"<pre><code>python examples/two_agent_examples/plan_execute/plan_execute_from_yaml.py \\\n  --config examples/two_agent_examples/plan_execute/pi_multiple_ways.yaml \\\n  --workspace pi_multiple_ways_myws \\\n  --resume-from executor_checkpoint_5.db\n</code></pre> <ul> <li>Great for re-running from step 6 in single mode, or for precise sub-step resumes in hierarchical mode using <code>&lt;MAIN&gt;_&lt;SUB&gt;</code>.</li> </ul>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#planning-mode-lock-per-workspace","title":"Planning-mode lock per workspace","text":"<p>On the first run in a workspace, the chosen planning mode is locked in <code>run_meta.json</code>. If you later pass a different <code>--planning-mode</code> for the same workspace, the script warns and keeps the original mode. Use a new <code>--workspace</code> to switch modes for a project.</p>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#when-the-plan-changes","title":"When the plan changes","text":"<p>The runner hashes your plan (<code>plan_sig</code>). If the plan changes:</p> <ul> <li>Single mode: <code>executor_progress.json</code> resets to step 0 for the new plan.</li> <li>Hierarchical mode: top-level or sub-plan changes reset the affected progress segments.</li> </ul> <p>If you want a clean re-run, either: - remove <code>executor_progress.json</code> (single), and/or - remove <code>hier_progress.json</code> (hierarchical), or - choose a new workspace.</p>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#example-run-the-provided-yaml","title":"Example: run the provided YAML","text":"<pre><code>python examples/two_agent_examples/plan_execute/plan_execute_from_yaml.py \\\n  --config examples/two_agent_examples/plan_execute/pi_multiple_ways.yaml \\\n  --planning-mode single\n</code></pre> <p>You\u2019ll see messages like:</p> <pre><code>[checkpoint] saved step snapshot: executor_checkpoint_1.db\n[checkpoint] saved step snapshot: executor_checkpoint_2.db\n...\n</code></pre> <p>Now resume from after step 4:</p> <pre><code>python examples/two_agent_examples/plan_execute/plan_execute_from_yaml.py \\\n  --config examples/two_agent_examples/plan_execute/pi_multiple_ways.yaml \\\n  --workspace pi_multiple_ways_&lt;your-workspace&gt; \\\n  --resume-from executor_checkpoint_4.db\n</code></pre> <p>Or try hierarchical mode:</p> <pre><code>python examples/two_agent_examples/plan_execute/plan_execute_from_yaml.py \\\n  --config examples/two_agent_examples/plan_execute/pi_multiple_ways.yaml \\\n  --planning-mode hierarchical\n</code></pre> <p>You\u2019ll see sub-step snapshots like <code>executor_checkpoint_3_2.db</code>.</p>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#tips-for-clusters-headless","title":"Tips for clusters / headless","text":"<ul> <li>Use <code>--interactive-timeout 0</code> so prompts immediately pick defaults.</li> <li>Always pass <code>--workspace</code> so outputs land where you expect.</li> <li>Use <code>--resume-from</code> to pin the exact snapshot to restore (no prompt).</li> </ul>"},{"location":"Plan-Execute-Runner-Checkpointing-Guide/#troubleshooting","title":"Troubleshooting","text":"<ul> <li> <p>\u201cAll steps already executed\u201d (single):   You\u2019re at the end per <code>executor_progress.json</code>. To re-run, delete it or resume from an earlier snapshot (e.g., <code>--resume-from executor_checkpoint_2.db</code>).</p> </li> <li> <p>\u201cTop-level plan changed \u2014 resetting . . .\u201d (hierarchical):   The plan hash changed; progress resets for safety.</p> </li> <li> <p>Mode mismatch warning:   The workspace is locked to the first-run mode. Use a new <code>--workspace</code> to change modes.</p> </li> </ul> <p>Happy checkpointing &amp; resuming!</p>"},{"location":"arxiv_agent/","title":"ArxivAgent Documentation","text":"<p><code>ArxivAgent</code> is a class that helps fetch, process, and summarize scientific papers from arXiv. It uses LLMs to generate summaries of papers relevant to a given query and context.</p>"},{"location":"arxiv_agent/#basic-usage","title":"Basic Usage","text":"<pre><code>from ursa.agents import ArxivAgent\n\n# Initialize the agent\nagent = ArxivAgent()\n\n# Run a query\nresult = agent.invoke(\n    arxiv_search_query=\"Experimental Constraints on neutron star radius\", \n    context=\"What are the constraints on the neutron star radius and what uncertainties are there on the constraints?\"\n)\n\n# Print the summary\nprint(result)\n</code></pre>"},{"location":"arxiv_agent/#parameters","title":"Parameters","text":"<p>When initializing <code>ArxivAgent</code>, you can customize its behavior with these parameters:</p> Parameter Type Default Description <code>llm</code> <code>BaseChatModel</code> <code>init_chat_model(\"openai:gpt-5-mini\")</code> The LLM model to use for summarization <code>summarize</code> bool True Whether to summarize the papers or just fetch them <code>process_images</code> bool True Whether to extract and describe images from papers <code>max_results</code> int 3 Maximum number of papers to fetch from arXiv <code>database_path</code> str 'arxiv_papers' Directory to store downloaded PDFs <code>summaries_path</code> str 'arxiv_generated_summaries' Directory to store paper summaries <code>vectorstore_path</code> str 'arxiv_vectorstores' Directory to store vector embeddings <code>download</code> bool True Whether to download papers or use existing ones"},{"location":"arxiv_agent/#advanced-usage","title":"Advanced Usage","text":""},{"location":"arxiv_agent/#customizing-the-agent","title":"Customizing the Agent","text":"<pre><code>from langchain.chat_models import init_chat_model\nfrom ursa.agents import ArxivAgent\n\nagent = ArxivAgent(\n    llm=init_chat_model(\"openai:gpt-5-mini\"),  # Use a more powerful model\n    max_results=5,       # Fetch more papers\n    process_images=False,  # Skip image processing to save time\n    download=False  # Use only papers already in database_path\n)\n</code></pre>"},{"location":"arxiv_agent/#running-multiple-queries","title":"Running Multiple Queries","text":"<pre><code># First query\nresult1 = agent.invoke(\n    arxiv_search_query=\"quantum computing error correction\", \n    context=\"Summarize recent advances in quantum error correction techniques\"\n)\n\n# Second query (will reuse downloaded papers if applicable)\nresult2 = agent.invoke(\n    arxiv_search_query=\"quantum computing algorithms\", \n    context=\"What are the most promising quantum algorithms for near-term devices?\"\n)\n</code></pre>"},{"location":"arxiv_agent/#how-it-works","title":"How It Works","text":"<ol> <li> <p>Fetching Papers: The agent searches arXiv for papers matching your query and downloads them as PDFs.</p> </li> <li> <p>Processing: If <code>summarize=True</code>, each paper is:</p> </li> <li>Converted to text</li> <li>Split into chunks</li> <li>Embedded into a vector database</li> <li> <p>If <code>process_images=True</code>, images are extracted and described using GPT-4 Vision</p> </li> <li> <p>Summarization: The agent:</p> </li> <li>Retrieves the most relevant chunks based on your context</li> <li>Generates a summary for each paper</li> <li> <p>Creates a final summary addressing your specific context</p> </li> <li> <p>Output: Returns a comprehensive summary that synthesizes information from all relevant papers.</p> </li> </ol>"},{"location":"arxiv_agent/#notes","title":"Notes","text":"<ul> <li>Summaries and vector stores are cached, making subsequent queries faster.</li> <li>The agent uses a ThreadPoolExecutor to process papers in parallel.</li> <li>You can find the combined summaries in 'summaries_combined.txt' and the final summary in 'final_summary.txt'.</li> </ul>"},{"location":"chatollama_setup/","title":"Running with ChatOllama","text":"<p>Disable OPENAI_API_KEY by setting the following two env variales: (without both of these env vars ollama complains about auth)</p> <pre><code>$ export OPENAI_API_KEY=ollama\n$ export OPENAI_BASE_URL=&lt;YOUR OLLAMA ENDPOINT&gt;\n</code></pre> <p>Example Auth Error</p> <pre><code>openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided: ollama. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n</code></pre>"},{"location":"combining_arxiv_and_execution/","title":"Using ArxivAgent and ExecutionAgent Together","text":"<p>This guide demonstrates how to combine the <code>ArxivAgent</code> and <code>ExecutionAgent</code> for comprehensive research and analysis workflows.</p>"},{"location":"combining_arxiv_and_execution/#overview","title":"Overview","text":"<p>This workflow enables you to: 1. Search and analyze papers from arXiv 2. Process the research findings 3. Generate executable code to analyze and visualize the data</p>"},{"location":"combining_arxiv_and_execution/#basic-usage","title":"Basic Usage","text":"<pre><code>from langchain.chat_models import init_chat_model\nfrom langchain_core.messages import HumanMessage\nfrom ursa.agents import ArxivAgent, ExecutionAgent\n\n# Initialize the language model\nmodel = init_chat_model(\n    model=\"openai:gpt-5-mini\",\n    max_tokens=50000,\n)\n\n# Initialize the ArxivAgent\narxiv_agent = ArxivAgent(\n    llm=model,\n    summarize=True,\n    process_images=False,\n    max_results=20,\n    database_path=\"arxiv_papers_materials1\",\n    summaries_path=\"arxiv_summaries_materials1\",\n    vectorstore_path=\"arxiv_vectorstores_materials1\",\n    download=True,\n)\n\n# Run a search and analysis\nresearch_results = arxiv_agent.invoke(\n    arxiv_search_query=\"high entropy alloy hardness\",\n    context=\"What data and uncertainties are reported for hardness of the high entropy alloy and how that that compare to other alloys?\",\n)\n\n# Initialize the ExecutionAgent\nexecutor = ExecutionAgent(llm=model)\n\n# Create a task for the ExecutionAgent\nexecution_plan = f\"\"\"\nThe following is the summaries of research papers on the high entropy alloy hardness: \n{research_results}\n\nSummarize the results in a markdown document. Include a plot of the data extracted from the papers. This \nwill be reviewed by experts in the field so technical accuracy and clarity is critical.\n\"\"\"\n\n# Prepare input for the ExecutionAgent\ninit = {\"messages\": [HumanMessage(content=execution_plan)]}\n\n# Execute the plan\n_ = executor.invoke(init)\n\n</code></pre>"},{"location":"combining_arxiv_and_execution/#parameters","title":"Parameters","text":""},{"location":"combining_arxiv_and_execution/#arxivagent","title":"ArxivAgent","text":"Parameter Description <code>llm</code> Language model to use for analysis <code>summarize</code> Whether to summarize papers (boolean) <code>process_images</code> Whether to process images in papers (boolean) <code>max_results</code> Maximum number of papers to retrieve <code>database_path</code> Path to store downloaded papers <code>summaries_path</code> Path to store paper summaries <code>vectorstore_path</code> Path to store vector embeddings <code>download</code> Whether to download full papers (boolean)"},{"location":"combining_arxiv_and_execution/#executionagent","title":"ExecutionAgent","text":"Parameter Description <code>llm</code> Language model to use for code generation and execution"},{"location":"combining_arxiv_and_execution/#workflow-steps","title":"Workflow Steps","text":"<ol> <li> <p>Research Phase: ArxivAgent searches arXiv for relevant papers based on your query, downloads them, and analyzes their content according to your context.</p> </li> <li> <p>Analysis Phase: ExecutionAgent takes the research results and generates code to analyze and visualize the data.</p> </li> <li> <p>Output: The ExecutionAgent produces a markdown document with analysis, visualizations, and insights from the research.</p> </li> </ol>"},{"location":"combining_arxiv_and_execution/#use-cases","title":"Use Cases","text":"<ul> <li>Literature reviews on scientific topics</li> <li>Data extraction and visualization from research papers</li> <li>Comparative analysis across multiple publications</li> <li>Technical report generation</li> </ul>"},{"location":"combining_arxiv_and_execution/#notes","title":"Notes","text":"<ul> <li>Ensure you have sufficient disk space for paper storage</li> <li>Processing a large number of papers may take significant time</li> <li>The quality of analysis depends on the capabilities of the chosen language model</li> </ul>"},{"location":"combining_arxiv_and_execution_neutronStar/","title":"Using ArxivAgent and ExecutionAgent for Astrophysics Research","text":"<p>This guide demonstrates how to use the <code>ArxivAgent</code> and <code>ExecutionAgent</code> together to research neutron star properties and generate a comprehensive analysis.</p>"},{"location":"combining_arxiv_and_execution_neutronStar/#overview","title":"Overview","text":"<p>This workflow allows you to: 1. Search arXiv for papers on neutron star radius constraints 2. Process and summarize the research findings 3. Generate a markdown document with data visualization</p>"},{"location":"combining_arxiv_and_execution_neutronStar/#basic-usage","title":"Basic Usage","text":"<pre><code>from langchain.chat_models import init_chat_model\nfrom langchain_core.messages import HumanMessage\nfrom ursa.agents import ArxivAgent, ExecutionAgent\n\n# Initialize the language model\nmodel = init_chat_model(\n    model=\"openai:gpt-5-mini\",\n    max_tokens=50000,\n)\n\n# Initialize the ArxivAgent\narxiv_agent = ArxivAgent(\n    llm=model,\n    summarize=True,\n    process_images=False,\n    max_results=5,\n    database_path=\"arxiv_papers_neutron_star\",\n    summaries_path=\"arxiv_summaries_neutron_star\",\n    vectorstore_path=\"arxiv_vectorstores_neutron_star\",\n    download_papers=True,\n)\n\n# Run a search on neutron star radius constraints\nresearch_results = arxiv_agent.invoke(\n    arxiv_search_query=\"Experimental Constraints on neutron star radius\",\n    context=\"What are the constraints on the neutron star radius and what uncertainties are there on the constraints?\",\n)\n\n# Initialize the ExecutionAgent\nexecutor = ExecutionAgent(llm=model)\n\n# Create a task for the ExecutionAgent\nexecution_plan = f\"\"\"\nThe following is the summaries of research papers on the contraints on neutron\nstar radius: \n{research_results}\n\nSummarize the results in a markdown document. Include a plot of the data extracted from the papers. This \nwill be reviewed by experts in the field so technical accuracy and clarity is \ncritical.\n\"\"\"\n\n# Prepare input for the ExecutionAgent\ninit = {\"messages\": [HumanMessage(content=execution_plan)]}\n\n# Execute the plan\n_ = executor.invoke(init)\n\n</code></pre>"},{"location":"combining_arxiv_and_execution_neutronStar/#parameters","title":"Parameters","text":""},{"location":"combining_arxiv_and_execution_neutronStar/#arxivagent","title":"ArxivAgent","text":"Parameter Description <code>llm</code> Language model to use for analysis <code>summarize</code> Whether to summarize papers (boolean) <code>process_images</code> Whether to process images in papers (boolean) <code>max_results</code> Maximum number of papers to retrieve (5 in this example) <code>database_path</code> Path to store downloaded papers <code>summaries_path</code> Path to store paper summaries <code>vectorstore_path</code> Path to store vector embeddings <code>download_papers</code> Whether to download full papers (boolean)"},{"location":"combining_arxiv_and_execution_neutronStar/#executionagent","title":"ExecutionAgent","text":"Parameter Description <code>llm</code> Language model to use for code generation and execution"},{"location":"combining_arxiv_and_execution_neutronStar/#workflow-steps","title":"Workflow Steps","text":"<ol> <li> <p>Research Phase: ArxivAgent searches arXiv for papers on neutron star radius constraints, downloads them, and analyzes their content.</p> </li> <li> <p>Analysis Phase: ExecutionAgent processes the research summaries and generates code to visualize the constraints and uncertainties.</p> </li> <li> <p>Output: The ExecutionAgent produces a markdown document with analysis, visualizations, and insights about neutron star radius constraints.</p> </li> </ol>"},{"location":"combining_arxiv_and_execution_neutronStar/#use-cases","title":"Use Cases","text":"<ul> <li>Astrophysics literature reviews</li> <li>Compilation of experimental constraints on astronomical objects</li> <li>Visualization of scientific data from multiple sources</li> <li>Creation of technical reports for expert audiences</li> </ul>"},{"location":"combining_arxiv_and_execution_neutronStar/#notes","title":"Notes","text":"<ul> <li>The quality of analysis depends on the available papers and the capabilities of the language model</li> <li>Consider adjusting <code>max_results</code> based on the breadth of research needed</li> <li>Ensure proper storage paths are set to avoid conflicts with other research projects</li> </ul>"},{"location":"execution_agent/","title":"ExecutionAgent Documentation","text":"<p><code>ExecutionAgent</code> is a class that enables AI-powered code execution, writing, and editing. It uses a state machine architecture to safely execute commands, write code files, and search for information.</p>"},{"location":"execution_agent/#basic-usage","title":"Basic Usage","text":"<pre><code>from ursa.agents import ExecutionAgent\n\n# Initialize the agent\nagent = ExecutionAgent()\n\n# Run a prompt\nresult = agent(\"Write and execute a python script to print the first 10 integers.\")\n\n# Access the final response\nprint(result[\"messages\"][-1].text)\n</code></pre>"},{"location":"execution_agent/#parameters","title":"Parameters","text":"<p>When initializing <code>ExecutionAgent</code>, you can customize its behavior with these parameters:</p> Parameter Type Default Description <code>llm</code> <code>BaseChatModel</code> <code>init_chat_model(\"openai:gpt-5-mini\")</code> The LLM model to use <code>extra_tools</code> <code>Optional[list[Callable[..., Any]]]</code> <code>None</code> Additional tools for the execution agent"},{"location":"execution_agent/#features","title":"Features","text":""},{"location":"execution_agent/#code-execution","title":"Code Execution","text":"<p>The agent can safely execute shell commands in a controlled environment:</p> <pre><code>result = agent(\"Install numpy and create a script that uses it to calculate the mean of [1, 2, 3, 4, 5]\")\n</code></pre>"},{"location":"execution_agent/#code-writing","title":"Code Writing","text":"<p>The agent can write code files to a workspace directory:</p> <pre><code>result = agent(\"Create a Flask web application that displays 'Hello World'\")\n</code></pre>"},{"location":"execution_agent/#advanced-usage","title":"Advanced Usage","text":""},{"location":"execution_agent/#customizing-the-workspace","title":"Customizing the Workspace","text":"<p>The agent creates a workspace folder with a randomly generated name for each run. You can access this workspace path from the agent:</p> <pre><code>result = agent(\"Create a Python script\"))\nworkspace_path = agent.workspace\nprint(f\"Files were created in: {workspace_path}\")\n</code></pre>"},{"location":"execution_agent/#setting-a-recursion-limit","title":"Setting a Recursion Limit","text":"<p>For complex tasks, you may need to adjust the recursion limit:</p> <pre><code>result = agent.invoke(\n    \"Create a complex project with multiple files and tests\",\n    recursion_limit=2000\n)\n</code></pre>"},{"location":"execution_agent/#safety-features","title":"Safety Features","text":"<p>The agent includes built-in safety checks for shell commands:</p> <ol> <li>Commands are evaluated for safety before execution</li> <li>Unsafe commands are blocked with explanations</li> <li>The agent suggests safer alternatives when appropriate</li> </ol>"},{"location":"execution_agent/#how-it-works","title":"How It Works","text":"<ol> <li>State Machine: The agent uses a directed graph to manage its workflow:</li> <li><code>agent</code> node: Processes user requests and generates responses</li> <li> <p><code>action</code> node: Executes tools (<code>run_cmd</code>, <code>write_code</code>, <code>edit_code</code>, <code>search</code>)</p> <ul> <li>extra tools can be provided to the agent as follows:    ```py    from langchain.tools import tool</li> </ul> <p>@tool    def do_magic(a: int, b: int) -&gt; float:        \"\"\"Do magic with integers a and b.</p> <pre><code>   Args:\n       a: first integer\n       b: second integer\n   \"\"\"\n   return sqrt(a**2 + b**2)\n</code></pre> <p>agent = ExecutionAgent(extra_tools=[do_magic])    <code>``    -</code>summarize` node: Creates a final summary when complete</p> </li> <li> <p>Tools:</p> </li> <li><code>run_cmd</code>: Executes shell commands in the workspace directory</li> <li><code>write_code</code>: Creates new code files with syntax highlighting</li> <li><code>edit_code</code>: Modifies existing code files with diff preview</li> <li> <p><code>search_tool</code>: Performs web searches via DuckDuckGo</p> </li> <li> <p>Visualization:</p> </li> <li>Code changes are displayed with syntax highlighting</li> <li>File edits show detailed diffs</li> <li>Command execution shows stdout and stderr</li> </ol>"},{"location":"execution_agent/#notes","title":"Notes","text":"<ul> <li>The agent creates a new workspace directory for each run</li> <li>Files are written to and executed from this workspace</li> <li>Shell commands have a 60000-second timeout by default</li> <li>The agent can handle keyboard interrupts during command execution</li> </ul>"},{"location":"humanInTheLoop_example/","title":"URSA Human-in-the-Loop Agent Interface Documentation","text":""},{"location":"humanInTheLoop_example/#previous-human-in-the-loop-example-has-been-depreciated","title":"Previous Human-in-the-Loop Example has been Depreciated","text":"<p>The HITL interface can now be accessed via the URSA CLI, launch with:</p> <p><code>$ ursa</code></p> <p>and help on commands can he accessed with </p> <p><code>$ ursa --help</code></p>"},{"location":"humanInTheLoop_example/#basic-usage","title":"Basic Usage","text":"<p>To prompt an URSA agent through the CLI, first select an agent, then issue a prompt to the agent:</p> <pre><code>ursa&gt; execute\nexecute: Make me a histogram of the first 10000 prime number spacings\n</code></pre> <p>You can also issue the prompt in one line by prepending the agent name:</p> <pre><code>ursa&gt; execute Make me a histogram of the first 10000 prime number spacings`\n</code></pre> <p>to see the names of available agents, prompt the CLI with <code>help</code>:</p> <pre><code>ursa&gt; help\n\nDocumented commands (type help &lt;topic&gt;):\n========================================\nEOF  agents  arxiv  chat  clear  execute  exit  help  models  web\n\nUndocumented commands:\n======================\nhypothesize  plan\n\n</code></pre> <p>Some additional documentation on the URSA github repo: LINK with more to come.</p> <p>We should have an in-depth documentation for it, but right now it's documented a bit on the main README and through the help flags with the CLI call. </p>"},{"location":"hypothesizer_agent/","title":"HypothesizerAgent Documentation","text":"<p><code>HypothesizerAgent</code> is a multi-agent system that iteratively refines solutions to complex problems through a structured debate process. It employs three specialized agents that work together to generate, critique, and provide alternative perspectives on potential solutions.</p>"},{"location":"hypothesizer_agent/#basic-usage","title":"Basic Usage","text":"<pre><code>from ursa.agents import HypothesizerAgent\n\n# Initialize the agent\nagent = HypothesizerAgent()\n\n# Run the agent with a question\nsolution = agent.invoke(\n    prompt=\"Find a city with at least 10 vowels in its name.\",\n    max_iter=3\n)\n\n# Print the final solution\nprint(solution)\n</code></pre>"},{"location":"hypothesizer_agent/#parameters","title":"Parameters","text":""},{"location":"hypothesizer_agent/#initialization","title":"Initialization","text":"Parameter Type Default Description <code>llm</code> <code>BaseChatModel</code> <code>init_chat_model(\"openai:gpt-5-mini\")</code> The language model to use for all agents <code>**kwargs</code> <code>dict</code> <code>{}</code> Additional parameters passed to the base agent"},{"location":"hypothesizer_agent/#run-method","title":"Run Method","text":"Parameter Type Default Description <code>prompt</code> str Required The question or problem to solve <code>max_iter</code> int 3 Maximum number of refinement iterations <code>recursion_limit</code> int 99999 Maximum recursion depth for the graph"},{"location":"hypothesizer_agent/#how-it-works","title":"How It Works","text":"<p>The system uses a three-agent debate process:</p> <ol> <li>Agent 1 (Hypothesizer): Generates initial solutions and refines them based on feedback</li> <li>Agent 2 (Critic): Identifies flaws, assumptions, and areas for improvement</li> <li>Agent 3 (Competitor/Stakeholder): Provides alternative perspectives from different stakeholders</li> </ol> <p>The process iterates through these agents multiple times, with each iteration building on the feedback from previous rounds.</p>"},{"location":"hypothesizer_agent/#features","title":"Features","text":"<ul> <li>Web Search Integration: Uses DuckDuckGo to gather information for each agent</li> <li>Iterative Refinement: Solutions improve through multiple rounds of critique and revision</li> <li>LaTeX Report Generation: Creates a comprehensive LaTeX document summarizing the process</li> <li>URL Tracking: Records all websites visited during the research process</li> </ul>"},{"location":"hypothesizer_agent/#output","title":"Output","text":"<p>The agent produces:</p> <ol> <li>A final refined solution (returned by the <code>run</code> method)</li> <li>A LaTeX document with:</li> <li>Executive summary of the iterative process</li> <li>Final solution in full</li> <li>Detailed appendix of all iterations</li> <li>List of websites visited during research</li> <li>A text file containing the full history of all iterations</li> </ol>"},{"location":"hypothesizer_agent/#example","title":"Example","text":"<pre><code>from ursa.agents import HypothesizerAgent\nfrom langchain.chat_models import init_chat_model\n\n# Initialize with a specific LLM\nagent = HypothesizerAgent(llm=init_chat_model(\"openai:gpt-5-mini\"))\n\n# Run with 5 iterations\nresult = agent.invoke(\n    prompt=\"What strategies could a small local bookstore use to compete with large online retailers?\",\n    max_iter=5\n)\n\nprint(result)\n</code></pre>"},{"location":"hypothesizer_agent/#notes","title":"Notes","text":"<ul> <li>Each iteration includes a solution, critique, and competitor perspective</li> <li>The agent performs web searches to gather information at each step</li> <li>The final LaTeX document provides a comprehensive record of the reasoning process</li> <li>Higher <code>max_iter</code> values produce more refined solutions but take longer to complete</li> </ul>"},{"location":"lammps_agent/","title":"LammpsAgent Documentation","text":"<p><code>LammpsAgent</code> is a class that helps set up and run a LAMMPS simulation workflow. At the highest level, it can:</p> <ul> <li>discover candidate interatomic potentials from the NIST database for a set of elements,</li> <li>summarize and choose a potential for the simulation task at hand,</li> <li>author a LAMMPS input script using the chosen potential (and an optional template / data file),</li> <li>execute LAMMPS via MPI (CPU or Kokkos GPU),</li> <li>iteratively \u201cfix\u201d the input script on failures by using run history until success or a max attempt limit.</li> </ul> <p>The agent writes the outputs into a local <code>workspace</code> directory and uses rich console panels to display progress, choices, diffs, and errors.</p>"},{"location":"lammps_agent/#dependencies","title":"Dependencies","text":"<p>The main dependency is the LAMMPS code that needs to be separately installed. LAMMPS is a classical molecular dynamics code developed by Sandia National Laboratories. Installation instructions can be found here. On MacOS and Linux systems, the simplest way to install LAMMPS is often via Conda, in the same conda environment where <code>ursa</code> is installed.</p> <p>The dependencies for <code>LammpsAgent</code> are not included with the basic <code>ursa</code> installation, but can be installed via <code>pip install 'ursa[lammps]'</code> or <code>uv add 'ursa[lammps]'</code>.</p>"},{"location":"lammps_agent/#basic-usage","title":"Basic Usage","text":"<pre><code>from ursa.agents import LammpsAgent\nfrom langchain_openai import ChatOpenAI\n\nagent = LammpsAgent(llm = ChatOpenAI(model='gpt-5'))\n\nresult = agent.invoke({\n    \"simulation_task\": \"Carry out a LAMMPS simulation of Cu to determine its equation of state.\",\n    \"elements\": [\"Cu\"],\n    \"template\": \"No template provided.\"  #Template for the input file\n})\n</code></pre> <p>For more advanced usage see examples here: <code>ursa/examples/two_agent_examples/lammps_execute/</code>.</p>"},{"location":"lammps_agent/#high-level-flow","title":"High-level flow","text":"<p>The agent compiles a <code>StateGraph(LammpsState)</code> with this logic:</p>"},{"location":"lammps_agent/#entry-routing","title":"Entry routing","text":"<p>Chooses one of three paths:</p> <ol> <li>User-provided potential:</li> <li>This path is chosen when the user provides a specific potential file, along with the <code>pair_style</code>/<code>pair_coeff</code> information required to generate the input script</li> <li>In this case the autonomous potential search/selection by the agent is skipped</li> <li> <p>The provided potential file is copied to <code>workspace</code></p> </li> <li> <p>User-chosen potential already in state (<code>state[\"chosen_potential\"]</code> exists):</p> </li> <li>This is similar to the above path, but the user selects a potential from the <code>atomman</code> database and initializes the state with this entry before invoking the agent</li> <li> <p>This path also skips the potential search/selection and goes straight to authoring a LAMMPS input script for the user-chosen potential</p> </li> <li> <p>Agent-selected potential:</p> </li> <li>Agent queries NIST (via atomman) for potentials matching the requested elements</li> <li>Summarizes NIST's data on each potential (up to <code>max_potentials</code>) with regards to the applicability of the potential for the given <code>simulation task</code></li> <li>Ultimately picks one potential </li> </ol> <p>If a <code>data_file</code> is provided to the agent, the entry router attempts to copy it into the workspace.</p>"},{"location":"lammps_agent/#potential-search-selection-agent-selected-path","title":"Potential search &amp; selection (agent-selected path)","text":"<ul> <li><code>_find_potentials</code>: queries <code>atomman.library.Database(remote=True)</code> for potentials matching:</li> <li><code>elements</code> from state</li> <li>supported <code>pair_styles</code> list (see <code>self.pair_styles</code>)</li> <li><code>_summarize_one</code>: for each candidate potential:</li> <li>extracts data on potential from  NIST</li> <li>trims extracted text to a token budget using <code>tiktoken</code></li> <li>summarizes usefulness for the requested <code>simulation_task</code></li> <li>writes summary to <code>workspace/potential_summaries/potential_&lt;i&gt;.txt</code></li> <li><code>_build_summaries</code>: builds a combined string of summaries for selection</li> <li><code>_choose</code>: the agent selects the final potential to be used and the rationale for choosing it</li> <li>writes rationale to <code>workspace/potential_summaries/Rationale.txt</code></li> <li>stores <code>chosen_potential</code> in state</li> </ul> <p>If <code>find_potential_only=True</code>, the graph exits after choosing the potential (or finding no matches).</p>"},{"location":"lammps_agent/#author-input","title":"Author input","text":"<ul> <li>Downloads potential files into <code>workspace</code> (only if not user-provided)</li> <li>Gets <code>pair_info</code> via <code>chosen_potential.pair_info()</code></li> <li>Optionally includes:</li> <li><code>template</code> from state for the LAMMPS input script</li> <li><code>data_file</code> (usually for the atomic structure that can be included in the input script) </li> <li>The agent authors the input script: <code>{ \"input_script\": \"&lt;string&gt;\" }</code></li> <li>Writes <code>workspace/in.lammps</code></li> <li>Enforces that logs should go to <code>./log.lammps</code> </li> </ul>"},{"location":"lammps_agent/#run-lammps","title":"Run LAMMPS","text":"<p>Runs <code>&lt;mpirun_cmd&gt;</code> with <code>-np &lt;mpi_procs&gt;</code> in <code>workspace</code>:</p> <p>Allowed options for <code>&lt;mpirun_cmd&gt;</code> are <code>mpirun</code> and <code>mpiexec</code> (see also Parameters section below).</p> <p>For example, LAMMPS run commands executed by the agent look like: </p> <ul> <li>CPU mode (default, when <code>ngpus &lt; 0</code>):</li> <li> <p><code>mpirun -np &lt;mpi_procs&gt; &lt;lammps_cmd&gt; -in in.lammps</code></p> </li> <li> <p>GPU/Kokkos mode (when <code>ngpus &gt;= 0</code>):</p> </li> <li><code>mpirun -np &lt;mpi_procs&gt; &lt;lammps_cmd&gt; -in in.lammps -k on g &lt;ngpus&gt; -sf kk -pk kokkos neigh half newton on</code></li> </ul> <p>Note that the running under GPU mode is preliminary.</p> <p>The agent captures <code>stdout</code>, <code>stderr</code>, and <code>returncode</code>, and appends an entry to <code>run_history</code>.</p>"},{"location":"lammps_agent/#fix-loop","title":"Fix loop","text":"<p>If the run fails: - formats the entire <code>run_history</code> (scripts + stdout/stderr) into an error blob - the agent produces a new <code>input_script</code>  - prints a unified diff between old and new scripts - overwrites <code>workspace/in.lammps</code> - increments <code>fix_attempts</code> - reruns LAMMPS</p> <p>Stops when: - run succeeds (<code>returncode == 0</code>), or - <code>fix_attempts &gt;= max_fix_attempts</code></p>"},{"location":"lammps_agent/#state-model-lammpsstate","title":"State model (<code>LammpsState</code>)","text":"<p>The graph state is a <code>TypedDict</code> containing (key fields):</p> <ul> <li>Inputs / problem definition</li> <li><code>simulation_task: str</code> \u2014 natural language description of what to simulate</li> <li><code>elements: list[str]</code> \u2014 chemical symbols used to identify candidate potentials</li> <li><code>template: Optional[str]</code> \u2014 optional LAMMPS input template to adapt</li> <li> <p><code>chosen_potential: Optional[Any]</code> \u2014 selected potential object (user-chosen)</p> </li> <li> <p>Potential selection internals</p> </li> <li><code>matches: list[Any]</code> \u2014 candidate potentials from atomman</li> <li><code>idx: int</code> \u2014 index used for summarization loop</li> <li><code>summaries: list[str]</code> \u2014 a brief summary of each potential</li> <li><code>full_texts: list[str]</code> \u2014 the data/metadata on the potential from NIST (capped at <code>max_tokens</code>)</li> <li> <p><code>summaries_combined: str</code> - a single string with the summaries of all the considered potentials</p> </li> <li> <p>Run artifacts</p> </li> <li><code>input_script: str</code> \u2014 current LAMMPS input text written to <code>in.lammps</code></li> <li><code>run_returncode: Optional[int]</code> - generally, <code>returncode = 0</code> indicates a successful simulation run</li> <li><code>run_stdout: str</code> - the stdout from the LAMMPS execution</li> <li><code>run_stderr: str</code> - the stderr from the LAMMPS execution </li> <li><code>run_history: list[dict[str, Any]]</code> \u2014 attempt-by-attempt record</li> <li><code>fix_attempts: int</code> - the number of times the agent has attempted to fix the LAMMPS input script</li> </ul>"},{"location":"lammps_agent/#parameters","title":"Parameters","text":"<p>Key parameters you can tune:</p>"},{"location":"lammps_agent/#potential-selection","title":"Potential selection","text":"<ul> <li><code>potential_files</code>, <code>pair_style</code>, <code>pair_coeff</code>: if all provided, the agent uses the user's potential files and skips search</li> <li><code>max_potentials</code> (default <code>5</code>): max number of candidate potentials to summarize before choosing one</li> <li><code>find_potential_only</code> (default <code>False</code>): exit after selecting a potential (no input LAMMPS input writing/running)</li> </ul>"},{"location":"lammps_agent/#fix-loop_1","title":"Fix loop","text":"<ul> <li><code>max_fix_attempts</code> (default <code>10</code>): maximum number of input rewrite attempts after failures</li> </ul>"},{"location":"lammps_agent/#data-file-support","title":"Data file support","text":"<ul> <li><code>data_file</code> (default <code>None</code>): path to a LAMMPS data file; the agent copies it to <code>workspace</code></li> <li><code>data_max_lines</code> (default <code>50</code>): number of lines from data included in the agent's prompt</li> </ul>"},{"location":"lammps_agent/#execution","title":"Execution","text":"<ul> <li><code>workspace</code> (default <code>./workspace</code>): where <code>in.lammps</code>, potentials, and summaries are written</li> <li><code>mpi_procs</code> (default <code>8</code>): number of mpi processes for LAMMPS run</li> <li><code>ngpus</code> (default <code>-1</code>): set <code>&gt;= 0</code> to enable Kokkos GPU flags</li> <li><code>lammps_cmd</code> (default <code>lmp_mpi</code>): the name of the LAMMPS executable to launch</li> <li><code>mpirun_cmd</code> (default <code>mpirun</code>): currently available options are <code>mpirun</code> and <code>mpiexec</code>. Other options such as <code>srun</code> will be added soon</li> </ul>"},{"location":"lammps_agent/#llm-context-trimming","title":"LLM / context trimming","text":"<ul> <li><code>tiktoken_model</code> (default <code>gpt-5-mini</code>): tokenizer model name used to trim fetched potential metadata text</li> <li><code>max_tokens</code> (default <code>200000</code>): token cap for extracted metadata text</li> </ul>"},{"location":"lammps_agent/#files-and-directories-created","title":"Files and directories created","text":"<p>Inside <code>workspace/</code>:</p> <ul> <li><code>in.lammps</code> \u2014 generated/updated input script</li> <li><code>log.lammps</code> \u2014 expected LAMMPS log output (the LLM is instructed to create it)</li> <li><code>potential_summaries/</code></li> <li><code>potential_&lt;i&gt;.txt</code> \u2014 per-potential LLM summaries</li> <li><code>Rationale.txt</code> \u2014 rationale for the selected potential</li> <li>downloaded potential files (from atomman or copied from user paths)</li> <li>copied <code>data_file</code> (if provided)</li> </ul>"},{"location":"planning_agent/","title":"PlanningAgent Documentation","text":"<p><code>PlanningAgent</code> is a class that implements a multi-step planning approach for complex problem solving. It uses a state machine architecture to generate plans, reflect on them, and formalize the final solution.</p>"},{"location":"planning_agent/#basic-usage","title":"Basic Usage","text":"<pre><code>from ursa.agents import PlanningAgent\n\n# Initialize the agent\nagent = PlanningAgent()\n\n# Run a planning task\nresult = agent.invoke(\"Find a city with at least 10 vowels in its name.\")\n\n# Access the final plan\nplan_steps = result[\"plan_steps\"]\n</code></pre>"},{"location":"planning_agent/#parameters","title":"Parameters","text":"<p>When initializing <code>PlanningAgent</code>, you can customize its behavior with these parameters:</p> Parameter Type Default Description <code>llm</code> BaseChatModel <code>init_chat_model(\"openai:gpt-5-mini\")</code> The LLM model to use for planning <code>**kwargs</code> <code>dict</code> <code>{}</code> Additional parameters passed to the base agent"},{"location":"planning_agent/#features","title":"Features","text":""},{"location":"planning_agent/#multi-step-planning","title":"Multi-step Planning","text":"<p>The agent follows a three-stage planning process:</p> <ol> <li>Generation: Creates an initial plan to solve the problem</li> <li>Reflection: Critically evaluates and improves the plan</li> <li>Formalization: Structures the final plan as a JSON object</li> </ol>"},{"location":"planning_agent/#structured-output","title":"Structured Output","text":"<p>The final output includes:</p> <ul> <li><code>messages</code>: The conversation history</li> <li><code>plan_steps</code>: A structured list of steps to solve the problem</li> </ul>"},{"location":"planning_agent/#advanced-usage","title":"Advanced Usage","text":""},{"location":"planning_agent/#customizing-reflection-steps","title":"Customizing Reflection Steps","text":"<p>You can adjust how many reflection iterations the agent performs:</p> <pre><code># Initialize with custom reflection steps\ninitial_state = {\n    \"messages\": [HumanMessage(content=\"Your complex problem here\")],\n    \"reflection_steps\": 5  # Default is 3\n}\n\nresult = agent.invoke(initial_state, {\"configurable\": {\"thread_id\": agent.thread_id}})\n</code></pre>"},{"location":"planning_agent/#streaming-results","title":"Streaming Results","text":"<p>You can stream the agent's thinking process:</p> <pre><code>for event in agent.stream(\n    {\"messages\": [HumanMessage(content=\"Your problem here\")]},\n    {\"configurable\": {\"thread_id\": agent.thread_id}}\n):\n    print(event[list(event.keys())[0]][\"messages\"][-1].text)\n</code></pre>"},{"location":"planning_agent/#setting-a-recursion-limit","title":"Setting a Recursion Limit","text":"<p>For complex planning tasks, you may need to adjust the recursion limit:</p> <pre><code>result = agent.invoke(\n    \"Solve this complex problem...\", \n    recursion_limit=200  # Default is 100\n)\n</code></pre>"},{"location":"planning_agent/#how-it-works","title":"How It Works","text":"<ol> <li>State Machine: The agent uses a directed graph to manage its workflow:</li> <li><code>generate</code> node: Creates or improves the plan</li> <li><code>reflect</code> node: Evaluates the plan for improvements</li> <li> <p><code>formalize</code> node: Structures the final plan as JSON</p> </li> <li> <p>Termination Conditions: The planning process ends when either:</p> </li> <li>The agent has completed the specified number of reflection steps</li> <li> <p>The agent explicitly marks the plan as \"[APPROVED]\"</p> </li> <li> <p>JSON Output: The final plan is structured as a JSON array of steps, each containing:</p> </li> <li>A description of the step</li> <li>Any relevant details for executing that step</li> </ol>"},{"location":"planning_agent/#notes","title":"Notes","text":"<ul> <li>The agent continues to refine its plan through multiple reflection cycles</li> <li>The final output is a structured JSON representation of the solution steps</li> <li>You can access the complete conversation history in the <code>messages</code> field of the result</li> </ul>"},{"location":"web_search_agent/","title":"WebSearchAgent Documentation","text":"<p><code>WebSearchAgent</code> is a powerful tool for conducting internet-based research on any topic. It leverages language models and web search capabilities to gather, process, and summarize information from online sources.</p>"},{"location":"web_search_agent/#basic-usage","title":"Basic Usage","text":"<pre><code>from ursa.agents import WebSearchAgent\nfrom langchain_openai import ChatOpenAI\n\n# Initialize with default model (gpt-5-mini)\nwebsearcher = WebSearchAgent()\n\n# Or initialize with a custom model\nmodel = ChatOpenAI(model=\"gpt-5-mini\", max_completion_tokens=10000)\nwebsearcher = WebSearchAgent(llm=model)\n\n# Run a web search query\nresult = websearcher.invoke(\"Who are the 2025 Detroit Tigers top 10 prospects and what year were they born?\")\n\n# Access the web search results\nsources = result[\"urls_visited\"]\n\nprint(\"Web Search Summary:\")\nprint(result[\"final_summary\"])\nprint(\"Sources:\", sources)\n</code></pre>"},{"location":"web_search_agent/#parameters","title":"Parameters","text":""},{"location":"web_search_agent/#initialization","title":"Initialization","text":"Parameter Type Default Description <code>llm</code> <code>BaseChatModel</code> init_chat_model(\"openai:gpt-5-mini\") The language model to use for web search <code>**kwargs</code> <code>dict</code> <code>{}</code> Additional parameters passed to the base agent"},{"location":"web_search_agent/#run-method","title":"Run Method","text":"Parameter Type Default Description <code>prompt</code> str Required The web search question or topic <code>recursion_limit</code> int 100 Maximum recursion depth for the web search process"},{"location":"web_search_agent/#features","title":"Features","text":"<ul> <li>Automated Web Search: Uses DuckDuckGo to find relevant information</li> <li>Content Processing: Extracts and summarizes content from web pages</li> <li>Iterative Web Search: Continues researching until sufficient information is gathered</li> <li>Source Tracking: Records all URLs visited during research</li> <li>Internet Connectivity Check: Verifies internet access before attempting research</li> </ul>"},{"location":"web_search_agent/#output","title":"Output","text":"<p>The agent returns a dictionary containing:</p> <ul> <li><code>messages</code>: A list of message objects, with the final message containing the comprehensive web search summary</li> <li><code>urls_visited</code>: A list of all sources consulted during the web search process</li> </ul>"},{"location":"web_search_agent/#advanced-usage","title":"Advanced Usage","text":"<pre><code>from langchain.chat_model import init_chat_model\nfrom ursa.agents import WebSearchAgent\n\n# Initialize with custom parameters\nwebsearcher = WebSearchAgent(\n    llm=init_chat_model(\"openai:gpt-5-mini\"),\n    url=\"https://www.example.com\"  # Custom URL for internet connectivity check\n)\n\n# Run with higher recursion limit for complex topics\nresult = websearcher.invoke(\n    \"What are the latest developments in quantum computing? Summarize in markdown format.\",\n    recursion_limit=200\n)\n</code></pre>"},{"location":"web_search_agent/#notes","title":"Notes","text":"<ul> <li>The agent requires internet connectivity to function properly</li> <li>Rate limiting is implemented to avoid overwhelming search services</li> <li>For networks with SSL inspection, you may need to set the <code>CERT_FILE</code> environment variable</li> <li>The websearch process includes multiple steps: search, content processing, review, and final summarization</li> </ul>"},{"location":"api_reference/agents/","title":"agents","text":""},{"location":"api_reference/agents/#ursa.agents.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Dynamically import attributes on first access.</p> <p>This avoids importing all agent modules at package import time, so a failure in one agent does not prevent using others.</p> Source code in <code>src/ursa/agents/__init__.py</code> <pre><code>def __getattr__(name: str) -&gt; Any:\n    \"\"\"Dynamically import attributes on first access.\n\n    This avoids importing all agent modules at package import time,\n    so a failure in one agent does not prevent using others.\n    \"\"\"\n    try:\n        module_name, attr_name = _lazy_attrs[name]\n    except KeyError:\n        raise AttributeError(\n            f\"module {__name__!r} has no attribute {name!r}\"\n        ) from None\n\n    module = importlib.import_module(module_name, __name__)\n    value = getattr(module, attr_name)\n    # Cache the loaded attribute so subsequent access is fast\n    globals()[name] = value\n    return value\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.acquisition_agents","title":"<code>acquisition_agents</code>","text":""},{"location":"api_reference/agents/#ursa.agents.acquisition_agents.ArxivAgent","title":"<code>ArxivAgent</code>","text":"<p>               Bases: <code>BaseAcquisitionAgent</code></p> <p>Drop-in replacement for your existing ArxivAgent that reuses the generic flow. Keeps the same behaviors (download PDFs, image processing, summarization/RAG).</p> Source code in <code>src/ursa/agents/acquisition_agents.py</code> <pre><code>class ArxivAgent(BaseAcquisitionAgent):\n    \"\"\"\n    Drop-in replacement for your existing ArxivAgent that reuses the generic flow.\n    Keeps the same behaviors (download PDFs, image processing, summarization/RAG).\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: BaseChatModel,\n        *,\n        process_images: bool = True,\n        max_results: int = 3,\n        download: bool = True,\n        rag_embedding=None,\n        database_path=\"arxiv_papers\",\n        summaries_path=\"arxiv_generated_summaries\",\n        vectorstore_path=\"arxiv_vectorstores\",\n        **kwargs,\n    ):\n        super().__init__(\n            llm,\n            rag_embedding=rag_embedding,\n            process_images=process_images,\n            max_results=max_results,\n            database_path=database_path,\n            summaries_path=summaries_path,\n            vectorstore_path=vectorstore_path,\n            download=download,\n            **kwargs,\n        )\n\n    def _id(self, hit_or_item: dict[str, Any]) -&gt; str:\n        # hits from arXiv feed have 'id' like \".../abs/XXXX.YYYY\"\n        arxiv_id = hit_or_item.get(\"arxiv_id\")\n        if arxiv_id:\n            return arxiv_id\n        feed_id = hit_or_item.get(\"id\", \"\")\n        if \"/abs/\" in feed_id:\n            return feed_id.split(\"/abs/\")[-1]\n        return _hash(json.dumps(hit_or_item))\n\n    def _citation(self, item: ItemMetadata) -&gt; str:\n        return f\"ArXiv ID: {item.get('id', '?')}\"\n\n    def _search(self, query: str) -&gt; list[dict[str, Any]]:\n        enc = quote(query)\n        url = f\"http://export.arxiv.org/api/query?search_query=all:{enc}&amp;start=0&amp;max_results={self.max_results}\"\n        try:\n            resp = requests.get(url, timeout=15)\n            resp.raise_for_status()\n            feed = feedparser.parse(resp.content)\n            entries = feed.entries if hasattr(feed, \"entries\") else []\n            hits = []\n            for e in entries:\n                full_id = e.id.split(\"/abs/\")[-1]\n                hits.append({\n                    \"id\": e.id,\n                    \"title\": e.title.strip(),\n                    \"arxiv_id\": full_id.split(\"/\")[-1],\n                })\n            return hits\n        except Exception as e:\n            return [\n                {\n                    \"id\": _hash(query + str(time.time())),\n                    \"title\": \"Search error\",\n                    \"error\": str(e),\n                }\n            ]\n\n    def _materialize(self, hit: dict[str, Any]) -&gt; ItemMetadata:\n        arxiv_id = self._id(hit)\n        title = hit.get(\"title\", \"\")\n        pdf_url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n        local_path = os.path.join(self.database_path, f\"{arxiv_id}.pdf\")\n        full_text = \"\"\n        try:\n            _download(pdf_url, local_path)\n            full_text = read_pdf(local_path)\n        except Exception as e:\n            full_text = f\"[Error loading ArXiv {arxiv_id}: {e}]\"\n        full_text = self._postprocess_text(full_text, local_path)\n        return {\n            \"id\": arxiv_id,\n            \"title\": title,\n            \"url\": pdf_url,\n            \"local_path\": local_path,\n            \"full_text\": full_text,\n        }\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.acquisition_agents.BaseAcquisitionAgent","title":"<code>BaseAcquisitionAgent</code>","text":"<p>               Bases: <code>BaseAgent</code></p> <p>A generic \"acquire-then-summarize-or-RAG\" agent.</p> Subclasses must implement <ul> <li>_search(self, query) -&gt; List[dict-like]: lightweight hits</li> <li>_materialize(self, hit) -&gt; ItemMetadata: download or scrape and return populated item</li> <li>_id(self, hit_or_item) -&gt; str: stable id for caching/file naming</li> <li>_citation(self, item) -&gt; str: human-readable citation string</li> </ul> Optional hooks <ul> <li>_postprocess_text(self, text, local_path) -&gt; str (e.g., image interpretation)</li> <li>_filter_hit(self, hit) -&gt; bool</li> </ul> Source code in <code>src/ursa/agents/acquisition_agents.py</code> <pre><code>class BaseAcquisitionAgent(BaseAgent):\n    \"\"\"\n    A generic \"acquire-then-summarize-or-RAG\" agent.\n\n    Subclasses must implement:\n      - _search(self, query) -&gt; List[dict-like]: lightweight hits\n      - _materialize(self, hit) -&gt; ItemMetadata: download or scrape and return populated item\n      - _id(self, hit_or_item) -&gt; str: stable id for caching/file naming\n      - _citation(self, item) -&gt; str: human-readable citation string\n\n    Optional hooks:\n      - _postprocess_text(self, text, local_path) -&gt; str (e.g., image interpretation)\n      - _filter_hit(self, hit) -&gt; bool\n    \"\"\"\n\n    def __init__(\n        self,\n        llm: BaseChatModel,\n        *,\n        summarize: bool = True,\n        rag_embedding=None,\n        process_images: bool = True,\n        max_results: int = 5,\n        database_path: str = \"acq_db\",\n        summaries_path: str = \"acq_summaries\",\n        vectorstore_path: str = \"acq_vectorstores\",\n        num_threads: int = 4,\n        download: bool = True,\n        **kwargs,\n    ):\n        super().__init__(llm, **kwargs)\n        self.summarize = summarize\n        self.rag_embedding = rag_embedding\n        self.process_images = process_images\n        self.max_results = max_results\n        self.database_path = self.workspace / database_path\n        self.summaries_path = self.workspace / summaries_path\n        self.vectorstore_path = self.workspace / vectorstore_path\n        self.download = download\n        self.num_threads = num_threads\n\n        self.database_path.mkdir(exist_ok=True, parents=True)\n        self.summaries_path.mkdir(exist_ok=True, parents=True)\n\n    # ---- abstract-ish methods ----\n    def _search(self, query: str) -&gt; list[dict[str, Any]]:\n        raise NotImplementedError\n\n    def _materialize(self, hit: dict[str, Any]) -&gt; ItemMetadata:\n        raise NotImplementedError\n\n    def _id(self, hit_or_item: dict[str, Any]) -&gt; str:\n        raise NotImplementedError\n\n    def _citation(self, item: ItemMetadata) -&gt; str:\n        # Subclass should format its ideal citation; fallback is ID or URL.\n        return item.get(\"id\") or item.get(\"url\", \"Unknown Source\")\n\n    # ---- optional hooks ----\n    def _filter_hit(self, hit: dict[str, Any]) -&gt; bool:\n        return True\n\n    def _postprocess_text(self, text: str, local_path: Optional[str]) -&gt; str:\n        # Default: optionally add image descriptions for PDFs\n        if (\n            self.process_images\n            and local_path\n            and local_path.lower().endswith(\".pdf\")\n        ):\n            try:\n                descs = extract_and_describe_images(local_path)\n                if any(descs):\n                    text += \"\\n\\n[Image Interpretations]\\n\" + \"\\n\".join(descs)\n            except Exception:\n                pass\n        return text\n\n    # ---- shared nodes ----\n    def _fetch_items(self, query: str) -&gt; list[ItemMetadata]:\n        hits = self._search(query)[: self.max_results] if self.download else []\n        items: list[ItemMetadata] = []\n\n        # If not downloading/scraping, try to load whatever is cached in database_path.\n        if not self.download:\n            for fname in os.listdir(self.database_path):\n                if fname.lower().endswith((\".pdf\", \".txt\", \".html\")):\n                    item_id = os.path.splitext(fname)[0]\n                    local_path = os.path.join(self.database_path, fname)\n                    full_text = \"\"\n                    try:\n                        if fname.lower().endswith(\".pdf\"):\n                            full_text = read_pdf(local_path)\n                        else:\n                            with open(\n                                local_path,\n                                \"r\",\n                                encoding=\"utf-8\",\n                                errors=\"ignore\",\n                            ) as f:\n                                full_text = f.read()\n                    except Exception as e:\n                        full_text = f\"[Error reading cached file: {e}]\"\n                    full_text = self._postprocess_text(full_text, local_path)\n                    items.append({\n                        \"id\": item_id,\n                        \"local_path\": local_path,\n                        \"full_text\": full_text,\n                    })\n            return items\n\n        # Normal path: search \u2192 materialize each\n        with ThreadPoolExecutor(\n            max_workers=min(self.num_threads, max(1, len(hits)))\n        ) as ex:\n            futures = [\n                ex.submit(self._materialize, h)\n                for h in hits\n                if self._filter_hit(h)\n            ]\n            for fut in as_completed(futures):\n                try:\n                    item = fut.result()\n                    items.append(item)\n                except Exception as e:\n                    items.append({\n                        \"id\": _hash(str(time.time())),\n                        \"full_text\": f\"[Error: {e}]\",\n                    })\n        return items\n\n    def _normalize_inputs(self, inputs) -&gt; AcquisitionState:\n        if isinstance(inputs, str):\n            return AcquisitionState(context=inputs)\n        elif isinstance(inputs, dict):\n            return inputs\n        else:\n            raise TypeError(f\"Invalid input for {self.__class__.__name__}\")\n\n    def format_result(self, state: AcquisitionState) -&gt; str:\n        if summary := state[\"final_summary\"]:\n            return summary\n\n        # Fallback to dumping an empty string if `self.summarize=False`.\n        # This only happens if a user disables summarization when configuring\n        # URSA or if the final_summary is an empty string itself.\n        return \"\"\n\n    async def _search_query(self, state: AcquisitionState) -&gt; AcquisitionState:\n        \"\"\"Generate a search query from the input search task (context)\"\"\"\n        existing = state.get(\"query\")\n        if existing:\n            return state\n\n        context = state[\"context\"]\n        query = await self.llm.ainvoke(\n            f\"The user stated {context}. Generate between 1 and 8 words for a search query to address the users need. Return only the words to search.\"\n        )\n        state[\"query\"] = query.content or context\n        return state\n\n    def _fetch_node(self, state: AcquisitionState) -&gt; AcquisitionState:\n        items = self._fetch_items(state[\"query\"])\n        return {**state, \"items\": items}\n\n    def _summarize_node(self, state: AcquisitionState) -&gt; AcquisitionState:\n        prompt = ChatPromptTemplate.from_template(\"\"\"\n        You are an assistant responsible for summarizing retrieved content in the context of this task: {context}\n\n        Summarize the content below:\n\n        {retrieved_content}\n        \"\"\")\n        chain = prompt | self.llm | StrOutputParser()\n\n        if \"items\" not in state or not state[\"items\"]:\n            return {**state, \"summaries\": None}\n\n        summaries: list[Optional[str]] = [None] * len(state[\"items\"])\n\n        def process(i: int, item: ItemMetadata):\n            item_id = item.get(\"id\", f\"item_{i}\")\n            out_path = os.path.join(\n                self.summaries_path, f\"{_safe_filename(item_id)}_summary.txt\"\n            )\n            try:\n                cleaned = remove_surrogates(item.get(\"full_text\", \"\"))\n                summary = chain.invoke(\n                    {\"retrieved_content\": cleaned, \"context\": state[\"context\"]},\n                    config=self.build_config(tags=[\"acq\", \"summarize_each\"]),\n                )\n            except Exception as e:\n                summary = f\"[Error summarizing item {item_id}: {e}]\"\n            with open(out_path, \"w\", encoding=\"utf-8\") as f:\n                f.write(summary)\n            return i, summary\n\n        with ThreadPoolExecutor(\n            max_workers=min(self.num_threads, len(state[\"items\"]))\n        ) as ex:\n            futures = [\n                ex.submit(process, i, it) for i, it in enumerate(state[\"items\"])\n            ]\n            for fut in as_completed(futures):\n                i, s = fut.result()\n                summaries[i] = s\n\n        return {**state, \"summaries\": summaries}  # type: ignore\n\n    def _rag_node(self, state: AcquisitionState) -&gt; AcquisitionState:\n        new_state = state.copy()\n        rag_agent = RAGAgent(\n            llm=self.llm,\n            workspace=self.workspace,\n            embedding=self.rag_embedding,\n            vectorstore_path=\"rag_vectorstore\",\n            database_path=self.database_path.name,\n        )\n        new_state[\"final_summary\"] = rag_agent.invoke(context=state[\"context\"])[\n            \"summary\"\n        ]\n        return new_state\n\n    def _aggregate_node(self, state: AcquisitionState) -&gt; AcquisitionState:\n        if not state.get(\"summaries\") or not state.get(\"items\"):\n            return {**state, \"final_summary\": None}\n\n        blocks: list[str] = []\n        for idx, (item, summ) in enumerate(\n            zip(state[\"items\"], state[\"summaries\"])\n        ):  # type: ignore\n            cite = self._citation(item)\n            blocks.append(f\"[{idx + 1}] {cite}\\n\\nSummary:\\n{summ}\")\n\n        combined = \"\\n\\n\" + (\"\\n\\n\" + \"-\" * 40 + \"\\n\\n\").join(blocks)\n        with open(\n            os.path.join(self.summaries_path, \"summaries_combined.txt\"),\n            \"w\",\n            encoding=\"utf-8\",\n        ) as f:\n            f.write(combined)\n\n        prompt = ChatPromptTemplate.from_template(\"\"\"\n        You are a scientific assistant extracting insights from multiple summaries.\n\n        Here are the summaries:\n\n        {Summaries}\n\n        Your task is to read all the summaries and provide a response to this task: {context}\n        \"\"\")\n        chain = prompt | self.llm | StrOutputParser()\n\n        final_summary = chain.invoke(\n            {\"Summaries\": combined, \"context\": state[\"context\"]},\n            config=self.build_config(tags=[\"acq\", \"aggregate\"]),\n        )\n        with open(\n            os.path.join(self.summaries_path, \"final_summary.txt\"),\n            \"w\",\n            encoding=\"utf-8\",\n        ) as f:\n            f.write(final_summary)\n\n        return {**state, \"final_summary\": final_summary}\n\n    def _build_graph(self):\n        self.add_node(self._search_query)\n        self.graph.set_entry_point(\"_search_query\")\n        self.add_node(self._fetch_node)\n        self.graph.add_edge(\"_search_query\", \"_fetch_node\")\n\n        if self.summarize:\n            if self.rag_embedding:\n                self.add_node(self._rag_node)\n                self.graph.add_edge(\"_fetch_node\", \"_rag_node\")\n                self.graph.set_finish_point(\"_rag_node\")\n            else:\n                self.add_node(self._summarize_node)\n                self.add_node(self._aggregate_node)\n\n                self.graph.add_edge(\"_fetch_node\", \"_summarize_node\")\n                self.graph.add_edge(\"_summarize_node\", \"_aggregate_node\")\n                self.graph.set_finish_point(\"_aggregate_node\")\n        else:\n            self.graph.set_finish_point(\"_fetch_node\")\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.acquisition_agents.OSTIAgent","title":"<code>OSTIAgent</code>","text":"<p>               Bases: <code>BaseAcquisitionAgent</code></p> <p>Minimal OSTI.gov acquisition agent.</p> NOTE <ul> <li>OSTI provides search endpoints that can return metadata including full-text links.</li> <li>Depending on your environment, you may prefer the public API or site scraping.</li> <li>Here we assume a JSON API that yields results with keys like:       {'osti_id': '12345', 'title': '...', 'pdf_url': 'https://...pdf', 'landing_page': 'https://...'}   Adapt field names if your OSTI integration differs.</li> </ul> <p>Customize <code>_search</code> and <code>_materialize</code> to match your OSTI access path.</p> Source code in <code>src/ursa/agents/acquisition_agents.py</code> <pre><code>class OSTIAgent(BaseAcquisitionAgent):\n    \"\"\"\n    Minimal OSTI.gov acquisition agent.\n\n    NOTE:\n      - OSTI provides search endpoints that can return metadata including full-text links.\n      - Depending on your environment, you may prefer the public API or site scraping.\n      - Here we assume a JSON API that yields results with keys like:\n            {'osti_id': '12345', 'title': '...', 'pdf_url': 'https://...pdf', 'landing_page': 'https://...'}\n        Adapt field names if your OSTI integration differs.\n\n    Customize `_search` and `_materialize` to match your OSTI access path.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args,\n        api_base: str = \"https://www.osti.gov/api/v1/records\",\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.api_base = api_base\n\n    def _id(self, hit_or_item: dict[str, Any]) -&gt; str:\n        if \"osti_id\" in hit_or_item:\n            return str(hit_or_item[\"osti_id\"])\n        if \"id\" in hit_or_item:\n            return str(hit_or_item[\"id\"])\n        if \"landing_page\" in hit_or_item:\n            return _hash(hit_or_item[\"landing_page\"])\n        return _hash(json.dumps(hit_or_item))\n\n    def _citation(self, item: ItemMetadata) -&gt; str:\n        t = item.get(\"title\", \"\") or \"\"\n        oid = item.get(\"id\", \"\")\n        return f\"OSTI {oid}: {t}\" if t else f\"OSTI {oid}\"\n\n    def _search(self, query: str) -&gt; list[dict[str, Any]]:\n        \"\"\"\n        Adjust params to your OSTI setup. This call is intentionally simple;\n        add paging/auth as needed.\n        \"\"\"\n        params = {\n            \"q\": query,\n            \"size\": self.max_results,\n        }\n        try:\n            r = requests.get(self.api_base, params=params, timeout=25)\n            r.raise_for_status()\n            data = r.json()\n            # Normalize to a list of hits; adapt key if your API differs.\n            if isinstance(data, dict) and \"records\" in data:\n                hits = data[\"records\"]\n            elif isinstance(data, list):\n                hits = data\n            else:\n                hits = []\n            return hits[: self.max_results]\n        except Exception as e:\n            return [\n                {\n                    \"id\": _hash(query + str(time.time())),\n                    \"title\": \"Search error\",\n                    \"error\": str(e),\n                }\n            ]\n\n    def _materialize(self, hit: dict[str, Any]) -&gt; ItemMetadata:\n        item_id = self._id(hit)\n        title = hit.get(\"title\") or hit.get(\"title_public\", \"\") or \"\"\n        landing = None\n        local_path = \"\"\n        full_text = \"\"\n\n        try:\n            pdf_url, landing_used, _ = resolve_pdf_from_osti_record(\n                hit,\n                headers={\"User-Agent\": \"Mozilla/5.0\"},\n                unpaywall_email=os.environ.get(\"UNPAYWALL_EMAIL\"),  # optional\n            )\n\n            if pdf_url:\n                # Try to download as PDF (validate headers)\n                with requests.get(\n                    pdf_url,\n                    headers={\"User-Agent\": \"Mozilla/5.0\"},\n                    timeout=25,\n                    allow_redirects=True,\n                    stream=True,\n                ) as r:\n                    r.raise_for_status()\n                    if _is_pdf_response(r):\n                        fname = _derive_filename_from_cd_or_url(\n                            r, f\"osti_{item_id}.pdf\"\n                        )\n                        local_path = os.path.join(self.database_path, fname)\n                        _download_stream_to(local_path, r)\n                        # Extract PDF text\n                        try:\n                            full_text = read_pdf(local_path)\n                        except Exception as e:\n                            full_text = (\n                                f\"[Downloaded but text extraction failed: {e}]\"\n                            )\n                    else:\n                        # Not a PDF; treat as HTML landing and parse text\n                        landing = r.url\n                        r.close()\n            # If we still have no text, try scraping the DOE PAGES landing or citation page\n            if not full_text:\n                # Prefer DOE PAGES landing if present, else OSTI biblio\n                landing = (\n                    landing\n                    or landing_used\n                    or next(\n                        (\n                            link.get(\"href\")\n                            for link in hit.get(\"links\", [])\n                            if link.get(\"rel\")\n                            in (\"citation_doe_pages\", \"citation\")\n                        ),\n                        None,\n                    )\n                )\n                if landing:\n                    soup = _get_soup(\n                        landing,\n                        timeout=25,\n                        headers={\"User-Agent\": \"Mozilla/5.0\"},\n                    )\n                    html_text = soup.get_text(\" \", strip=True)\n                    full_text = html_text[:1_000_000]  # keep it bounded\n                    # Save raw HTML for cache/inspection\n                    local_path = os.path.join(\n                        self.database_path, f\"{item_id}.html\"\n                    )\n                    with open(local_path, \"w\", encoding=\"utf-8\") as f:\n                        f.write(str(soup))\n                else:\n                    full_text = \"[No PDF or landing page text available.]\"\n\n        except Exception as e:\n            full_text = f\"[Error materializing OSTI {item_id}: {e}]\"\n\n        full_text = self._postprocess_text(full_text, local_path)\n        return {\n            \"id\": item_id,\n            \"title\": title,\n            \"url\": landing,\n            \"local_path\": local_path,\n            \"full_text\": full_text,\n            \"extra\": {\"raw_hit\": hit},\n        }\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.acquisition_agents.WebSearchAgent","title":"<code>WebSearchAgent</code>","text":"<p>               Bases: <code>BaseAcquisitionAgent</code></p> <p>Uses DuckDuckGo Search (ddgs) to find pages, downloads HTML or PDFs, extracts text, and then follows the same summarize/RAG path.</p> Source code in <code>src/ursa/agents/acquisition_agents.py</code> <pre><code>class WebSearchAgent(BaseAcquisitionAgent):\n    \"\"\"\n    Uses DuckDuckGo Search (ddgs) to find pages, downloads HTML or PDFs,\n    extracts text, and then follows the same summarize/RAG path.\n    \"\"\"\n\n    def __init__(self, *args, user_agent: str = \"Mozilla/5.0\", **kwargs):\n        super().__init__(*args, **kwargs)\n        self.user_agent = user_agent\n        if DDGS is None:\n            raise ImportError(\n                \"duckduckgo-search (DDGS) is required for WebSearchAgentGeneric.\"\n            )\n\n    def _id(self, hit_or_item: dict[str, Any]) -&gt; str:\n        url = hit_or_item.get(\"href\") or hit_or_item.get(\"url\") or \"\"\n        return (\n            _hash(url)\n            if url\n            else hit_or_item.get(\"id\", _hash(json.dumps(hit_or_item)))\n        )\n\n    def _citation(self, item: ItemMetadata) -&gt; str:\n        t = item.get(\"title\", \"\") or \"\"\n        u = item.get(\"url\", \"\") or \"\"\n        return f\"{t} ({u})\" if t else (u or item.get(\"id\", \"Web result\"))\n\n    def _search(self, query: str) -&gt; list[dict[str, Any]]:\n        results: list[dict[str, Any]] = []\n        with DDGS() as ddgs:\n            for r in ddgs.text(\n                query, max_results=self.max_results, backend=\"auto\"\n            ):\n                # r keys typically: title, href, body\n                results.append(r)\n        return results\n\n    def _materialize(self, hit: dict[str, Any]) -&gt; ItemMetadata:\n        url = hit.get(\"href\") or hit.get(\"url\")\n        title = hit.get(\"title\", \"\")\n        if not url:\n            return {\"id\": self._id(hit), \"title\": title, \"full_text\": \"\"}\n\n        headers = {\"User-Agent\": self.user_agent}\n        local_path = \"\"\n        full_text = \"\"\n        item_id = self._id(hit)\n\n        try:\n            if _looks_like_pdf_url(url):\n                local_path = os.path.join(\n                    self.database_path, _safe_filename(item_id) + \".pdf\"\n                )\n                _download(url, local_path)\n                full_text = read_pdf(local_path)\n            else:\n                r = requests.get(url, headers=headers, timeout=20)\n                r.raise_for_status()\n                html = r.text\n                local_path = os.path.join(\n                    self.database_path, _safe_filename(item_id) + \".html\"\n                )\n                with open(local_path, \"w\", encoding=\"utf-8\") as f:\n                    f.write(html)\n                full_text = extract_main_text_only(html)\n                # full_text = _basic_readable_text_from_html(html)\n        except Exception as e:\n            full_text = f\"[Error retrieving {url}: {e}]\"\n\n        full_text = self._postprocess_text(full_text, local_path)\n        return {\n            \"id\": item_id,\n            \"title\": title,\n            \"url\": url,\n            \"local_path\": local_path,\n            \"full_text\": full_text,\n            \"extra\": {\"snippet\": hit.get(\"body\", \"\")},\n        }\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base","title":"<code>base</code>","text":"<p>Base agent class providing telemetry, configuration, and execution abstractions.</p> <p>This module defines the BaseAgent abstract class, which serves as the foundation for all agent implementations in the Ursa framework. It provides:</p> <ul> <li>Standardized initialization with LLM configuration</li> <li>Telemetry and metrics collection</li> <li>Thread and checkpoint management</li> <li>Input normalization and validation</li> <li>Execution flow control with invoke/stream methods</li> <li>Graph integration utilities for LangGraph compatibility</li> <li>Runtime enforcement of the agent interface contract</li> </ul> <p>Agents built on this base class benefit from consistent behavior, observability, and integration capabilities while only needing to implement the core _invoke method.</p>"},{"location":"api_reference/agents/#ursa.agents.base.AgentContext","title":"<code>AgentContext</code>  <code>dataclass</code>","text":"<p>Immutable context provided during graph execution</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>@dataclass(frozen=True, kw_only=True)\nclass AgentContext:\n    \"\"\"Immutable context provided during graph execution\"\"\"\n\n    llm: BaseChatModel\n    \"\"\" Chat model for use during tool calls \"\"\"\n\n    workspace: Path\n    \"\"\" Workspace path for the agent \"\"\"\n\n    tool_character_limit: int = 30000\n    \"\"\" Suggested limit on tool call responses \"\"\"\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.AgentContext.llm","title":"<code>llm</code>  <code>instance-attribute</code>","text":"<p>Chat model for use during tool calls</p>"},{"location":"api_reference/agents/#ursa.agents.base.AgentContext.tool_character_limit","title":"<code>tool_character_limit = 30000</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Suggested limit on tool call responses</p>"},{"location":"api_reference/agents/#ursa.agents.base.AgentContext.workspace","title":"<code>workspace</code>  <code>instance-attribute</code>","text":"<p>Workspace path for the agent</p>"},{"location":"api_reference/agents/#ursa.agents.base.AgentWithTools","title":"<code>AgentWithTools</code>","text":"<p>Mixin that equips an agent with LangGraph tools management.</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>class AgentWithTools:\n    \"\"\"Mixin that equips an agent with LangGraph tools management.\"\"\"\n\n    def __init__(\n        self,\n        *args,\n        tools: list[BaseTool] | dict[str, BaseTool] | None = None,\n        handle_tool_errors=_default_tool_error_handler,\n        **kwargs,\n    ):\n        self._tools: dict[str, BaseTool] = {}\n        self.handle_tool_errors = handle_tool_errors\n        self.tool_node = ToolNode([])\n        self._apply_tools(tools, rebuild_graph=False)\n        super().__init__(*args, **kwargs)\n\n    def __post_init__(self):\n        super().__post_init__()\n\n    @property\n    def tools(self) -&gt; dict[str, BaseTool]:\n        return dict(self._tools)\n\n    @tools.setter\n    def tools(self, tools: dict[str, BaseTool] | list[BaseTool] | None):\n        self._apply_tools(tools)\n\n    def add_tool(self, tools: BaseTool | list[BaseTool]) -&gt; None:\n        bundle = tools if isinstance(tools, list) else [tools]\n        merged = dict(self._tools)\n        merged.update({tool.name: tool for tool in bundle})\n        self._apply_tools(merged)\n\n    async def add_mcp_tools(\n        self,\n        client: MultiServerMCPClient,\n        tool_name: None | str | list[str] = None,\n    ) -&gt; None:\n        \"\"\"Add tools from an MCP client to the agent\n\n        Args:\n           client: the MCP client to add tools from\n           tool_name: if provided, only add named tools\n        \"\"\"\n        tools = await client.get_tools()\n        if tool_name is not None:\n            tool_name = (\n                tool_name if isinstance(tool_name, list) else [tool_name]\n            )\n            tools = [tool for tool in tools if tool.name in tool_name]\n        self.add_tool(tools)\n\n    def remove_tool(self, tool_names: str | list[str]) -&gt; None:\n        names = tool_names if isinstance(tool_names, list) else [tool_names]\n        trimmed = {\n            name: tool\n            for name, tool in self._tools.items()\n            if name not in names\n        }\n        self._apply_tools(trimmed)\n\n    def _apply_tools(\n        self,\n        tools: dict[str, BaseTool] | list[BaseTool] | None,\n        *,\n        rebuild_graph: bool = True,\n    ) -&gt; None:\n        if tools is None:\n            mapping: dict[str, BaseTool] = {}\n        elif isinstance(tools, dict):\n            mapping = dict(tools)\n        else:\n            mapping = {tool.name: tool for tool in tools}\n\n        self._tools = mapping\n        self.tool_node = ToolNode(\n            list(self._tools.values()),\n            handle_tool_errors=self.handle_tool_errors,\n        )\n\n        if rebuild_graph and hasattr(self, \"build_graph\"):\n            self.__dict__.pop(\"compiled_graph\", None)\n            if hasattr(self, \"graph\"):\n                self.build_graph()\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.AgentWithTools.add_mcp_tools","title":"<code>add_mcp_tools(client, tool_name=None)</code>  <code>async</code>","text":"<p>Add tools from an MCP client to the agent</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>MultiServerMCPClient</code> <p>the MCP client to add tools from</p> required <code>tool_name</code> <code>None | str | list[str]</code> <p>if provided, only add named tools</p> <code>None</code> Source code in <code>src/ursa/agents/base.py</code> <pre><code>async def add_mcp_tools(\n    self,\n    client: MultiServerMCPClient,\n    tool_name: None | str | list[str] = None,\n) -&gt; None:\n    \"\"\"Add tools from an MCP client to the agent\n\n    Args:\n       client: the MCP client to add tools from\n       tool_name: if provided, only add named tools\n    \"\"\"\n    tools = await client.get_tools()\n    if tool_name is not None:\n        tool_name = (\n            tool_name if isinstance(tool_name, list) else [tool_name]\n        )\n        tools = [tool for tool in tools if tool.name in tool_name]\n    self.add_tool(tools)\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent","title":"<code>BaseAgent</code>","text":"<p>               Bases: <code>Generic[TState]</code>, <code>ABC</code></p> <p>Abstract base class for all agent implementations in the Ursa framework.</p> <p>BaseAgent provides a standardized foundation for building LLM-powered agents with built-in telemetry, configuration management, and execution flow control. It handles common tasks like input normalization, thread management, metrics collection, and LangGraph integration.</p> <p>Subclasses only need to implement the _invoke method to define their core functionality, while inheriting standardized invocation patterns, telemetry, and graph integration capabilities. The class enforces a consistent interface through runtime checks that prevent subclasses from overriding critical methods like invoke().</p> <p>The agent supports both direct invocation with inputs and streaming responses, with automatic tracking of token usage, execution time, and other metrics. It also provides utilities for integrating with LangGraph through node wrapping and configuration.</p> Subclass Inheritance Guidelines <ul> <li>Must Override: _invoke() - Define your agent's core functionality</li> <li>Can Override: _stream() - Enable streaming support                 _normalize_inputs() - Customize input handling                 Various helper methods (_default_node_tags, etc.)</li> <li>Never Override: invoke() - Final method with runtime enforcement                   stream() - Handles telemetry and delegates to _stream                   call() - Delegates to invoke                   Other public methods (build_config, write_state, add_node)</li> </ul> <p>To create a custom agent, inherit from this class and implement the _invoke method:</p> <pre><code>class MyAgent(BaseAgent):\n    def _invoke(self, inputs: Mapping[str, Any], **config: Any) -&gt; Any:\n        # Process inputs and return results\n        ...\n</code></pre> Source code in <code>src/ursa/agents/base.py</code> <pre><code>class BaseAgent(Generic[TState], ABC):\n    \"\"\"Abstract base class for all agent implementations in the Ursa framework.\n\n    BaseAgent provides a standardized foundation for building LLM-powered agents with\n    built-in telemetry, configuration management, and execution flow control. It handles\n    common tasks like input normalization, thread management, metrics collection, and\n    LangGraph integration.\n\n    Subclasses only need to implement the _invoke method to define their core\n    functionality, while inheriting standardized invocation patterns, telemetry, and\n    graph integration capabilities. The class enforces a consistent interface through\n    runtime checks that prevent subclasses from overriding critical methods like\n    invoke().\n\n    The agent supports both direct invocation with inputs and streaming responses, with\n    automatic tracking of token usage, execution time, and other metrics. It also\n    provides utilities for integrating with LangGraph through node wrapping and\n    configuration.\n\n    Subclass Inheritance Guidelines:\n        - Must Override: _invoke() - Define your agent's core functionality\n        - Can Override: _stream() - Enable streaming support\n                        _normalize_inputs() - Customize input handling\n                        Various helper methods (_default_node_tags, etc.)\n        - Never Override: invoke() - Final method with runtime enforcement\n                          stream() - Handles telemetry and delegates to _stream\n                          __call__() - Delegates to invoke\n                          Other public methods (build_config, write_state, add_node)\n\n    To create a custom agent, inherit from this class and implement the _invoke method:\n\n    ```python\n    class MyAgent(BaseAgent):\n        def _invoke(self, inputs: Mapping[str, Any], **config: Any) -&gt; Any:\n            # Process inputs and return results\n            ...\n    ```\n    \"\"\"\n\n    # This will be shared across all BaseAgent instances.\n    _invoke_depth: int = 0\n\n    _TELEMETRY_KW = {\n        \"raw_debug\",\n        \"save_json\",\n        \"save_otel\",\n        \"metrics_path\",\n        \"save_raw_snapshot\",\n        \"save_raw_records\",\n        \"otel_endpoint\",\n        \"otel_headers\",\n    }\n\n    _CONTROL_KW = {\"config\", \"recursion_limit\", \"tags\", \"metadata\", \"callbacks\"}\n\n    state_type: type[TState] = dict\n\n    def __init__(\n        self,\n        llm: BaseChatModel,\n        workspace: Optional[Path] = None,\n        checkpointer: Optional[BaseCheckpointSaver] = None,\n        enable_metrics: bool = True,\n        metrics_dir: str = \"ursa_metrics\",  # dir to save metrics, with a default\n        autosave_metrics: bool = True,\n        otel_metrics: bool = False,\n        thread_id: Optional[str] = None,\n    ):\n        \"\"\"Initializes the base agent with a language model and optional configurations.\n\n        Args:\n            llm: a BaseChatModel instance.\n            checkpointer: Optional checkpoint saver for persisting agent state.\n            enable_metrics: Whether to collect performance and usage metrics.\n            metrics_dir: Directory path where metrics will be saved.\n            autosave_metrics: Whether to automatically save metrics to disk.\n            thread_id: Unique identifier for this agent instance. Generated if not\n                       provided.\n        \"\"\"\n        self.llm: BaseChatModel = llm\n        self.workspace = Path(workspace or \"ursa_workspace\")\n        self.thread_id = thread_id or uuid4().hex\n        self.checkpointer = checkpointer\n        self.telemetry = Telemetry(\n            enable=enable_metrics,\n            output_dir=self.workspace.joinpath(metrics_dir),\n            save_json_default=autosave_metrics,\n        )\n\n        self.workspace.mkdir(exist_ok=True, parents=True)\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Agent name.\"\"\"\n        return self.__class__.__name__\n\n    @property\n    def context(self) -&gt; AgentContext:\n        \"\"\"Immutable run-scoped information provided to the Agent's graph\"\"\"\n        return AgentContext(llm=self.llm, workspace=self.workspace)\n\n    def add_node(\n        self,\n        f: Callable[..., Mapping[str, Any]],\n        node_name: Optional[str] = None,\n        agent_name: Optional[str] = None,\n        **kwargs,\n    ) -&gt; StateGraph:\n        \"\"\"Add a node to the state graph with token usage tracking.\n\n        This method adds a function as a node to the state graph, wrapping it to track\n        token usage during execution. The node is identified by either the provided\n        node_name or the function's name.\n\n        Args:\n            f: The function to add as a node. Should return a mapping of string keys to\n                any values.\n            node_name: Optional name for the node. If not provided, the function's name\n                will be used.\n            agent_name: Optional agent name for tracking. If not provided, the agent's\n                name in snake_case will be used.\n\n        Returns:\n            The updated StateGraph with the new node added.\n        \"\"\"\n        _node_name = node_name or f.__name__\n        _agent_name = agent_name or _to_snake(self.name)\n        wrapped_node = self._wrap_node(f, _node_name, _agent_name)\n        return self.graph.add_node(_node_name, wrapped_node, **kwargs)\n\n    def write_state(self, filename: str, state: dict) -&gt; None:\n        \"\"\"Writes agent state to a JSON file.\n\n        Serializes the provided state dictionary to JSON format and writes it to the\n        specified file. The JSON is written with non-ASCII characters preserved.\n\n        Args:\n            filename: Path to the file where state will be written.\n            state: Dictionary containing the agent state to be serialized.\n        \"\"\"\n        json_state = dumps(state, ensure_ascii=False)\n        with open(filename, \"w\") as f:\n            f.write(json_state)\n\n    def build_config(self, **overrides) -&gt; dict:\n        \"\"\"Constructs a config dictionary for agent operations with telemetry support.\n\n        This method creates a standardized configuration dictionary that includes thread\n        identification, telemetry callbacks, and other metadata needed for agent\n        operations. The configuration can be customized through override parameters.\n\n        Args:\n            **overrides: Optional configuration overrides that can include keys like\n                'recursion_limit', 'configurable', 'metadata', 'tags', etc.\n\n        Returns:\n            dict: A complete configuration dictionary with all necessary parameters.\n        \"\"\"\n        # Create the base configuration with essential fields.\n        base = {\n            \"configurable\": {\"thread_id\": self.thread_id},\n            \"metadata\": {\n                \"thread_id\": self.thread_id,\n                \"telemetry_run_id\": self.telemetry.context.get(\"run_id\"),\n            },\n            \"tags\": [self.name],\n            \"callbacks\": self.telemetry.callbacks,\n        }\n\n        # Try to determine the model name from either direct or nested attributes\n        model_name = getattr(self, \"llm_model\", None) or getattr(\n            getattr(self, \"llm\", None), \"model\", None\n        )\n\n        # Add model name to metadata if available\n        if model_name:\n            base[\"metadata\"][\"model\"] = model_name\n\n        # Handle configurable dictionary overrides by merging with base configurable\n        if \"configurable\" in overrides and isinstance(\n            overrides[\"configurable\"], dict\n        ):\n            base[\"configurable\"].update(overrides.pop(\"configurable\"))\n\n        # Handle metadata dictionary overrides by merging with base metadata\n        if \"metadata\" in overrides and isinstance(overrides[\"metadata\"], dict):\n            base[\"metadata\"].update(overrides.pop(\"metadata\"))\n\n        # Merge tags from caller-provided overrides, avoid duplicates\n        if \"tags\" in overrides and isinstance(overrides[\"tags\"], list):\n            base[\"tags\"] = base[\"tags\"] + [\n                t for t in overrides.pop(\"tags\") if t not in base[\"tags\"]\n            ]\n\n        # Apply any remaining overrides directly to the base configuration\n        base.update(overrides)\n\n        return base\n\n    def _invoke_engine(\n        self,\n        invoke_method,\n        inputs: Optional[InputLike] = None,\n        raw_debug: bool = False,\n        save_json: Optional[bool] = None,\n        save_otel: Optional[bool] = None,\n        metrics_path: Optional[str] = None,\n        otel_endpoint: Optional[str] = None,\n        otel_headers: Optional[str] = None,\n        save_raw_snapshot: Optional[bool] = None,\n        save_raw_records: Optional[bool] = None,\n        config: Optional[dict] = None,\n        **kwargs: Any,\n    ):\n        BaseAgent._invoke_depth += 1\n\n        try:\n            # Start telemetry tracking for the top-level invocation\n            if BaseAgent._invoke_depth == 1:\n                self.telemetry.begin_run(\n                    agent=self.name, thread_id=self.thread_id\n                )\n\n            # Handle the case where inputs are provided as keyword arguments\n            if inputs is None:\n                # Separate kwargs into input parameters and control parameters\n                kw_inputs: dict[str, Any] = {}\n                control_kwargs: dict[str, Any] = {}\n                for k, v in kwargs.items():\n                    if k in self._TELEMETRY_KW or k in self._CONTROL_KW:\n                        control_kwargs[k] = v\n                    else:\n                        kw_inputs[k] = v\n                inputs = kw_inputs\n\n                # Only control kwargs remain for further processing\n                kwargs = control_kwargs\n\n            # Handle the case where inputs are provided as a positional argument\n            else:\n                # Ensure no ambiguous keyword arguments are present\n                for k in kwargs.keys():\n                    if not (k in self._TELEMETRY_KW or k in self._CONTROL_KW):\n                        raise TypeError(\n                            f\"Unexpected keyword argument '{k}'. \"\n                            \"Pass inputs as a single mapping or omit the positional \"\n                            \"inputs and pass them as keyword arguments.\"\n                        )\n\n            # Allow subclasses to normalize or transform the input format\n            normalized = self._normalize_inputs(inputs)\n\n            # Delegate to the subclass implementation with the normalized inputs\n            # and any control parameters\n            return invoke_method(normalized, config=config, **kwargs)\n\n        finally:\n            # Clean up the invocation depth tracking\n            BaseAgent._invoke_depth -= 1\n\n            # For the top-level invocation, finalize telemetry and generate outputs\n            if BaseAgent._invoke_depth == 0:\n                self.telemetry.render(\n                    raw=raw_debug,\n                    save_json=save_json,\n                    save_otel=save_otel,\n                    filepath=metrics_path,\n                    otel_endpoint=otel_endpoint,\n                    otel_headers=otel_headers,\n                    save_raw_snapshot=save_raw_snapshot,\n                    save_raw_records=save_raw_records,\n                )\n\n    # NOTE: The `invoke` method uses the PEP 570 `/,*` notation to explicitly state which\n    # arguments can and cannot be passed as positional or keyword arguments.\n    @final\n    def invoke(\n        self,\n        inputs: Optional[InputLike] = None,\n        /,\n        *,\n        raw_debug: bool = False,\n        save_json: Optional[bool] = None,\n        save_otel: Optional[bool] = None,\n        metrics_path: Optional[str] = None,\n        save_raw_snapshot: Optional[bool] = None,\n        save_raw_records: Optional[bool] = None,\n        config: Optional[dict] = None,\n        **kwargs: Any,\n    ) -&gt; Any:\n        \"\"\"Executes the agent with the provided inputs and configuration.\n\n        This is the main entry point for agent execution. It handles input normalization,\n        telemetry tracking, and proper execution context management. The method supports\n        flexible input formats - either as a positional argument or as keyword arguments.\n\n        Args:\n            inputs: Optional positional input to the agent. If provided, all non-control\n                keyword arguments will be rejected to avoid ambiguity.\n            raw_debug: If True, displays raw telemetry data for debugging purposes.\n            save_json: If True, saves telemetry data as JSON.\n            save_otel: If True, saves telemetry data to OpenTelemetry endpoint.\n            metrics_path: Optional file path where telemetry metrics should be saved.\n            save_raw_snapshot: If True, saves a raw snapshot of the telemetry data.\n            save_raw_records: If True, saves raw telemetry records.\n            config: Optional configuration dictionary to override default settings.\n            **kwargs: Additional keyword arguments that can be either:\n                - Input parameters (when no positional input is provided)\n                - Control parameters recognized by the agent\n\n        Returns:\n            The result of the agent's execution.\n\n        Raises:\n            TypeError: If both positional inputs and non-control keyword arguments are\n                provided simultaneously.\n        \"\"\"\n        return self._invoke_engine(\n            invoke_method=self._invoke,\n            inputs=inputs,\n            raw_debug=raw_debug,\n            save_json=save_json,\n            save_otel=save_otel,\n            metrics_path=metrics_path,\n            save_raw_snapshot=save_raw_snapshot,\n            save_raw_records=save_raw_records,\n            config=config,\n            **kwargs,\n        )\n\n    # NOTE: The `ainvoke` method uses the PEP 570 `/,*` notation to explicitly state which\n    # arguments can and cannot be passed as positional or keyword arguments.\n    @final\n    def ainvoke(\n        self,\n        inputs: Optional[InputLike] = None,\n        /,\n        *,\n        raw_debug: bool = False,\n        save_json: Optional[bool] = None,\n        save_otel: Optional[bool] = None,\n        metrics_path: Optional[str] = None,\n        save_raw_snapshot: Optional[bool] = None,\n        save_raw_records: Optional[bool] = None,\n        config: Optional[dict] = None,\n        **kwargs: Any,\n    ) -&gt; Any:\n        \"\"\"Asynchrnously executes the agent with the provided inputs and configuration.\n\n        (Async version of `invoke`.)\n\n        This is the main entry point for agent execution. It handles input normalization,\n        telemetry tracking, and proper execution context management. The method supports\n        flexible input formats - either as a positional argument or as keyword arguments.\n\n        Args:\n            inputs: Optional positional input to the agent. If provided, all non-control\n                keyword arguments will be rejected to avoid ambiguity.\n            raw_debug: If True, displays raw telemetry data for debugging purposes.\n            save_json: If True, saves telemetry data as JSON.\n            save_otel: If True, saves telemetry data to OpenTelemetry endpoint.\n            metrics_path: Optional file path where telemetry metrics should be saved.\n            save_raw_snapshot: If True, saves a raw snapshot of the telemetry data.\n            save_raw_records: If True, saves raw telemetry records.\n            config: Optional configuration dictionary to override default settings.\n            **kwargs: Additional keyword arguments that can be either:\n                - Input parameters (when no positional input is provided)\n                - Control parameters recognized by the agent\n\n        Returns:\n            The result of the agent's execution.\n\n        Raises:\n            TypeError: If both positional inputs and non-control keyword arguments are\n                provided simultaneously.\n        \"\"\"\n        return self._invoke_engine(\n            invoke_method=self._ainvoke,\n            inputs=inputs,\n            raw_debug=raw_debug,\n            save_json=save_json,\n            save_otel=save_otel,\n            metrics_path=metrics_path,\n            save_raw_snapshot=save_raw_snapshot,\n            save_raw_records=save_raw_records,\n            config=config,\n            **kwargs,\n        )\n\n    def format_query(self, prompt: str, state: TState | None = None) -&gt; TState:\n        \"\"\"Format a plain text prompt into the agent's input schema\n        possibly incorporating the prior state.\n\n        Agents should override this method for their operation\n        \"\"\"\n\n        if state is not None and \"messages\" in state:\n            state[\"messages\"].append(HumanMessage(content=str(prompt)))\n            return state\n        return self._normalize_inputs(prompt)\n\n    def format_result(self, result: TState) -&gt; str:\n        \"\"\"Extracts a plain text response from the Agent's output schema\n\n        Agents should override this method for their operation\n        \"\"\"\n\n        if \"messages\" in result:\n            if isinstance(result[\"messages\"], list) and isinstance(\n                result[\"messages\"][-1], BaseMessage\n            ):\n                return result[\"messages\"][-1].text\n        raise NotImplementedError()\n\n    def _normalize_inputs(self, inputs: InputLike) -&gt; Mapping[str, Any]:\n        \"\"\"Normalizes various input formats into a standardized mapping.\n\n        This method converts different input types into a consistent dictionary format\n        that can be processed by the agent. String inputs are wrapped as messages, while\n        mappings are passed through unchanged.\n\n        Args:\n            inputs: The input to normalize. Can be a string (which will be converted to a\n                message) or a mapping (which will be returned as-is).\n\n        Returns:\n            A mapping containing the normalized inputs, with keys appropriate for agent\n            processing.\n\n        Raises:\n            TypeError: If the input type is not supported (neither string nor mapping).\n        \"\"\"\n        if isinstance(inputs, str):\n            # Adjust to your message type\n            return {\"messages\": [HumanMessage(content=inputs)]}\n        if isinstance(inputs, Mapping):\n            return inputs\n        raise TypeError(f\"Unsupported input type: {type(inputs)}\")\n\n    @cached_property\n    def compiled_graph(self) -&gt; CompiledStateGraph:\n        \"\"\"Return the compiled StateGraph application for the agent.\"\"\"\n        graph = self.build_graph()\n        compiled = graph.compile(\n            checkpointer=self.checkpointer,\n            store=self.storage,\n        ).with_config({\"recursion_limit\": 50000})\n        return self._finalize_graph(compiled)\n\n    @cached_property\n    def storage(self) -&gt; BaseStore:\n        \"\"\"Create a SQLite-backed LangGraph store for persistent graph data.\"\"\"\n        store_path = self.workspace / \"graph_store.sqlite\"\n        conn = sqlite3.connect(\n            store_path, check_same_thread=False, isolation_level=None\n        )\n        store = SqliteStore(conn)\n        store.setup()\n        self.hook_storage_setup(store)\n        return store\n\n    def hook_storage_setup(self, store: BaseStore) -&gt; None:\n        pass\n\n    @final\n    def build_graph(self) -&gt; StateGraph:\n        \"\"\"Build and return the StateGraph backing this agent.\"\"\"\n        self.graph = StateGraph(\n            self.state_type,\n            context_schema=AgentContext,\n        )\n        self._build_graph()\n        return self.graph\n\n    @abstractmethod\n    def _build_graph(self) -&gt; None:\n        \"\"\"Construct the StateGraph for this agent without compiling.\n\n        Called during `__post_init__()` after the Agent has been fully\n        Initialized (`__post_init__` is called after `__init__`) to\n        instantiate `self.graph`\n\n        Agents should implement this to define their their behavior.\n\n        Agents should treat `self.graph` as read-only\n        \"\"\"\n        ...\n\n    def _finalize_graph(\n        self, graph_app: CompiledStateGraph\n    ) -&gt; CompiledStateGraph:\n        \"\"\"Hook for subclasses to wrap or modify the compiled graph.\"\"\"\n        return graph_app\n\n    def _tool_is_async_only(self, tool: Any) -&gt; bool:\n        \"\"\"Return True for tools that can only be invoked asynchronously.\n\n        MCP tools are commonly exposed as StructuredTool instances with a\n        coroutine implementation but no synchronous function implementation.\n        Those raise errors like:\n\n            \"StructuredTool does not support sync invocation.\"\n\n        when called via `.invoke()`.\n        \"\"\"\n\n        func = getattr(tool, \"func\", None)\n        coroutine = getattr(tool, \"coroutine\", None)\n        return func is None and coroutine is not None\n\n    def _has_async_only_tools(self) -&gt; bool:\n        tools_obj = getattr(self, \"tools\", None)\n        if not tools_obj:\n            return False\n\n        try:\n            tool_iter = (\n                tools_obj.values() if isinstance(tools_obj, dict) else tools_obj\n            )\n        except Exception:\n            return False\n\n        return any(self._tool_is_async_only(t) for t in tool_iter)\n\n    def _invoke(self, input, **config):\n        config = self.build_config(**config)\n\n        # If we have async-only tools (e.g. MCP StructuredTools), we must run the\n        # graph via `ainvoke` so ToolNode dispatches tools asynchronously.\n        if self._has_async_only_tools():\n            try:\n                asyncio.get_running_loop()\n            except RuntimeError:\n                return asyncio.run(\n                    self.compiled_graph.ainvoke(\n                        input, config=config, context=self.context\n                    )\n                )\n\n            raise RuntimeError(\n                \"This agent has async-only tools, but `.invoke()` was called \"\n                \"from an async context (a running event loop was detected). \"\n                \"Use `await agent.ainvoke(...)` instead.\"\n            )\n\n        try:\n            return self.compiled_graph.invoke(\n                input, config=config, context=self.context\n            )\n        except Exception as e:\n            # Fallback: if a tool raises the canonical sync-invoke error, retry\n            # with ainvoke for backwards compatibility.\n            if \"does not support sync invocation\" in str(e):\n                try:\n                    asyncio.get_running_loop()\n                except RuntimeError:\n                    return asyncio.run(\n                        self.compiled_graph.ainvoke(\n                            input, config=config, context=self.context\n                        )\n                    )\n            raise\n\n    async def _ainvoke(self, input, **config):\n        config = self.build_config(**config)\n        return await self.compiled_graph.ainvoke(\n            input, config=config, context=self.context\n        )\n\n    def _stream(self, input, **config):\n        config = self.build_config(**config)\n        yield from self.compiled_graph.stream(\n            input, config=config, context=self.context\n        )\n\n    def __call__(self, inputs: InputLike, /, **kwargs: Any) -&gt; Any:\n        \"\"\"Specify calling behavior for class instance.\"\"\"\n        return self.invoke(inputs, **kwargs)\n\n    # Runtime enforcement: forbid subclasses from overriding invoke\n    def __init_subclass__(cls, **kwargs):\n        \"\"\"Ensure subclass does not override key method.\"\"\"\n        super().__init_subclass__(**kwargs)\n        if \"invoke\" in cls.__dict__:\n            err_msg = (\n                f\"{cls.__name__} must not override BaseAgent.invoke(); \"\n                \"implement _invoke() only.\"\n            )\n            raise TypeError(err_msg)\n\n        # Init graph after subclass has been fully constructed\n        orig_init = cls.__init__\n\n        def __init__(self, *args, **kwargs):\n            orig_init(self, *args, **kwargs)\n            self.__post_init__()\n\n        cls.__init__ = __init__\n\n    def __post_init__(self):\n        self.build_graph()\n\n    def stream(\n        self,\n        inputs: InputLike,\n        config: Any | None = None,  # allow positional/keyword like LangGraph\n        /,\n        *,\n        raw_debug: bool = False,\n        save_json: bool | None = None,\n        save_otel: bool | None = None,\n        metrics_path: str | None = None,\n        save_raw_snapshot: bool | None = None,\n        save_raw_records: bool | None = None,\n        **kwargs: Any,\n    ) -&gt; Iterator[Any]:\n        \"\"\"Streams agent responses with telemetry tracking.\n\n        This method serves as the public streaming entry point for agent interactions.\n        It wraps the actual streaming implementation with telemetry tracking to capture\n        metrics and debugging information.\n\n        Args:\n            inputs: The input to process, which will be normalized internally.\n            config: Optional configuration for the agent, compatible with LangGraph\n                positional/keyword argument style.\n            raw_debug: If True, renders raw debug information in telemetry output.\n            save_json: If True, saves telemetry data as JSON.\n            metrics_path: Optional file path where metrics should be saved.\n            save_raw_snapshot: If True, saves raw snapshot data in telemetry.\n            save_raw_records: If True, saves raw record data in telemetry.\n            **kwargs: Additional keyword arguments passed to the streaming\n                implementation.\n\n        Returns:\n            An iterator yielding the agent's responses.\n\n        Note:\n            This method tracks invocation depth to properly handle nested agent calls\n            and ensure telemetry is only rendered once at the top level.\n        \"\"\"\n        # Track invocation depth to handle nested agent calls\n        BaseAgent._invoke_depth += 1\n\n        try:\n            # Start telemetry tracking for top-level invocations only\n            if BaseAgent._invoke_depth == 1:\n                self.telemetry.begin_run(\n                    agent=self.name, thread_id=self.thread_id\n                )\n\n            # Normalize inputs and delegate to the actual streaming implementation\n            normalized = self._normalize_inputs(inputs)\n            yield from self._stream(normalized, config=config, **kwargs)\n\n        finally:\n            # Decrement invocation depth when exiting\n            BaseAgent._invoke_depth -= 1\n\n            # Render telemetry data only for top-level invocations\n            if BaseAgent._invoke_depth == 0:\n                self.telemetry.render(\n                    raw=raw_debug,\n                    save_json=save_json,\n                    save_otel=save_otel,\n                    filepath=metrics_path,\n                    save_raw_snapshot=save_raw_snapshot,\n                    save_raw_records=save_raw_records,\n                )\n\n    def _default_node_tags(\n        self, name: str, extra: Sequence[str] | None = None\n    ) -&gt; list[str]:\n        \"\"\"Generate default tags for a graph node.\n\n        Args:\n            name: The name of the node.\n            extra: Optional sequence of additional tags to include.\n\n        Returns:\n            list[str]: A list of tags for the node, including the agent name, 'graph',\n                the node name, and any extra tags provided.\n        \"\"\"\n        # Start with standard tags: agent name, graph indicator, and node name\n        tags = [self.name, \"graph\", name]\n\n        # Add any extra tags if provided\n        if extra:\n            tags.extend(extra)\n\n        return tags\n\n    def _node_cfg(self, name: str, *extra_tags: str) -&gt; dict:\n        \"\"\"Build a consistent configuration for a node/runnable.\n\n        Creates a configuration dict that can be reapplied after operations like\n        .map(), subgraph compile, etc.\n\n        Args:\n            name: The name of the node.\n            *extra_tags: Additional tags to include in the node configuration.\n\n        Returns:\n            dict: A configuration dictionary with run_name, tags, and metadata.\n        \"\"\"\n        # Determine the namespace - use first extra tag if available, otherwise\n        # convert agent name to snake_case\n        ns = extra_tags[0] if extra_tags else _to_snake(self.name)\n\n        # Combine all tags: agent name, graph indicator, node name, and any extra tags\n        tags = [self.name, \"graph\", name, *extra_tags]\n\n        # Return the complete configuration dictionary\n        return dict(\n            run_name=\"node\",  # keep \"node:\" prefixing in the timer\n            tags=tags,\n            metadata={\n                \"langgraph_node\": name,\n                \"ursa_ns\": ns,\n                \"ursa_agent\": self.name,\n            },\n        )\n\n    def ns(self, runnable_or_fn, name: str, *extra_tags: str):\n        \"\"\"Return a runnable with node configuration applied.\n\n        Applies the agent's node configuration to a runnable or callable. This method\n        should be called again after operations like .map() or subgraph .compile() as\n        these operations may drop configuration.\n\n        Args:\n            runnable_or_fn: A runnable or callable to configure.\n            name: The name to assign to this node.\n            *extra_tags: Additional tags to apply to the node.\n\n        Returns:\n            A configured runnable with the agent's node configuration applied.\n        \"\"\"\n        # Convert input to a runnable if it's not already one\n        r = coerce_to_runnable(runnable_or_fn, name=name, trace=True)\n        # Apply node configuration and return the configured runnable\n        return r.with_config(**self._node_cfg(name, *extra_tags))\n\n    def _wrap_node(self, fn_or_runnable, name: str, *extra_tags: str):\n        \"\"\"Wrap a function or runnable as a node in the graph.\n\n        This is a convenience wrapper around the ns() method.\n\n        Args:\n            fn_or_runnable: A function or runnable to wrap as a node.\n            name: The name to assign to this node.\n            *extra_tags: Additional tags to apply to the node.\n\n        Returns:\n            A configured runnable with the agent's node configuration applied.\n        \"\"\"\n        return self.ns(fn_or_runnable, name, *extra_tags)\n\n    def _wrap_cond(self, fn: Any, name: str, *extra_tags: str):\n        \"\"\"Wrap a conditional function as a routing node in the graph.\n\n        Creates a runnable lambda with routing-specific configuration.\n\n        Args:\n            fn: The conditional function to wrap.\n            name: The name of the routing node.\n            *extra_tags: Additional tags to apply to the node.\n\n        Returns:\n            A configured RunnableLambda with routing-specific metadata.\n        \"\"\"\n        # Use the first extra tag as namespace, or fall back to agent name in snake_case\n        ns = extra_tags[0] if extra_tags else _to_snake(self.name)\n\n        # Create and return a configured RunnableLambda for routing\n        return RunnableLambda(fn).with_config(\n            run_name=\"node\",\n            tags=[\n                self.name,\n                \"graph\",\n                f\"route:{name}\",\n                *extra_tags,\n            ],\n            metadata={\n                \"langgraph_node\": f\"route:{name}\",\n                \"ursa_ns\": ns,\n                \"ursa_agent\": self.name,\n            },\n        )\n\n    def _named(self, runnable: Any, name: str, *extra_tags: str):\n        \"\"\"Apply a specific name and configuration to a runnable.\n\n        Configures a runnable with a specific name and the agent's metadata.\n\n        Args:\n            runnable: The runnable to configure.\n            name: The name to assign to this runnable.\n            *extra_tags: Additional tags to apply to the runnable.\n\n        Returns:\n            A configured runnable with the specified name and agent metadata.\n        \"\"\"\n        # Use the first extra tag as namespace, or fall back to agent name in snake_case\n        ns = extra_tags[0] if extra_tags else _to_snake(self.name)\n\n        # Apply configuration and return the configured runnable\n        return runnable.with_config(\n            run_name=name,\n            tags=[self.name, \"graph\", name, *extra_tags],\n            metadata={\n                \"langgraph_node\": name,\n                \"ursa_ns\": ns,\n                \"ursa_agent\": self.name,\n            },\n        )\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.compiled_graph","title":"<code>compiled_graph</code>  <code>cached</code> <code>property</code>","text":"<p>Return the compiled StateGraph application for the agent.</p>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.context","title":"<code>context</code>  <code>property</code>","text":"<p>Immutable run-scoped information provided to the Agent's graph</p>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.name","title":"<code>name</code>  <code>property</code>","text":"<p>Agent name.</p>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.storage","title":"<code>storage</code>  <code>cached</code> <code>property</code>","text":"<p>Create a SQLite-backed LangGraph store for persistent graph data.</p>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.__call__","title":"<code>__call__(inputs, /, **kwargs)</code>","text":"<p>Specify calling behavior for class instance.</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>def __call__(self, inputs: InputLike, /, **kwargs: Any) -&gt; Any:\n    \"\"\"Specify calling behavior for class instance.\"\"\"\n    return self.invoke(inputs, **kwargs)\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.__init__","title":"<code>__init__(llm, workspace=None, checkpointer=None, enable_metrics=True, metrics_dir='ursa_metrics', autosave_metrics=True, otel_metrics=False, thread_id=None)</code>","text":"<p>Initializes the base agent with a language model and optional configurations.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>BaseChatModel</code> <p>a BaseChatModel instance.</p> required <code>checkpointer</code> <code>Optional[BaseCheckpointSaver]</code> <p>Optional checkpoint saver for persisting agent state.</p> <code>None</code> <code>enable_metrics</code> <code>bool</code> <p>Whether to collect performance and usage metrics.</p> <code>True</code> <code>metrics_dir</code> <code>str</code> <p>Directory path where metrics will be saved.</p> <code>'ursa_metrics'</code> <code>autosave_metrics</code> <code>bool</code> <p>Whether to automatically save metrics to disk.</p> <code>True</code> <code>thread_id</code> <code>Optional[str]</code> <p>Unique identifier for this agent instance. Generated if not        provided.</p> <code>None</code> Source code in <code>src/ursa/agents/base.py</code> <pre><code>def __init__(\n    self,\n    llm: BaseChatModel,\n    workspace: Optional[Path] = None,\n    checkpointer: Optional[BaseCheckpointSaver] = None,\n    enable_metrics: bool = True,\n    metrics_dir: str = \"ursa_metrics\",  # dir to save metrics, with a default\n    autosave_metrics: bool = True,\n    otel_metrics: bool = False,\n    thread_id: Optional[str] = None,\n):\n    \"\"\"Initializes the base agent with a language model and optional configurations.\n\n    Args:\n        llm: a BaseChatModel instance.\n        checkpointer: Optional checkpoint saver for persisting agent state.\n        enable_metrics: Whether to collect performance and usage metrics.\n        metrics_dir: Directory path where metrics will be saved.\n        autosave_metrics: Whether to automatically save metrics to disk.\n        thread_id: Unique identifier for this agent instance. Generated if not\n                   provided.\n    \"\"\"\n    self.llm: BaseChatModel = llm\n    self.workspace = Path(workspace or \"ursa_workspace\")\n    self.thread_id = thread_id or uuid4().hex\n    self.checkpointer = checkpointer\n    self.telemetry = Telemetry(\n        enable=enable_metrics,\n        output_dir=self.workspace.joinpath(metrics_dir),\n        save_json_default=autosave_metrics,\n    )\n\n    self.workspace.mkdir(exist_ok=True, parents=True)\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.__init_subclass__","title":"<code>__init_subclass__(**kwargs)</code>","text":"<p>Ensure subclass does not override key method.</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>def __init_subclass__(cls, **kwargs):\n    \"\"\"Ensure subclass does not override key method.\"\"\"\n    super().__init_subclass__(**kwargs)\n    if \"invoke\" in cls.__dict__:\n        err_msg = (\n            f\"{cls.__name__} must not override BaseAgent.invoke(); \"\n            \"implement _invoke() only.\"\n        )\n        raise TypeError(err_msg)\n\n    # Init graph after subclass has been fully constructed\n    orig_init = cls.__init__\n\n    def __init__(self, *args, **kwargs):\n        orig_init(self, *args, **kwargs)\n        self.__post_init__()\n\n    cls.__init__ = __init__\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.add_node","title":"<code>add_node(f, node_name=None, agent_name=None, **kwargs)</code>","text":"<p>Add a node to the state graph with token usage tracking.</p> <p>This method adds a function as a node to the state graph, wrapping it to track token usage during execution. The node is identified by either the provided node_name or the function's name.</p> <p>Parameters:</p> Name Type Description Default <code>f</code> <code>Callable[..., Mapping[str, Any]]</code> <p>The function to add as a node. Should return a mapping of string keys to any values.</p> required <code>node_name</code> <code>Optional[str]</code> <p>Optional name for the node. If not provided, the function's name will be used.</p> <code>None</code> <code>agent_name</code> <code>Optional[str]</code> <p>Optional agent name for tracking. If not provided, the agent's name in snake_case will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>StateGraph</code> <p>The updated StateGraph with the new node added.</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>def add_node(\n    self,\n    f: Callable[..., Mapping[str, Any]],\n    node_name: Optional[str] = None,\n    agent_name: Optional[str] = None,\n    **kwargs,\n) -&gt; StateGraph:\n    \"\"\"Add a node to the state graph with token usage tracking.\n\n    This method adds a function as a node to the state graph, wrapping it to track\n    token usage during execution. The node is identified by either the provided\n    node_name or the function's name.\n\n    Args:\n        f: The function to add as a node. Should return a mapping of string keys to\n            any values.\n        node_name: Optional name for the node. If not provided, the function's name\n            will be used.\n        agent_name: Optional agent name for tracking. If not provided, the agent's\n            name in snake_case will be used.\n\n    Returns:\n        The updated StateGraph with the new node added.\n    \"\"\"\n    _node_name = node_name or f.__name__\n    _agent_name = agent_name or _to_snake(self.name)\n    wrapped_node = self._wrap_node(f, _node_name, _agent_name)\n    return self.graph.add_node(_node_name, wrapped_node, **kwargs)\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.ainvoke","title":"<code>ainvoke(inputs=None, /, *, raw_debug=False, save_json=None, save_otel=None, metrics_path=None, save_raw_snapshot=None, save_raw_records=None, config=None, **kwargs)</code>","text":"<p>Asynchrnously executes the agent with the provided inputs and configuration.</p> <p>(Async version of <code>invoke</code>.)</p> <p>This is the main entry point for agent execution. It handles input normalization, telemetry tracking, and proper execution context management. The method supports flexible input formats - either as a positional argument or as keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Optional[InputLike]</code> <p>Optional positional input to the agent. If provided, all non-control keyword arguments will be rejected to avoid ambiguity.</p> <code>None</code> <code>raw_debug</code> <code>bool</code> <p>If True, displays raw telemetry data for debugging purposes.</p> <code>False</code> <code>save_json</code> <code>Optional[bool]</code> <p>If True, saves telemetry data as JSON.</p> <code>None</code> <code>save_otel</code> <code>Optional[bool]</code> <p>If True, saves telemetry data to OpenTelemetry endpoint.</p> <code>None</code> <code>metrics_path</code> <code>Optional[str]</code> <p>Optional file path where telemetry metrics should be saved.</p> <code>None</code> <code>save_raw_snapshot</code> <code>Optional[bool]</code> <p>If True, saves a raw snapshot of the telemetry data.</p> <code>None</code> <code>save_raw_records</code> <code>Optional[bool]</code> <p>If True, saves raw telemetry records.</p> <code>None</code> <code>config</code> <code>Optional[dict]</code> <p>Optional configuration dictionary to override default settings.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments that can be either: - Input parameters (when no positional input is provided) - Control parameters recognized by the agent</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the agent's execution.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If both positional inputs and non-control keyword arguments are provided simultaneously.</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>@final\ndef ainvoke(\n    self,\n    inputs: Optional[InputLike] = None,\n    /,\n    *,\n    raw_debug: bool = False,\n    save_json: Optional[bool] = None,\n    save_otel: Optional[bool] = None,\n    metrics_path: Optional[str] = None,\n    save_raw_snapshot: Optional[bool] = None,\n    save_raw_records: Optional[bool] = None,\n    config: Optional[dict] = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Asynchrnously executes the agent with the provided inputs and configuration.\n\n    (Async version of `invoke`.)\n\n    This is the main entry point for agent execution. It handles input normalization,\n    telemetry tracking, and proper execution context management. The method supports\n    flexible input formats - either as a positional argument or as keyword arguments.\n\n    Args:\n        inputs: Optional positional input to the agent. If provided, all non-control\n            keyword arguments will be rejected to avoid ambiguity.\n        raw_debug: If True, displays raw telemetry data for debugging purposes.\n        save_json: If True, saves telemetry data as JSON.\n        save_otel: If True, saves telemetry data to OpenTelemetry endpoint.\n        metrics_path: Optional file path where telemetry metrics should be saved.\n        save_raw_snapshot: If True, saves a raw snapshot of the telemetry data.\n        save_raw_records: If True, saves raw telemetry records.\n        config: Optional configuration dictionary to override default settings.\n        **kwargs: Additional keyword arguments that can be either:\n            - Input parameters (when no positional input is provided)\n            - Control parameters recognized by the agent\n\n    Returns:\n        The result of the agent's execution.\n\n    Raises:\n        TypeError: If both positional inputs and non-control keyword arguments are\n            provided simultaneously.\n    \"\"\"\n    return self._invoke_engine(\n        invoke_method=self._ainvoke,\n        inputs=inputs,\n        raw_debug=raw_debug,\n        save_json=save_json,\n        save_otel=save_otel,\n        metrics_path=metrics_path,\n        save_raw_snapshot=save_raw_snapshot,\n        save_raw_records=save_raw_records,\n        config=config,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.build_config","title":"<code>build_config(**overrides)</code>","text":"<p>Constructs a config dictionary for agent operations with telemetry support.</p> <p>This method creates a standardized configuration dictionary that includes thread identification, telemetry callbacks, and other metadata needed for agent operations. The configuration can be customized through override parameters.</p> <p>Parameters:</p> Name Type Description Default <code>**overrides</code> <p>Optional configuration overrides that can include keys like 'recursion_limit', 'configurable', 'metadata', 'tags', etc.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A complete configuration dictionary with all necessary parameters.</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>def build_config(self, **overrides) -&gt; dict:\n    \"\"\"Constructs a config dictionary for agent operations with telemetry support.\n\n    This method creates a standardized configuration dictionary that includes thread\n    identification, telemetry callbacks, and other metadata needed for agent\n    operations. The configuration can be customized through override parameters.\n\n    Args:\n        **overrides: Optional configuration overrides that can include keys like\n            'recursion_limit', 'configurable', 'metadata', 'tags', etc.\n\n    Returns:\n        dict: A complete configuration dictionary with all necessary parameters.\n    \"\"\"\n    # Create the base configuration with essential fields.\n    base = {\n        \"configurable\": {\"thread_id\": self.thread_id},\n        \"metadata\": {\n            \"thread_id\": self.thread_id,\n            \"telemetry_run_id\": self.telemetry.context.get(\"run_id\"),\n        },\n        \"tags\": [self.name],\n        \"callbacks\": self.telemetry.callbacks,\n    }\n\n    # Try to determine the model name from either direct or nested attributes\n    model_name = getattr(self, \"llm_model\", None) or getattr(\n        getattr(self, \"llm\", None), \"model\", None\n    )\n\n    # Add model name to metadata if available\n    if model_name:\n        base[\"metadata\"][\"model\"] = model_name\n\n    # Handle configurable dictionary overrides by merging with base configurable\n    if \"configurable\" in overrides and isinstance(\n        overrides[\"configurable\"], dict\n    ):\n        base[\"configurable\"].update(overrides.pop(\"configurable\"))\n\n    # Handle metadata dictionary overrides by merging with base metadata\n    if \"metadata\" in overrides and isinstance(overrides[\"metadata\"], dict):\n        base[\"metadata\"].update(overrides.pop(\"metadata\"))\n\n    # Merge tags from caller-provided overrides, avoid duplicates\n    if \"tags\" in overrides and isinstance(overrides[\"tags\"], list):\n        base[\"tags\"] = base[\"tags\"] + [\n            t for t in overrides.pop(\"tags\") if t not in base[\"tags\"]\n        ]\n\n    # Apply any remaining overrides directly to the base configuration\n    base.update(overrides)\n\n    return base\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.build_graph","title":"<code>build_graph()</code>","text":"<p>Build and return the StateGraph backing this agent.</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>@final\ndef build_graph(self) -&gt; StateGraph:\n    \"\"\"Build and return the StateGraph backing this agent.\"\"\"\n    self.graph = StateGraph(\n        self.state_type,\n        context_schema=AgentContext,\n    )\n    self._build_graph()\n    return self.graph\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.format_query","title":"<code>format_query(prompt, state=None)</code>","text":"<p>Format a plain text prompt into the agent's input schema possibly incorporating the prior state.</p> <p>Agents should override this method for their operation</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>def format_query(self, prompt: str, state: TState | None = None) -&gt; TState:\n    \"\"\"Format a plain text prompt into the agent's input schema\n    possibly incorporating the prior state.\n\n    Agents should override this method for their operation\n    \"\"\"\n\n    if state is not None and \"messages\" in state:\n        state[\"messages\"].append(HumanMessage(content=str(prompt)))\n        return state\n    return self._normalize_inputs(prompt)\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.format_result","title":"<code>format_result(result)</code>","text":"<p>Extracts a plain text response from the Agent's output schema</p> <p>Agents should override this method for their operation</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>def format_result(self, result: TState) -&gt; str:\n    \"\"\"Extracts a plain text response from the Agent's output schema\n\n    Agents should override this method for their operation\n    \"\"\"\n\n    if \"messages\" in result:\n        if isinstance(result[\"messages\"], list) and isinstance(\n            result[\"messages\"][-1], BaseMessage\n        ):\n            return result[\"messages\"][-1].text\n    raise NotImplementedError()\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.invoke","title":"<code>invoke(inputs=None, /, *, raw_debug=False, save_json=None, save_otel=None, metrics_path=None, save_raw_snapshot=None, save_raw_records=None, config=None, **kwargs)</code>","text":"<p>Executes the agent with the provided inputs and configuration.</p> <p>This is the main entry point for agent execution. It handles input normalization, telemetry tracking, and proper execution context management. The method supports flexible input formats - either as a positional argument or as keyword arguments.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Optional[InputLike]</code> <p>Optional positional input to the agent. If provided, all non-control keyword arguments will be rejected to avoid ambiguity.</p> <code>None</code> <code>raw_debug</code> <code>bool</code> <p>If True, displays raw telemetry data for debugging purposes.</p> <code>False</code> <code>save_json</code> <code>Optional[bool]</code> <p>If True, saves telemetry data as JSON.</p> <code>None</code> <code>save_otel</code> <code>Optional[bool]</code> <p>If True, saves telemetry data to OpenTelemetry endpoint.</p> <code>None</code> <code>metrics_path</code> <code>Optional[str]</code> <p>Optional file path where telemetry metrics should be saved.</p> <code>None</code> <code>save_raw_snapshot</code> <code>Optional[bool]</code> <p>If True, saves a raw snapshot of the telemetry data.</p> <code>None</code> <code>save_raw_records</code> <code>Optional[bool]</code> <p>If True, saves raw telemetry records.</p> <code>None</code> <code>config</code> <code>Optional[dict]</code> <p>Optional configuration dictionary to override default settings.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments that can be either: - Input parameters (when no positional input is provided) - Control parameters recognized by the agent</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the agent's execution.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If both positional inputs and non-control keyword arguments are provided simultaneously.</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>@final\ndef invoke(\n    self,\n    inputs: Optional[InputLike] = None,\n    /,\n    *,\n    raw_debug: bool = False,\n    save_json: Optional[bool] = None,\n    save_otel: Optional[bool] = None,\n    metrics_path: Optional[str] = None,\n    save_raw_snapshot: Optional[bool] = None,\n    save_raw_records: Optional[bool] = None,\n    config: Optional[dict] = None,\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"Executes the agent with the provided inputs and configuration.\n\n    This is the main entry point for agent execution. It handles input normalization,\n    telemetry tracking, and proper execution context management. The method supports\n    flexible input formats - either as a positional argument or as keyword arguments.\n\n    Args:\n        inputs: Optional positional input to the agent. If provided, all non-control\n            keyword arguments will be rejected to avoid ambiguity.\n        raw_debug: If True, displays raw telemetry data for debugging purposes.\n        save_json: If True, saves telemetry data as JSON.\n        save_otel: If True, saves telemetry data to OpenTelemetry endpoint.\n        metrics_path: Optional file path where telemetry metrics should be saved.\n        save_raw_snapshot: If True, saves a raw snapshot of the telemetry data.\n        save_raw_records: If True, saves raw telemetry records.\n        config: Optional configuration dictionary to override default settings.\n        **kwargs: Additional keyword arguments that can be either:\n            - Input parameters (when no positional input is provided)\n            - Control parameters recognized by the agent\n\n    Returns:\n        The result of the agent's execution.\n\n    Raises:\n        TypeError: If both positional inputs and non-control keyword arguments are\n            provided simultaneously.\n    \"\"\"\n    return self._invoke_engine(\n        invoke_method=self._invoke,\n        inputs=inputs,\n        raw_debug=raw_debug,\n        save_json=save_json,\n        save_otel=save_otel,\n        metrics_path=metrics_path,\n        save_raw_snapshot=save_raw_snapshot,\n        save_raw_records=save_raw_records,\n        config=config,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.ns","title":"<code>ns(runnable_or_fn, name, *extra_tags)</code>","text":"<p>Return a runnable with node configuration applied.</p> <p>Applies the agent's node configuration to a runnable or callable. This method should be called again after operations like .map() or subgraph .compile() as these operations may drop configuration.</p> <p>Parameters:</p> Name Type Description Default <code>runnable_or_fn</code> <p>A runnable or callable to configure.</p> required <code>name</code> <code>str</code> <p>The name to assign to this node.</p> required <code>*extra_tags</code> <code>str</code> <p>Additional tags to apply to the node.</p> <code>()</code> <p>Returns:</p> Type Description <p>A configured runnable with the agent's node configuration applied.</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>def ns(self, runnable_or_fn, name: str, *extra_tags: str):\n    \"\"\"Return a runnable with node configuration applied.\n\n    Applies the agent's node configuration to a runnable or callable. This method\n    should be called again after operations like .map() or subgraph .compile() as\n    these operations may drop configuration.\n\n    Args:\n        runnable_or_fn: A runnable or callable to configure.\n        name: The name to assign to this node.\n        *extra_tags: Additional tags to apply to the node.\n\n    Returns:\n        A configured runnable with the agent's node configuration applied.\n    \"\"\"\n    # Convert input to a runnable if it's not already one\n    r = coerce_to_runnable(runnable_or_fn, name=name, trace=True)\n    # Apply node configuration and return the configured runnable\n    return r.with_config(**self._node_cfg(name, *extra_tags))\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.stream","title":"<code>stream(inputs, config=None, /, *, raw_debug=False, save_json=None, save_otel=None, metrics_path=None, save_raw_snapshot=None, save_raw_records=None, **kwargs)</code>","text":"<p>Streams agent responses with telemetry tracking.</p> <p>This method serves as the public streaming entry point for agent interactions. It wraps the actual streaming implementation with telemetry tracking to capture metrics and debugging information.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>InputLike</code> <p>The input to process, which will be normalized internally.</p> required <code>config</code> <code>Any | None</code> <p>Optional configuration for the agent, compatible with LangGraph positional/keyword argument style.</p> <code>None</code> <code>raw_debug</code> <code>bool</code> <p>If True, renders raw debug information in telemetry output.</p> <code>False</code> <code>save_json</code> <code>bool | None</code> <p>If True, saves telemetry data as JSON.</p> <code>None</code> <code>metrics_path</code> <code>str | None</code> <p>Optional file path where metrics should be saved.</p> <code>None</code> <code>save_raw_snapshot</code> <code>bool | None</code> <p>If True, saves raw snapshot data in telemetry.</p> <code>None</code> <code>save_raw_records</code> <code>bool | None</code> <p>If True, saves raw record data in telemetry.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the streaming implementation.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Iterator[Any]</code> <p>An iterator yielding the agent's responses.</p> Note <p>This method tracks invocation depth to properly handle nested agent calls and ensure telemetry is only rendered once at the top level.</p> Source code in <code>src/ursa/agents/base.py</code> <pre><code>def stream(\n    self,\n    inputs: InputLike,\n    config: Any | None = None,  # allow positional/keyword like LangGraph\n    /,\n    *,\n    raw_debug: bool = False,\n    save_json: bool | None = None,\n    save_otel: bool | None = None,\n    metrics_path: str | None = None,\n    save_raw_snapshot: bool | None = None,\n    save_raw_records: bool | None = None,\n    **kwargs: Any,\n) -&gt; Iterator[Any]:\n    \"\"\"Streams agent responses with telemetry tracking.\n\n    This method serves as the public streaming entry point for agent interactions.\n    It wraps the actual streaming implementation with telemetry tracking to capture\n    metrics and debugging information.\n\n    Args:\n        inputs: The input to process, which will be normalized internally.\n        config: Optional configuration for the agent, compatible with LangGraph\n            positional/keyword argument style.\n        raw_debug: If True, renders raw debug information in telemetry output.\n        save_json: If True, saves telemetry data as JSON.\n        metrics_path: Optional file path where metrics should be saved.\n        save_raw_snapshot: If True, saves raw snapshot data in telemetry.\n        save_raw_records: If True, saves raw record data in telemetry.\n        **kwargs: Additional keyword arguments passed to the streaming\n            implementation.\n\n    Returns:\n        An iterator yielding the agent's responses.\n\n    Note:\n        This method tracks invocation depth to properly handle nested agent calls\n        and ensure telemetry is only rendered once at the top level.\n    \"\"\"\n    # Track invocation depth to handle nested agent calls\n    BaseAgent._invoke_depth += 1\n\n    try:\n        # Start telemetry tracking for top-level invocations only\n        if BaseAgent._invoke_depth == 1:\n            self.telemetry.begin_run(\n                agent=self.name, thread_id=self.thread_id\n            )\n\n        # Normalize inputs and delegate to the actual streaming implementation\n        normalized = self._normalize_inputs(inputs)\n        yield from self._stream(normalized, config=config, **kwargs)\n\n    finally:\n        # Decrement invocation depth when exiting\n        BaseAgent._invoke_depth -= 1\n\n        # Render telemetry data only for top-level invocations\n        if BaseAgent._invoke_depth == 0:\n            self.telemetry.render(\n                raw=raw_debug,\n                save_json=save_json,\n                save_otel=save_otel,\n                filepath=metrics_path,\n                save_raw_snapshot=save_raw_snapshot,\n                save_raw_records=save_raw_records,\n            )\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.base.BaseAgent.write_state","title":"<code>write_state(filename, state)</code>","text":"<p>Writes agent state to a JSON file.</p> <p>Serializes the provided state dictionary to JSON format and writes it to the specified file. The JSON is written with non-ASCII characters preserved.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the file where state will be written.</p> required <code>state</code> <code>dict</code> <p>Dictionary containing the agent state to be serialized.</p> required Source code in <code>src/ursa/agents/base.py</code> <pre><code>def write_state(self, filename: str, state: dict) -&gt; None:\n    \"\"\"Writes agent state to a JSON file.\n\n    Serializes the provided state dictionary to JSON format and writes it to the\n    specified file. The JSON is written with non-ASCII characters preserved.\n\n    Args:\n        filename: Path to the file where state will be written.\n        state: Dictionary containing the agent state to be serialized.\n    \"\"\"\n    json_state = dumps(state, ensure_ascii=False)\n    with open(filename, \"w\") as f:\n        f.write(json_state)\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.chat_agent","title":"<code>chat_agent</code>","text":""},{"location":"api_reference/agents/#ursa.agents.chat_agent.ChatAgent","title":"<code>ChatAgent</code>","text":"<p>               Bases: <code>BaseAgent[ChatState]</code></p> <p>Chat Agent</p> Source code in <code>src/ursa/agents/chat_agent.py</code> <pre><code>class ChatAgent(BaseAgent[ChatState]):\n    \"\"\"Chat Agent\"\"\"\n\n    state_type = ChatState\n\n    def _response_node(self, state: ChatState) -&gt; ChatState:\n        res = self.llm.invoke(state[\"messages\"])\n        return {\"messages\": [res]}\n\n    def format_query(self, prompt: str, state: ChatState | None = None):\n        if state is None:\n            state = ChatState(\n                messages=[SystemMessage(content=get_chatter_system_prompt())]\n            )\n        state[\"messages\"].append(HumanMessage(content=prompt))\n\n        return state\n\n    def format_result(self, result: ChatState) -&gt; str:\n        return result[\"messages\"][-1].text\n\n    def _build_graph(self):\n        self.add_node(self._response_node)\n        self.graph.set_entry_point(\"_response_node\")\n        self.graph.set_finish_point(\"_response_node\")\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.code_review_agent","title":"<code>code_review_agent</code>","text":""},{"location":"api_reference/agents/#ursa.agents.code_review_agent.read_file","title":"<code>read_file(filename, state)</code>","text":"<p>Reads in a file with a given filename into a string</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>string filename to read in</p> required Source code in <code>src/ursa/agents/code_review_agent.py</code> <pre><code>@tool\ndef read_file(filename: str, state: Annotated[dict, InjectedState]):\n    \"\"\"\n    Reads in a file with a given filename into a string\n\n    Args:\n        filename: string filename to read in\n    \"\"\"\n    workspace_dir = state[\"workspace\"]\n    full_filename = os.path.join(workspace_dir, filename)\n\n    print(\"[READING]: \", full_filename)\n    with open(full_filename, \"r\") as file:\n        file_contents = file.read()\n    return file_contents\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.code_review_agent.run_cmd","title":"<code>run_cmd(query, state)</code>","text":"<p>Run command from commandline</p> Source code in <code>src/ursa/agents/code_review_agent.py</code> <pre><code>@tool\ndef run_cmd(query: str, state: Annotated[dict, InjectedState]) -&gt; str:\n    \"\"\"Run command from commandline\"\"\"\n    workspace_dir = state[\"workspace\"]\n\n    print(\"RUNNING: \", query)\n    process = subprocess.Popen(\n        query.split(\" \"),\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=True,\n        cwd=workspace_dir,\n    )\n\n    stdout, stderr = process.communicate(timeout=600)\n\n    print(\"STDOUT: \", stdout)\n    print(\"STDERR: \", stderr)\n\n    return f\"STDOUT: {stdout} and STDERR: {stderr}\"\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.code_review_agent.write_file","title":"<code>write_file(code, filename, state)</code>","text":"<p>Writes text to a file in the given workspace as requested.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>Text to write to a file</p> required <code>filename</code> <code>str</code> <p>the filename to write to</p> required <p>Returns:</p> Type Description <code>str</code> <p>Execution results</p> Source code in <code>src/ursa/agents/code_review_agent.py</code> <pre><code>@tool\ndef write_file(\n    code: str, filename: str, state: Annotated[dict, InjectedState]\n) -&gt; str:\n    \"\"\"\n    Writes text to a file in the given workspace as requested.\n\n    Args:\n        code: Text to write to a file\n        filename: the filename to write to\n\n    Returns:\n        Execution results\n    \"\"\"\n    workspace_dir = state[\"workspace\"]\n\n    print(\"[WRITING]: \", filename)\n    try:\n        # Extract code if wrapped in markdown code blocks\n        if \"```\" in code:\n            code_parts = code.split(\"```\")\n            if len(code_parts) &gt;= 3:\n                # Extract the actual code\n                if \"\\n\" in code_parts[1]:\n                    code = \"\\n\".join(code_parts[1].strip().split(\"\\n\")[1:])\n                else:\n                    code = code_parts[2].strip()\n\n        # Write code to a file\n        code_file = os.path.join(workspace_dir, filename)\n\n        with open(code_file, \"w\") as f:\n            f.write(code)\n        print(f\"Written code to file: {code_file}\")\n\n        return f\"File {filename} written successfully.\"\n\n    except Exception as e:\n        print(f\"Error generating code: {str(e)}\")\n        # Return minimal code that prints the error\n        return f\"Failed to write {filename} successfully.\"\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent","title":"<code>execution_agent</code>","text":"<p>Execution agent that builds a tool-enabled state graph to autonomously run tasks.</p> <p>This module implements ExecutionAgent, a LangGraph-based agent that executes user instructions by invoking LLM tool calls and coordinating a controlled workflow.</p> <p>Key features: - Workspace management with optional symlinking for external sources. - Safety-checked shell execution via run_command with output size budgeting. - Code authoring and edits through write_code and edit_code with rich previews. - Web search capability through DuckDuckGoSearchResults. - Summarization of the session and optional memory logging. - Configurable graph with nodes for agent, action, and summarize.</p> <p>Implementation notes: - LLM prompts are sourced from prompt_library.execution_prompts. - Outputs from subprocess are trimmed under MAX_TOOL_MSG_CHARS to fit tool messages. - The agent uses ToolNode and LangGraph StateGraph to loop until no tool calls remain. - Safety gates block unsafe shell commands and surface the rationale to the user.</p> <p>Environment: - MAX_TOOL_MSG_CHARS caps combined stdout/stderr in tool responses.</p> <p>Entry points: - ExecutionAgent._invoke(...) runs the compiled graph. - main() shows a minimal demo that writes and runs a script.</p>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.ExecutionAgent","title":"<code>ExecutionAgent</code>","text":"<p>               Bases: <code>AgentWithTools</code>, <code>BaseAgent[ExecutionState]</code></p> <p>Orchestrates model-driven code execution, tool calls, and state management.</p> <p>Orchestrates model-driven code execution, tool calls, and state management for iterative program synthesis and shell interaction.</p> <p>This agent wraps an LLM with a small execution graph that alternates between issuing model queries, invoking tools (read, run, write, edit, search), performing safety checks, and summarizing progress. It manages a workspace on disk, optional symlinks, and an optional memory backend to persist summaries.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>BaseChatModel</code> <p>Model identifier or bound chat model instance. If a string is provided, the BaseAgent initializer will resolve it.</p> required <code>agent_memory</code> <code>Any | AgentMemory</code> <p>Memory backend used to store summarized agent interactions. If provided, summaries are saved here.</p> <code>None</code> <code>log_state</code> <code>bool</code> <p>When True, the agent writes intermediate json state to disk for debugging and auditability.</p> <code>False</code> <code>**kwargs</code> <p>Passed through to the BaseAgent constructor (e.g., model configuration, checkpointer).</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>safe_codes</code> <code>list[str]</code> <p>List of trusted programming languages for the agent. Defaults to python and julia</p> <code>executor_prompt</code> <code>str</code> <p>Prompt used when invoking the executor LLM loop.</p> <code>recap_prompt</code> <code>str</code> <p>Prompt used to request concise summaries for memory or final output.</p> <code>tools</code> <code>dict[str, Tool]</code> <p>Tools available to the agent (run_command, write_code, edit_code, read_file, run_web_search, run_osti_search, run_arxiv_search), keyed by tool name for quick lookups.</p> <code>tool_node</code> <code>ToolNode</code> <p>Graph node that dispatches tool calls.</p> <code>llm</code> <code>BaseChatModel</code> <p>LLM instance bound to the available tools.</p> <p>Methods:</p> Name Description <code>query_executor</code> <p>Send messages to the executor LLM, ensure workspace exists, and handle symlink setup before returning the model response.</p> <code>recap</code> <p>Produce and optionally persist a summary of recent interactions to the memory backend.</p> <code>_build_graph</code> <p>Construct and compile the StateGraph for the agent loop.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>Accessing the .action attribute raises to encourage using .stream(...) or .invoke(...).</p> Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>class ExecutionAgent(AgentWithTools, BaseAgent[ExecutionState]):\n    \"\"\"Orchestrates model-driven code execution, tool calls, and state management.\n\n    Orchestrates model-driven code execution, tool calls, and state management for\n    iterative program synthesis and shell interaction.\n\n    This agent wraps an LLM with a small execution graph that alternates\n    between issuing model queries, invoking tools (read, run, write, edit, search),\n    performing safety checks, and summarizing progress. It manages a\n    workspace on disk, optional symlinks, and an optional memory backend to\n    persist summaries.\n\n    Args:\n        llm (BaseChatModel): Model identifier or bound chat model\n            instance. If a string is provided, the BaseAgent initializer will\n            resolve it.\n        agent_memory (Any | AgentMemory, optional): Memory backend used to\n            store summarized agent interactions. If provided, summaries are\n            saved here.\n        log_state (bool): When True, the agent writes intermediate json state\n            to disk for debugging and auditability.\n        **kwargs: Passed through to the BaseAgent constructor (e.g., model\n            configuration, checkpointer).\n\n    Attributes:\n        safe_codes (list[str]): List of trusted programming languages for the\n            agent. Defaults to python and julia\n        executor_prompt (str): Prompt used when invoking the executor LLM\n            loop.\n        recap_prompt (str): Prompt used to request concise summaries for\n            memory or final output.\n        tools (dict[str, Tool]): Tools available to the agent (run_command, write_code,\n            edit_code, read_file, run_web_search, run_osti_search, run_arxiv_search),\n            keyed by tool name for quick lookups.\n        tool_node (ToolNode): Graph node that dispatches tool calls.\n        llm (BaseChatModel): LLM instance bound to the available tools.\n\n    Methods:\n        query_executor(state): Send messages to the executor LLM, ensure\n            workspace exists, and handle symlink setup before returning the\n            model response.\n        recap(state): Produce and optionally persist a summary of recent\n            interactions to the memory backend.\n        _build_graph(): Construct and compile the StateGraph for the agent\n            loop.\n\n    Raises:\n        AttributeError: Accessing the .action attribute raises to encourage\n            using .stream(...) or .invoke(...).\n    \"\"\"\n\n    state_type = ExecutionState\n\n    def __init__(\n        self,\n        llm: BaseChatModel,\n        agent_memory: Optional[Any | AgentMemory] = None,\n        log_state: bool = False,\n        extra_tools: Optional[list[BaseTool] | None] = None,\n        tokens_before_summarize: int = 50000,\n        messages_to_keep: int = 20,\n        safe_codes: Optional[list[str]] = None,\n        **kwargs,\n    ):\n        default_tools = [\n            run_command,\n            write_code,\n            edit_code,\n            read_file,\n            run_web_search,\n            run_osti_search,\n            run_arxiv_search,\n        ]\n        if extra_tools:\n            default_tools.extend(extra_tools)\n\n        super().__init__(llm=llm, tools=default_tools, **kwargs)\n        self.agent_memory = agent_memory\n        self.safe_codes = set(safe_codes or [\"python\", \"julia\"])\n        self.executor_prompt = executor_prompt\n        self.recap_prompt = recap_prompt\n        self.extra_tools = extra_tools\n        self.log_state = log_state\n        self.tokens_before_summarize = tokens_before_summarize\n        self.messages_to_keep = messages_to_keep\n\n    def _patch_dangling(\n        self, state: ExecutionState, summarized: bool\n    ) -&gt; ExecutionState:\n        new_state = deepcopy(state)\n        dangling_response = (\n            \"Response Not Found from tool. \"\n            \"May have timed out or been forgotten due to summarization.\"\n        )\n\n        tool_ids = []\n        for msg in new_state[\"messages\"]:\n            if count_tokens_approximately([msg]) &gt; 100000 and isinstance(\n                msg, ToolMessage\n            ):\n                trunc_message = \"Message too long - truncated.\"\n                msg.content = trunc_message\n                summarized = True\n            if hasattr(msg, \"tool_calls\"):\n                for call in msg.tool_calls:\n                    tool_ids.append(call[\"id\"])\n            if isinstance(msg, ToolMessage):\n                tool_ids.remove(msg.tool_call_id)\n        if tool_ids:\n            summarized = True\n            print(\n                f\"[Dangling Tool Call Warning] The following tool IDs \"\n                f\"were dangling:\\n{tool_ids}\\nReplies of missing response applied.\"\n            )\n            for tool_id in tool_ids:\n                for msg_ind, msg in enumerate(new_state[\"messages\"]):\n                    if hasattr(msg, \"tool_calls\"):\n                        if any([tc[\"id\"] == tool_id for tc in msg.tool_calls]):\n                            # Inserts tool response one after the dangling tool call\n                            #    Mutates new_state so break afterward to reset loop.\n                            #    Not as efficient as could be but should be correct\n                            new_state[\"messages\"].insert(\n                                msg_ind + 1,\n                                ToolMessage(\n                                    content=dangling_response,\n                                    tool_call_id=tool_id,\n                                ),\n                            )\n                            break\n        return new_state, summarized\n\n    # Check message history length and summarize to shorten the token usage:\n    def _summarize_context(self, state: ExecutionState) -&gt; ExecutionState:\n        new_state = deepcopy(state)\n        summarized = False\n        tokens_before_summarize = count_tokens_approximately(\n            new_state[\"messages\"][1:]\n        )\n\n        if tokens_before_summarize &gt; self.tokens_before_summarize:\n            # Start from 1 to skip system message.\n            conversation_to_summarize = new_state[\"messages\"][\n                1 : -self.messages_to_keep\n            ]\n            conversation_to_keep = new_state[\"messages\"][\n                -self.messages_to_keep :\n            ]\n            tool_ids = []\n            for msg in conversation_to_summarize:\n                if hasattr(msg, \"tool_calls\"):\n                    for call in msg.tool_calls:\n                        tool_ids.append(call[\"id\"])\n                if isinstance(msg, ToolMessage):\n                    tool_ids.remove(msg.tool_call_id)\n            if tool_ids:\n                print(\n                    f\"[Summarizing] The following tool IDs would be cut off:\\n{tool_ids}\"\n                )\n                conversation_to_keep_copy = conversation_to_keep.copy()\n                for msg in conversation_to_keep_copy:\n                    if (\n                        isinstance(msg, ToolMessage)\n                        and msg.tool_call_id in tool_ids\n                    ):\n                        conversation_to_summarize.append(msg)\n                        conversation_to_keep.remove(msg)\n                        tool_ids.remove(msg.tool_call_id)\n            if tool_ids:\n                # We may need to implement something here for if a tool has not\n                # responded but its tool call is far enough back that it is being\n                # summarized away. Likely an edge case for non-async, but async\n                # may cause a problem here.\n                print(\n                    f\"Tool ID '{tool_ids}' was in the messages to summarize, but was not found in the responses. Could be dangling tool call.\"\n                )\n                pass\n\n            summarize_prompt = f\"\"\"\n            Your only tasks is to provide a detailed, comprehensive summary of the following\n            conversation.\n\n            Your summary will be the only information retained from the conversation, so ensure\n            it contains all details that need to be remembered to meet the goals of the work.\n\n            Conversation to summarize:\n            {conversation_to_summarize}\n            \"\"\"\n            summary = self.llm.invoke(summarize_prompt)\n            summarized_messages = [\n                SystemMessage(content=self.executor_prompt),\n                summary,\n            ]\n            summarized_messages.extend(conversation_to_keep)\n            verbose = False\n            # Keeping this here for future capability add\n            #    removing this from printing generally,\n            #    but we may want to bring this back with some\n            #    verbosity option in the future.\n            #\n            # Right now setting verbose to False so this is\n            #     always skipped but here to revisit.\n            if verbose:\n                tokens_after_summarize = count_tokens_approximately(\n                    summarized_messages\n                )\n                console.print(\n                    Panel(\n                        (\n                            f\"Summarized Conversation History:\\n\"\n                            f\"Summary:\\n{summary.text}\\n\"\n                            f\"Approximate tokens before: {tokens_before_summarize}\\n\"\n                            f\"Approximate tokens after: {tokens_after_summarize}\\n\"\n                        ),\n                        title=\"[bold yellow1 on black]Summarize Past Context\",\n                        border_style=\"yellow1\",\n                        style=\"bold yellow1 on black\",\n                    )\n                )\n            new_state[\"messages\"] = summarized_messages\n            summarized = True\n        return new_state, summarized\n\n    # Define the function that calls the model\n    def query_executor(\n        self, state: ExecutionState, runtime: Runtime[AgentContext]\n    ) -&gt; ExecutionState:\n        \"\"\"Prepare workspace, handle optional symlinks, and invoke the executor LLM.\n\n        This method copies the incoming state, ensures a workspace directory exists\n        (creating one with a default name when absent), optionally creates a symlink\n        described by state[\"symlinkdir\"], sets or injects the executor system prompt\n        as the first message, and invokes the bound LLM. When logging is enabled,\n        it persists the pre-invocation state to disk.\n\n        Args:\n            state: The current execution state. Expected keys include:\n                - \"messages\": Ordered list of System/Human/AI/Tool messages.\n                - \"workspace\": Optional path to the working directory.\n                - \"symlinkdir\": Optional dict with \"source\" and \"dest\" keys.\n\n        Returns:\n            ExecutionState: Partial state update containing:\n                - \"messages\": A list with the model's response as the latest entry.\n                - \"workspace\": The resolved workspace path.\n        \"\"\"\n        # Add model to the state so it can be passed to tools like the URSA Arxiv or OSTI tools\n        new_state = deepcopy(state)\n        new_state.setdefault(\"symlinkdir\", {})\n\n        full_overwrite = False\n\n        # 1.5) Check message history length and summarize to shorten the token usage:\n        new_state, full_overwrite = self._summarize_context(new_state)\n\n        # 2) Optionally create a symlink if symlinkdir is provided and not yet linked.\n        sd = new_state.get(\"symlinkdir\")\n        if sd and \"is_linked\" not in sd:\n            # symlinkdir structure: {\"source\": \"/path/to/src\", \"dest\": \"link/name\"}\n            symlinkdir = sd\n\n            src = Path(symlinkdir[\"source\"]).expanduser().resolve()\n            dst = runtime.context.workspace.joinpath(symlinkdir[\"dest\"])\n\n            # If a file/link already exists at the destination, replace it.\n            if dst.exists() or dst.is_symlink():\n                dst.unlink()\n\n            # Ensure parent directories for the link exist.\n            dst.parent.mkdir(parents=True, exist_ok=True)\n\n            # Create the symlink (tell pathlib if the target is a directory).\n            dst.symlink_to(src, target_is_directory=src.is_dir())\n            print(f\"{RED}Symlinked:{RESET} {src} (source) --&gt; {dst} (dest)\")\n            new_state[\"symlinkdir\"][\"is_linked\"] = True\n            full_overwrite = True\n\n        new_state, full_overwrite = self._patch_dangling(\n            new_state, full_overwrite\n        )\n\n        # 3) Ensure the executor prompt is the first SystemMessage.\n        messages = deepcopy(new_state[\"messages\"])\n        if isinstance(messages[0], SystemMessage):\n            messages[0] = SystemMessage(content=self.executor_prompt)\n        else:\n            messages = [SystemMessage(content=self.executor_prompt)] + messages\n\n        # 4) Invoke the LLM with the prepared message sequence.\n        try:\n            response = self.llm.invoke(\n                messages, self.build_config(tags=[\"agent\"])\n            )\n            new_state[\"messages\"].append(response)\n        except Exception as e:\n            response = AIMessage(content=f\"Response error {e}\")\n            msg = new_state[\"messages\"][-1].text\n            print(\"Error: \", e, \" \", msg)\n            new_state[\"messages\"].append(response)\n\n        # 5) Optionally persist the pre-invocation state for audit/debugging.\n        if self.log_state:\n            self.write_state(\"execution_agent.json\", new_state)\n        if full_overwrite:\n            return {\n                \"messages\": Overwrite(new_state[\"messages\"]),\n                \"symlinkdir\": new_state[\"symlinkdir\"],\n            }\n        else:\n            return {\"messages\": response, \"symlinkdir\": new_state[\"symlinkdir\"]}\n\n    def recap(self, state: ExecutionState) -&gt; ExecutionState:\n        \"\"\"Produce a concise summary of the conversation and optionally persist memory.\n\n        This method builds a summarization prompt, invokes the LLM to obtain a compact\n        summary of recent interactions, optionally logs salient details to the agent\n        memory backend, and writes debug state when logging is enabled.\n\n        Args:\n            state (ExecutionState): The execution state containing message history.\n\n        Returns:\n            ExecutionState: A partial update with a single string message containing\n                the recap.\n        \"\"\"\n        new_state = deepcopy(state)\n        full_overwrite = False\n\n        # 0) Check message history length and summarize to shorten the token usage:\n        new_state, full_overwrite = self._summarize_context(new_state)\n\n        # 1) Construct the summarization message list (system prompt + prior messages).\n        recap_message = HumanMessage(content=self.recap_prompt)\n        new_state[\"messages\"] = new_state[\"messages\"] + [recap_message]\n\n        # 2) Invoke the LLM to generate a recap; capture content even on failure.\n        new_state, full_overwrite = self._patch_dangling(\n            new_state, full_overwrite\n        )\n        try:\n            response = self.llm.invoke(\n                input=new_state[\"messages\"],\n                config=self.build_config(tags=[\"recap\"]),\n            )\n            response_content = response.text\n        except Exception as e:\n            response_content = f\"Response error {e}\"\n            response = AIMessage(content=response_content)\n            print(\"Error: \", e, \" \", new_state[\"messages\"][-1].text)\n\n        console.print(\n            Panel(\n                Markdown(response_content),\n                title=\"[bold grey85 on black]Recap of Work\",\n                border_style=\"grey85 on black\",\n                style=\"grey85 on black\",\n                expand=False,  # Make panel fit content width\n            )\n        )\n\n        # 3) Optionally persist salient details to the memory backend.\n        if self.agent_memory:\n            memories: list[str] = []\n            # Collect human/system/tool message content; for AI tool calls, store args.\n            for msg in new_state[\"messages\"]:\n                msg_content = msg.text\n                if not isinstance(msg, AIMessage):\n                    memories.append(msg_content)\n                elif not msg.tool_calls:\n                    memories.append(msg_content)\n                else:\n                    tool_strings = []\n                    for tool in msg.tool_calls:\n                        tool_strings.append(\"Tool Name: \" + tool[\"name\"])\n                        for arg_name in tool[\"args\"]:\n                            tool_strings.append(\n                                f\"Arg: {str(arg_name)}\\nValue: \"\n                                f\"{str(tool['args'][arg_name])}\"\n                            )\n                    memories.append(\"\\n\".join(tool_strings))\n            memories.append(response_content)\n            self.agent_memory.add_memories(memories)\n\n        if full_overwrite:\n            # 4) Optionally write state to disk for debugging/auditing.\n            new_state[\"messages\"].append(response)\n            if self.log_state:\n                self.write_state(\"execution_agent.json\", new_state)\n            return Overwrite(new_state)\n        else:\n            if self.log_state:\n                new_state[\"messages\"].append(response)\n                self.write_state(\"execution_agent.json\", new_state)\n            return {\"messages\": [recap_message, response]}\n\n    def _build_graph(self):\n        \"\"\"Construct and compile the agent's LangGraph state machine.\"\"\"\n\n        # Bind tools to llm and context summarizer\n\n        self.llm = self.llm.bind_tools(self.tools.values())\n\n        # Register nodes:\n        # - \"agent\": LLM planning/execution step\n        # - \"action\": tool dispatch (run_command, write_code, etc.)\n        # - \"recap\": summary/finalization step\n        self.add_node(self.query_executor, \"agent\")\n        self.add_node(self.tool_node, \"action\")\n        self.add_node(self.recap, \"recap\")\n\n        # Set entrypoint: execution starts with the \"agent\" node.\n        self.graph.set_entry_point(\"agent\")\n\n        # From \"agent\", either continue (tools) or finish (recap),\n        # based on presence of tool calls in the last message.\n        self.graph.add_conditional_edges(\n            \"agent\",\n            self._wrap_cond(should_continue, \"should_continue\", \"execution\"),\n            {\"continue\": \"action\", \"recap\": \"recap\"},\n        )\n\n        # After tools run, return control to the agent for the next step.\n        self.graph.add_edge(\"action\", \"agent\")\n\n        # The graph completes at the \"recap\" node.\n        self.graph.set_finish_point(\"recap\")\n\n    def format_result(self, state: ExecutionState) -&gt; str:\n        return state[\"messages\"][-1].text\n\n    def hook_storage_setup(self, store):\n        # Record the edit operation\n        if store is None:\n            return\n        for safe_code in self.safe_codes:\n            store.put(\n                (\"workspace\", \"safe_codes\"),\n                safe_code,\n                {},\n            )\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.ExecutionAgent.query_executor","title":"<code>query_executor(state, runtime)</code>","text":"<p>Prepare workspace, handle optional symlinks, and invoke the executor LLM.</p> <p>This method copies the incoming state, ensures a workspace directory exists (creating one with a default name when absent), optionally creates a symlink described by state[\"symlinkdir\"], sets or injects the executor system prompt as the first message, and invokes the bound LLM. When logging is enabled, it persists the pre-invocation state to disk.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>ExecutionState</code> <p>The current execution state. Expected keys include: - \"messages\": Ordered list of System/Human/AI/Tool messages. - \"workspace\": Optional path to the working directory. - \"symlinkdir\": Optional dict with \"source\" and \"dest\" keys.</p> required <p>Returns:</p> Name Type Description <code>ExecutionState</code> <code>ExecutionState</code> <p>Partial state update containing: - \"messages\": A list with the model's response as the latest entry. - \"workspace\": The resolved workspace path.</p> Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>def query_executor(\n    self, state: ExecutionState, runtime: Runtime[AgentContext]\n) -&gt; ExecutionState:\n    \"\"\"Prepare workspace, handle optional symlinks, and invoke the executor LLM.\n\n    This method copies the incoming state, ensures a workspace directory exists\n    (creating one with a default name when absent), optionally creates a symlink\n    described by state[\"symlinkdir\"], sets or injects the executor system prompt\n    as the first message, and invokes the bound LLM. When logging is enabled,\n    it persists the pre-invocation state to disk.\n\n    Args:\n        state: The current execution state. Expected keys include:\n            - \"messages\": Ordered list of System/Human/AI/Tool messages.\n            - \"workspace\": Optional path to the working directory.\n            - \"symlinkdir\": Optional dict with \"source\" and \"dest\" keys.\n\n    Returns:\n        ExecutionState: Partial state update containing:\n            - \"messages\": A list with the model's response as the latest entry.\n            - \"workspace\": The resolved workspace path.\n    \"\"\"\n    # Add model to the state so it can be passed to tools like the URSA Arxiv or OSTI tools\n    new_state = deepcopy(state)\n    new_state.setdefault(\"symlinkdir\", {})\n\n    full_overwrite = False\n\n    # 1.5) Check message history length and summarize to shorten the token usage:\n    new_state, full_overwrite = self._summarize_context(new_state)\n\n    # 2) Optionally create a symlink if symlinkdir is provided and not yet linked.\n    sd = new_state.get(\"symlinkdir\")\n    if sd and \"is_linked\" not in sd:\n        # symlinkdir structure: {\"source\": \"/path/to/src\", \"dest\": \"link/name\"}\n        symlinkdir = sd\n\n        src = Path(symlinkdir[\"source\"]).expanduser().resolve()\n        dst = runtime.context.workspace.joinpath(symlinkdir[\"dest\"])\n\n        # If a file/link already exists at the destination, replace it.\n        if dst.exists() or dst.is_symlink():\n            dst.unlink()\n\n        # Ensure parent directories for the link exist.\n        dst.parent.mkdir(parents=True, exist_ok=True)\n\n        # Create the symlink (tell pathlib if the target is a directory).\n        dst.symlink_to(src, target_is_directory=src.is_dir())\n        print(f\"{RED}Symlinked:{RESET} {src} (source) --&gt; {dst} (dest)\")\n        new_state[\"symlinkdir\"][\"is_linked\"] = True\n        full_overwrite = True\n\n    new_state, full_overwrite = self._patch_dangling(\n        new_state, full_overwrite\n    )\n\n    # 3) Ensure the executor prompt is the first SystemMessage.\n    messages = deepcopy(new_state[\"messages\"])\n    if isinstance(messages[0], SystemMessage):\n        messages[0] = SystemMessage(content=self.executor_prompt)\n    else:\n        messages = [SystemMessage(content=self.executor_prompt)] + messages\n\n    # 4) Invoke the LLM with the prepared message sequence.\n    try:\n        response = self.llm.invoke(\n            messages, self.build_config(tags=[\"agent\"])\n        )\n        new_state[\"messages\"].append(response)\n    except Exception as e:\n        response = AIMessage(content=f\"Response error {e}\")\n        msg = new_state[\"messages\"][-1].text\n        print(\"Error: \", e, \" \", msg)\n        new_state[\"messages\"].append(response)\n\n    # 5) Optionally persist the pre-invocation state for audit/debugging.\n    if self.log_state:\n        self.write_state(\"execution_agent.json\", new_state)\n    if full_overwrite:\n        return {\n            \"messages\": Overwrite(new_state[\"messages\"]),\n            \"symlinkdir\": new_state[\"symlinkdir\"],\n        }\n    else:\n        return {\"messages\": response, \"symlinkdir\": new_state[\"symlinkdir\"]}\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.ExecutionAgent.recap","title":"<code>recap(state)</code>","text":"<p>Produce a concise summary of the conversation and optionally persist memory.</p> <p>This method builds a summarization prompt, invokes the LLM to obtain a compact summary of recent interactions, optionally logs salient details to the agent memory backend, and writes debug state when logging is enabled.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>ExecutionState</code> <p>The execution state containing message history.</p> required <p>Returns:</p> Name Type Description <code>ExecutionState</code> <code>ExecutionState</code> <p>A partial update with a single string message containing the recap.</p> Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>def recap(self, state: ExecutionState) -&gt; ExecutionState:\n    \"\"\"Produce a concise summary of the conversation and optionally persist memory.\n\n    This method builds a summarization prompt, invokes the LLM to obtain a compact\n    summary of recent interactions, optionally logs salient details to the agent\n    memory backend, and writes debug state when logging is enabled.\n\n    Args:\n        state (ExecutionState): The execution state containing message history.\n\n    Returns:\n        ExecutionState: A partial update with a single string message containing\n            the recap.\n    \"\"\"\n    new_state = deepcopy(state)\n    full_overwrite = False\n\n    # 0) Check message history length and summarize to shorten the token usage:\n    new_state, full_overwrite = self._summarize_context(new_state)\n\n    # 1) Construct the summarization message list (system prompt + prior messages).\n    recap_message = HumanMessage(content=self.recap_prompt)\n    new_state[\"messages\"] = new_state[\"messages\"] + [recap_message]\n\n    # 2) Invoke the LLM to generate a recap; capture content even on failure.\n    new_state, full_overwrite = self._patch_dangling(\n        new_state, full_overwrite\n    )\n    try:\n        response = self.llm.invoke(\n            input=new_state[\"messages\"],\n            config=self.build_config(tags=[\"recap\"]),\n        )\n        response_content = response.text\n    except Exception as e:\n        response_content = f\"Response error {e}\"\n        response = AIMessage(content=response_content)\n        print(\"Error: \", e, \" \", new_state[\"messages\"][-1].text)\n\n    console.print(\n        Panel(\n            Markdown(response_content),\n            title=\"[bold grey85 on black]Recap of Work\",\n            border_style=\"grey85 on black\",\n            style=\"grey85 on black\",\n            expand=False,  # Make panel fit content width\n        )\n    )\n\n    # 3) Optionally persist salient details to the memory backend.\n    if self.agent_memory:\n        memories: list[str] = []\n        # Collect human/system/tool message content; for AI tool calls, store args.\n        for msg in new_state[\"messages\"]:\n            msg_content = msg.text\n            if not isinstance(msg, AIMessage):\n                memories.append(msg_content)\n            elif not msg.tool_calls:\n                memories.append(msg_content)\n            else:\n                tool_strings = []\n                for tool in msg.tool_calls:\n                    tool_strings.append(\"Tool Name: \" + tool[\"name\"])\n                    for arg_name in tool[\"args\"]:\n                        tool_strings.append(\n                            f\"Arg: {str(arg_name)}\\nValue: \"\n                            f\"{str(tool['args'][arg_name])}\"\n                        )\n                memories.append(\"\\n\".join(tool_strings))\n        memories.append(response_content)\n        self.agent_memory.add_memories(memories)\n\n    if full_overwrite:\n        # 4) Optionally write state to disk for debugging/auditing.\n        new_state[\"messages\"].append(response)\n        if self.log_state:\n            self.write_state(\"execution_agent.json\", new_state)\n        return Overwrite(new_state)\n    else:\n        if self.log_state:\n            new_state[\"messages\"].append(response)\n            self.write_state(\"execution_agent.json\", new_state)\n        return {\"messages\": [recap_message, response]}\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.ExecutionState","title":"<code>ExecutionState</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>TypedDict representing the execution agent's mutable run state used by nodes.</p> <p>Fields: - messages: list of messages (System/Human/AI/Tool). - symlinkdir: optional dict describing a symlink operation (source, dest,   is_linked).</p> Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>class ExecutionState(TypedDict):\n    \"\"\"TypedDict representing the execution agent's mutable run state used by nodes.\n\n    Fields:\n    - messages: list of messages (System/Human/AI/Tool).\n    - symlinkdir: optional dict describing a symlink operation (source, dest,\n      is_linked).\n    \"\"\"\n\n    messages: Annotated[list[AnyMessage], add_messages]\n    symlinkdir: dict\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.command_safe","title":"<code>command_safe(state)</code>","text":"<p>Return 'safe' if the last command was safe, otherwise 'unsafe'.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>ExecutionState</code> <p>The current execution state containing messages and tool calls.</p> required <p>Returns:     A literal \"safe\" if no '[UNSAFE]' tags are in the last command,     otherwise \"unsafe\".</p> Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>def command_safe(state: ExecutionState) -&gt; Literal[\"safe\", \"unsafe\"]:\n    \"\"\"Return 'safe' if the last command was safe, otherwise 'unsafe'.\n\n    Args:\n        state: The current execution state containing messages and tool calls.\n    Returns:\n        A literal \"safe\" if no '[UNSAFE]' tags are in the last command,\n        otherwise \"unsafe\".\n    \"\"\"\n    index = -1\n    message = state[\"messages\"][index]\n    # Loop through all the consecutive tool messages in reverse order\n    while isinstance(message, ToolMessage):\n        if \"[UNSAFE]\" in message.text:\n            return \"unsafe\"\n\n        index -= 1\n        message = state[\"messages\"][index]\n\n    return \"safe\"\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.execution_agent.should_continue","title":"<code>should_continue(state)</code>","text":"<p>Return 'recap' if no tool calls in the last message, else 'continue'.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>ExecutionState</code> <p>The current execution state containing messages.</p> required <p>Returns:</p> Type Description <code>Literal['recap', 'continue']</code> <p>A literal \"recap\" if the last message has no tool calls,</p> <code>Literal['recap', 'continue']</code> <p>otherwise \"continue\".</p> Source code in <code>src/ursa/agents/execution_agent.py</code> <pre><code>def should_continue(state: ExecutionState) -&gt; Literal[\"recap\", \"continue\"]:\n    \"\"\"Return 'recap' if no tool calls in the last message, else 'continue'.\n\n    Args:\n        state: The current execution state containing messages.\n\n    Returns:\n        A literal \"recap\" if the last message has no tool calls,\n        otherwise \"continue\".\n    \"\"\"\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    # If there is no tool call, then we finish\n    if not last_message.tool_calls:\n        return \"recap\"\n    # Otherwise if there is, we continue\n    else:\n        return \"continue\"\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.hypothesizer_agent","title":"<code>hypothesizer_agent</code>","text":""},{"location":"api_reference/agents/#ursa.agents.hypothesizer_agent.HypothesizerAgent","title":"<code>HypothesizerAgent</code>","text":"<p>               Bases: <code>BaseAgent[HypothesizerState]</code></p> Source code in <code>src/ursa/agents/hypothesizer_agent.py</code> <pre><code>class HypothesizerAgent(BaseAgent[HypothesizerState]):\n    state_type = HypothesizerState\n\n    def __init__(self, llm: BaseChatModel, max_iterations: int = 2, **kwargs):\n        super().__init__(llm, **kwargs)\n        self.hypothesizer_prompt = hypothesizer_prompt\n        self.critic_prompt = critic_prompt\n        self.competitor_prompt = competitor_prompt\n        self.search_tool = DDGS()\n        self.strllm = self.llm | StrOutputParser()\n        self.max_iterations = max_iterations\n\n    def _normalize_inputs(self, inputs) -&gt; HypothesizerState:\n        if isinstance(inputs, str):\n            return HypothesizerState(\n                question=inputs,\n                max_iterations=self.max_iterations,\n                current_iteration=0,\n            )\n        return cast(HypothesizerState, inputs)\n\n    def format_result(self, result: HypothesizerState) -&gt; str:\n        return result.get(\n            \"solution\", \"Hypothesizer failed to return a solution\"\n        )\n\n    def parse_visited_sites(self, raw_search_results) -&gt; set[str]:\n        visited_sites = set()\n        try:\n            if isinstance(raw_search_results, str):\n                results_list = ast.literal_eval(raw_search_results)\n            else:\n                results_list = raw_search_results\n            # Each item typically might have \"link\", \"title\", \"snippet\"\n            for item in results_list:\n                link = item.get(\"link\")\n                if link:\n                    visited_sites.add(link)\n        except (ValueError, SyntaxError, TypeError):\n            # If it's not valid Python syntax or something else goes wrong\n            print(\"[DEBUG] Could not parse search results as Python list.\")\n            print(\"[DEBUG] raw_search_results:\", raw_search_results)\n\n        return visited_sites\n\n    def agent1_generate_solution(\n        self, state: HypothesizerState\n    ) -&gt; HypothesizerState:\n        \"\"\"Agent 1: Hypothesizer.\"\"\"\n        print(\n            f\"[iteration {state['current_iteration'] + 1}] Entering agent1_generate_solution. Iteration: {state['current_iteration'] + 1}\"\n        )\n\n        current_iter = state[\"current_iteration\"]\n        user_content = f\"Question: {state['question']}\\n\"\n\n        if current_iter &gt; 0:\n            user_content += (\n                f\"\\nPrevious solution: {state['agent1_solution'][-1]}\"\n            )\n            user_content += f\"\\nCritique: {state['agent2_critiques'][-1]}\"\n            user_content += (\n                f\"\\nCompetitor perspective: {state['agent3_perspectives'][-1]}\"\n            )\n\n            user_content += (\n                \"\\n\\n**You must explicitly list how this new solution differs from the previous solution,** \"\n                \"point by point, explaining what changes were made in response to the critique and competitor perspective.\"\n                \"\\nAfterward, provide your updated solution.\"\n            )\n        else:\n            user_content += \"Research this problem and generate a solution.\"\n\n        search_query = self.strllm.invoke(\n            f\"Here is a problem description: {state['question']}. Turn it into a short query to be fed into a search engine.\"\n        )\n        if '\"' in search_query:\n            search_query = search_query.split('\"')[1]\n        raw_search_results = self.search_tool.text(\n            search_query or state[\"question\"]\n        )\n        user_content += f\"\\nSearch results: {raw_search_results}\"\n\n        # Parse the results if possible, so we can collect URLs\n        visited_sites = self.parse_visited_sites(raw_search_results)\n\n        # Provide a system message to define this agent's role\n        messages = [\n            SystemMessage(content=self.hypothesizer_prompt),\n            HumanMessage(content=user_content),\n        ]\n        solution = self.strllm.invoke(messages)\n\n        # Print the entire solution in green\n        print(f\"{GREEN}[Agent1 - Hypothesizer solution]\\n{solution}{RESET}\")\n        print(\n            f\"[iteration {state['current_iteration'] + 1}] Exiting agent1_generate_solution.\"\n        )\n        return {\n            \"agent1_solution\": [solution],\n            \"question_search_query\": search_query,\n            \"visited_sites\": visited_sites,\n        }\n\n    def agent2_critique(self, state: HypothesizerState) -&gt; HypothesizerState:\n        \"\"\"Agent 2: Critic.\"\"\"\n        print(\n            f\"[iteration {state['current_iteration'] + 1}] Entering agent2_critique.\"\n        )\n\n        solution = state[\"agent1_solution\"][-1]\n        user_content = (\n            f\"Question: {state['question']}\\n\"\n            f\"Proposed solution: {solution}\\n\"\n            \"Provide a detailed critique of this solution. Identify potential flaws, assumptions, and areas for improvement.\"\n        )\n\n        fact_check_query = f\"fact check {state['question_search_query']} solution effectiveness\"\n\n        fact_check_results = self.search_tool.text(fact_check_query)\n        visited_sites = self.parse_visited_sites(fact_check_results)\n        user_content += f\"\\nFact check results: {fact_check_results}\"\n\n        messages = [\n            SystemMessage(content=self.critic_prompt),\n            HumanMessage(content=user_content),\n        ]\n        critique = self.strllm.invoke(messages)\n\n        # Print the entire critique in blue\n        print(f\"{BLUE}[Agent2 - Critic]\\n{critique}{RESET}\")\n        print(\n            f\"[iteration {state['current_iteration'] + 1}] Exiting agent2_critique.\"\n        )\n        return {\n            \"agent2_critiques\": [critique],\n            \"visited_sites\": visited_sites,\n        }\n\n    def agent3_competitor_perspective(\n        self, state: HypothesizerState\n    ) -&gt; HypothesizerState:\n        \"\"\"Agent 3: Competitor/Stakeholder Simulator.\"\"\"\n        print(\n            f\"[iteration {state['current_iteration'] + 1}] Entering agent3_competitor_perspective.\"\n        )\n\n        solution = state[\"agent1_solution\"][-1]\n        critique = state[\"agent2_critiques\"][-1]\n\n        user_content = (\n            f\"Question: {state['question']}\\n\"\n            f\"Proposed solution: {solution}\\n\"\n            f\"Critique: {critique}\\n\"\n            \"Simulate how a competitor, government agency, or other stakeholder might respond to this solution.\"\n        )\n\n        competitor_search_query = (\n            f\"competitor responses to {state['question_search_query']}\"\n        )\n\n        competitor_info = self.search_tool.text(competitor_search_query)\n        visited_sites = self.parse_visited_sites(competitor_info)\n        user_content += f\"\\nCompetitor information: {competitor_info}\"\n\n        messages = [\n            SystemMessage(content=self.competitor_prompt),\n            HumanMessage(content=user_content),\n        ]\n        perspective = self.strllm.invoke(messages)\n\n        # Print the entire perspective in red\n        print(\n            f\"{RED}[Agent3 - Competitor/Stakeholder Perspective]\\n{perspective}{RESET}\"\n        )\n        print(\n            f\"[iteration {state['current_iteration'] + 1}] Exiting agent3_competitor_perspective.\"\n        )\n        return {\n            \"agent3_perspectives\": [perspective],\n            \"visited_sites\": visited_sites,\n        }\n\n    def increment_iteration(\n        self, state: HypothesizerState\n    ) -&gt; HypothesizerState:\n        current_iteration = state[\"current_iteration\"] + 1\n        return {\"current_iteration\": current_iteration}\n\n    def generate_solution(self, state: HypothesizerState) -&gt; HypothesizerState:\n        \"\"\"Generate the overall, refined solution based on all iterations.\"\"\"\n        print(\n            f\"[iteration {state['current_iteration']}] Entering generate_solution.\"\n        )\n        prompt = f\"Original question: {state['question']}\\n\\n\"\n        prompt += \"Evolution of solutions:\\n\"\n\n        for i, (solution_text, critique_text, perspective_text) in enumerate(\n            zip(\n                state[\"agent1_solution\"],\n                state[\"agent2_critiques\"],\n                state[\"agent3_perspectives\"],\n            ),\n            start=1,\n        ):\n            prompt += f\"\\nIteration {i}:\\n\"\n            prompt += f\"Solution: {solution_text}\\n\"\n            prompt += f\"Critique: {critique_text}\\n\"\n            prompt += f\"Competitor perspective: {perspective_text}\\n\"\n\n        prompt += \"\\nBased on this iterative process, provide the overall, refined solution.\"\n\n        print(\n            f\"[iteration {state['current_iteration']}] Generating overall solution with LLM...\"\n        )\n        solution = self.strllm.invoke(prompt)\n        print(\n            f\"[iteration {state['current_iteration']}] Overall solution obtained. Preview:\",\n            solution[:200],\n            \"...\",\n        )\n\n        print(\n            f\"[iteration {state['current_iteration']}] Exiting generate_solution.\"\n        )\n        return {\"solution\": solution}\n\n    def print_visited_sites(\n        self, state: HypothesizerState\n    ) -&gt; HypothesizerState:\n        new_state = state.copy()\n        # all_sites = list(new_state[\"visited_sites\"])\n        # print(\"[DEBUG] Visited Sites:\")\n        # for s in all_sites:\n        #     print(\"  \", s)\n        return new_state\n\n    def summarize_process_as_latex(\n        self, state: HypothesizerState\n    ) -&gt; HypothesizerState:\n        \"\"\"\n        Summarize how the solution changed over time, referencing\n        each iteration's critique and competitor perspective,\n        then produce a final LaTeX document.\n        \"\"\"\n        print(\"Entering summarize_process_as_latex.\")\n        # Build a single string describing the entire iterative process\n        iteration_details = \"\"\n        for i, (sol, crit, comp) in enumerate(\n            zip(\n                state[\"agent1_solution\"],\n                state[\"agent2_critiques\"],\n                state[\"agent3_perspectives\"],\n            ),\n            start=1,\n        ):\n            iteration_details += (\n                f\"\\\\subsection*{{Iteration {i}}}\\n\\n\"\n                f\"\\\\textbf{{Solution:}}\\\\\\\\\\n{sol}\\n\\n\"\n                f\"\\\\textbf{{Critique:}}\\\\\\\\\\n{crit}\\n\\n\"\n                f\"\\\\textbf{{Competitor Perspective:}}\\\\\\\\\\n{comp}\\n\\n\"\n            )\n\n        # -----------------------------\n        # Write iteration_details to disk as .txt\n        # -----------------------------\n        timestamp_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n        txt_filename = Path(\n            self.workspace,\n            f\"iteration_details_{timestamp_str}_chat_history.txt\",\n        )\n        with open(txt_filename, \"w\", encoding=\"utf-8\") as f:\n            f.write(iteration_details)\n\n        print(f\"Wrote iteration details to {txt_filename}.\")\n\n        # Prompt the LLM to produce a LaTeX doc\n        # We'll just pass it as a single string to the LLM;\n        # you could also do system+human messages if you prefer.\n        prompt = f\"\"\"\\\n            You are a system that produces a FULL LaTeX document.\n            Here is information about a multi-iteration process:\n\n            Original question: {state[\"question\"]}\n\n            Below are the solutions, critiques, and competitor perspectives from each iteration:\n\n            {iteration_details}\n\n            The solution we arrived at was:\n\n            {state[\"solution\"]}\n\n            Now produce a valid LaTeX document.  Be sure to use a table of contents.\n            It must start with an Executive Summary (that may be multiple pages) which summarizes\n            the entire iterative process.  Following that, we should include the solution in full,\n            not summarized, but reformatted for appropriate LaTeX.  And then, finally (and this will be\n            quite long), we must take all the steps - solutions, critiques, and competitor perspectives\n            and *NOT SUMMARIZE THEM* but merely reformat them for the reader.  This will be in an Appendix\n            of the full content of the steps.  Finally, include a listing of all of the websites we\n            used in our research.\n\n            You must ONLY RETURN LaTeX, nothing else.  It must be valid LaTeX syntax!\n\n            Your output should start with:\n            \\\\documentclass{{article}}\n            \\\\usepackage[margin=1in]{{geometry}}\n            etc.\n\n            It must compile without errors under pdflatex.\n        \"\"\"\n\n        # Now produce a valid LaTeX document that nicely summarizes this entire iterative process.\n        # It must include the overall solution in full, not summarized, but reformatted for appropriate\n        # LaTeX. The summarization is for the other steps.\n\n        # all_visited_sites = list(state[\"visited_sites\"])\n        # (Optional) remove duplicates by converting to a set, then back to a list\n        # visited_sites_unique = list(set(all_visited_sites))\n        # if visited_sites_unique:\n        #     websites_latex = \"\\\\section*{Websites Visited}\\\\begin{itemize}\\n\"\n        #     for url in visited_sites_unique:\n        #         print(f\"We visited: {url}\")\n        #         # Use \\url{} to handle special characters in URLs\n        #         websites_latex += f\"\\\\item \\\\url{{{url}}}\\n\"\n        #     websites_latex += \"\\\\end{itemize}\\n\\n\"\n        # else:\n        #     # If no sites visited, or the list is empty\n        #     websites_latex = (\n        #         \"\\\\section*{Websites Visited}\\nNo sites were visited.\\n\\n\"\n        #     )\n        # print(websites_latex)\n        websites_latex = \"\"\n\n        # Ask the LLM to produce *only* LaTeX content\n        latex_response = self.strllm.invoke(prompt)\n\n        latex_doc = latex_response\n\n        def inject_into_latex(original_tex: str, injection: str) -&gt; str:\n            \"\"\"\n            Find the last occurrence of '\\\\end{document}' in 'original_tex'\n            and insert 'injection' right before it.\n            If '\\\\end{document}' is not found, just append the injection at the end.\n            \"\"\"\n            injection_index = original_tex.rfind(r\"\\end{document}\")\n            if injection_index == -1:\n                # If the LLM didn't include \\end{document}, just append\n                return original_tex + \"\\n\" + injection\n            else:\n                # Insert right before \\end{document}\n                return (\n                    original_tex[:injection_index]\n                    + \"\\n\"\n                    + injection\n                    + \"\\n\"\n                    + original_tex[injection_index:]\n                )\n\n        final_latex = inject_into_latex(latex_doc, websites_latex)\n\n        print(\n            f\"[iteration {state['current_iteration']}] Received LaTeX from LLM. Preview:\"\n        )\n        print(latex_response[:300], \"...\")\n        print(\n            f\"[iteration {state['current_iteration']}] Exiting summarize_process_as_latex.\"\n        )\n        return {\"summary_report\": final_latex}\n\n    def _build_graph(self):\n        # Add nodes\n        self.add_node(self.agent1_generate_solution, \"agent1\")\n        self.add_node(self.agent2_critique, \"agent2\")\n        self.add_node(self.agent3_competitor_perspective, \"agent3\")\n        self.add_node(self.increment_iteration, \"increment_iteration\")\n        self.add_node(self.generate_solution, \"finalize\")\n        self.add_node(self.print_visited_sites, \"print_sites\")\n        self.add_node(self.summarize_process_as_latex, \"summarize_as_latex\")\n\n        # Add simple edges for the known flow\n        self.graph.add_edge(\"agent1\", \"agent2\")\n        self.graph.add_edge(\"agent2\", \"agent3\")\n        self.graph.add_edge(\"agent3\", \"increment_iteration\")\n\n        # Then from increment_iteration, we have a conditional:\n        # If we 'continue', we go back to agent1\n        # If we 'finish', we jump to the finalize node\n        self.graph.add_conditional_edges(\n            \"increment_iteration\",\n            should_continue,\n            {\"continue\": \"agent1\", \"finish\": \"finalize\"},\n        )\n\n        self.graph.add_edge(\"finalize\", \"summarize_as_latex\")\n        self.graph.add_edge(\"summarize_as_latex\", \"print_sites\")\n        # self.graph.add_edge(\"summarize_as_latex\", \"compile_pdf\")\n        # self.graph.add_edge(\"compile_pdf\", \"print_sites\")\n\n        # Set the entry point\n        self.graph.set_entry_point(\"agent1\")\n        self.graph.set_finish_point(\"print_sites\")\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.hypothesizer_agent.HypothesizerAgent.agent1_generate_solution","title":"<code>agent1_generate_solution(state)</code>","text":"<p>Agent 1: Hypothesizer.</p> Source code in <code>src/ursa/agents/hypothesizer_agent.py</code> <pre><code>def agent1_generate_solution(\n    self, state: HypothesizerState\n) -&gt; HypothesizerState:\n    \"\"\"Agent 1: Hypothesizer.\"\"\"\n    print(\n        f\"[iteration {state['current_iteration'] + 1}] Entering agent1_generate_solution. Iteration: {state['current_iteration'] + 1}\"\n    )\n\n    current_iter = state[\"current_iteration\"]\n    user_content = f\"Question: {state['question']}\\n\"\n\n    if current_iter &gt; 0:\n        user_content += (\n            f\"\\nPrevious solution: {state['agent1_solution'][-1]}\"\n        )\n        user_content += f\"\\nCritique: {state['agent2_critiques'][-1]}\"\n        user_content += (\n            f\"\\nCompetitor perspective: {state['agent3_perspectives'][-1]}\"\n        )\n\n        user_content += (\n            \"\\n\\n**You must explicitly list how this new solution differs from the previous solution,** \"\n            \"point by point, explaining what changes were made in response to the critique and competitor perspective.\"\n            \"\\nAfterward, provide your updated solution.\"\n        )\n    else:\n        user_content += \"Research this problem and generate a solution.\"\n\n    search_query = self.strllm.invoke(\n        f\"Here is a problem description: {state['question']}. Turn it into a short query to be fed into a search engine.\"\n    )\n    if '\"' in search_query:\n        search_query = search_query.split('\"')[1]\n    raw_search_results = self.search_tool.text(\n        search_query or state[\"question\"]\n    )\n    user_content += f\"\\nSearch results: {raw_search_results}\"\n\n    # Parse the results if possible, so we can collect URLs\n    visited_sites = self.parse_visited_sites(raw_search_results)\n\n    # Provide a system message to define this agent's role\n    messages = [\n        SystemMessage(content=self.hypothesizer_prompt),\n        HumanMessage(content=user_content),\n    ]\n    solution = self.strllm.invoke(messages)\n\n    # Print the entire solution in green\n    print(f\"{GREEN}[Agent1 - Hypothesizer solution]\\n{solution}{RESET}\")\n    print(\n        f\"[iteration {state['current_iteration'] + 1}] Exiting agent1_generate_solution.\"\n    )\n    return {\n        \"agent1_solution\": [solution],\n        \"question_search_query\": search_query,\n        \"visited_sites\": visited_sites,\n    }\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.hypothesizer_agent.HypothesizerAgent.agent2_critique","title":"<code>agent2_critique(state)</code>","text":"<p>Agent 2: Critic.</p> Source code in <code>src/ursa/agents/hypothesizer_agent.py</code> <pre><code>def agent2_critique(self, state: HypothesizerState) -&gt; HypothesizerState:\n    \"\"\"Agent 2: Critic.\"\"\"\n    print(\n        f\"[iteration {state['current_iteration'] + 1}] Entering agent2_critique.\"\n    )\n\n    solution = state[\"agent1_solution\"][-1]\n    user_content = (\n        f\"Question: {state['question']}\\n\"\n        f\"Proposed solution: {solution}\\n\"\n        \"Provide a detailed critique of this solution. Identify potential flaws, assumptions, and areas for improvement.\"\n    )\n\n    fact_check_query = f\"fact check {state['question_search_query']} solution effectiveness\"\n\n    fact_check_results = self.search_tool.text(fact_check_query)\n    visited_sites = self.parse_visited_sites(fact_check_results)\n    user_content += f\"\\nFact check results: {fact_check_results}\"\n\n    messages = [\n        SystemMessage(content=self.critic_prompt),\n        HumanMessage(content=user_content),\n    ]\n    critique = self.strllm.invoke(messages)\n\n    # Print the entire critique in blue\n    print(f\"{BLUE}[Agent2 - Critic]\\n{critique}{RESET}\")\n    print(\n        f\"[iteration {state['current_iteration'] + 1}] Exiting agent2_critique.\"\n    )\n    return {\n        \"agent2_critiques\": [critique],\n        \"visited_sites\": visited_sites,\n    }\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.hypothesizer_agent.HypothesizerAgent.agent3_competitor_perspective","title":"<code>agent3_competitor_perspective(state)</code>","text":"<p>Agent 3: Competitor/Stakeholder Simulator.</p> Source code in <code>src/ursa/agents/hypothesizer_agent.py</code> <pre><code>def agent3_competitor_perspective(\n    self, state: HypothesizerState\n) -&gt; HypothesizerState:\n    \"\"\"Agent 3: Competitor/Stakeholder Simulator.\"\"\"\n    print(\n        f\"[iteration {state['current_iteration'] + 1}] Entering agent3_competitor_perspective.\"\n    )\n\n    solution = state[\"agent1_solution\"][-1]\n    critique = state[\"agent2_critiques\"][-1]\n\n    user_content = (\n        f\"Question: {state['question']}\\n\"\n        f\"Proposed solution: {solution}\\n\"\n        f\"Critique: {critique}\\n\"\n        \"Simulate how a competitor, government agency, or other stakeholder might respond to this solution.\"\n    )\n\n    competitor_search_query = (\n        f\"competitor responses to {state['question_search_query']}\"\n    )\n\n    competitor_info = self.search_tool.text(competitor_search_query)\n    visited_sites = self.parse_visited_sites(competitor_info)\n    user_content += f\"\\nCompetitor information: {competitor_info}\"\n\n    messages = [\n        SystemMessage(content=self.competitor_prompt),\n        HumanMessage(content=user_content),\n    ]\n    perspective = self.strllm.invoke(messages)\n\n    # Print the entire perspective in red\n    print(\n        f\"{RED}[Agent3 - Competitor/Stakeholder Perspective]\\n{perspective}{RESET}\"\n    )\n    print(\n        f\"[iteration {state['current_iteration'] + 1}] Exiting agent3_competitor_perspective.\"\n    )\n    return {\n        \"agent3_perspectives\": [perspective],\n        \"visited_sites\": visited_sites,\n    }\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.hypothesizer_agent.HypothesizerAgent.generate_solution","title":"<code>generate_solution(state)</code>","text":"<p>Generate the overall, refined solution based on all iterations.</p> Source code in <code>src/ursa/agents/hypothesizer_agent.py</code> <pre><code>def generate_solution(self, state: HypothesizerState) -&gt; HypothesizerState:\n    \"\"\"Generate the overall, refined solution based on all iterations.\"\"\"\n    print(\n        f\"[iteration {state['current_iteration']}] Entering generate_solution.\"\n    )\n    prompt = f\"Original question: {state['question']}\\n\\n\"\n    prompt += \"Evolution of solutions:\\n\"\n\n    for i, (solution_text, critique_text, perspective_text) in enumerate(\n        zip(\n            state[\"agent1_solution\"],\n            state[\"agent2_critiques\"],\n            state[\"agent3_perspectives\"],\n        ),\n        start=1,\n    ):\n        prompt += f\"\\nIteration {i}:\\n\"\n        prompt += f\"Solution: {solution_text}\\n\"\n        prompt += f\"Critique: {critique_text}\\n\"\n        prompt += f\"Competitor perspective: {perspective_text}\\n\"\n\n    prompt += \"\\nBased on this iterative process, provide the overall, refined solution.\"\n\n    print(\n        f\"[iteration {state['current_iteration']}] Generating overall solution with LLM...\"\n    )\n    solution = self.strllm.invoke(prompt)\n    print(\n        f\"[iteration {state['current_iteration']}] Overall solution obtained. Preview:\",\n        solution[:200],\n        \"...\",\n    )\n\n    print(\n        f\"[iteration {state['current_iteration']}] Exiting generate_solution.\"\n    )\n    return {\"solution\": solution}\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.hypothesizer_agent.HypothesizerAgent.summarize_process_as_latex","title":"<code>summarize_process_as_latex(state)</code>","text":"<p>Summarize how the solution changed over time, referencing each iteration's critique and competitor perspective, then produce a final LaTeX document.</p> Source code in <code>src/ursa/agents/hypothesizer_agent.py</code> <pre><code>def summarize_process_as_latex(\n    self, state: HypothesizerState\n) -&gt; HypothesizerState:\n    \"\"\"\n    Summarize how the solution changed over time, referencing\n    each iteration's critique and competitor perspective,\n    then produce a final LaTeX document.\n    \"\"\"\n    print(\"Entering summarize_process_as_latex.\")\n    # Build a single string describing the entire iterative process\n    iteration_details = \"\"\n    for i, (sol, crit, comp) in enumerate(\n        zip(\n            state[\"agent1_solution\"],\n            state[\"agent2_critiques\"],\n            state[\"agent3_perspectives\"],\n        ),\n        start=1,\n    ):\n        iteration_details += (\n            f\"\\\\subsection*{{Iteration {i}}}\\n\\n\"\n            f\"\\\\textbf{{Solution:}}\\\\\\\\\\n{sol}\\n\\n\"\n            f\"\\\\textbf{{Critique:}}\\\\\\\\\\n{crit}\\n\\n\"\n            f\"\\\\textbf{{Competitor Perspective:}}\\\\\\\\\\n{comp}\\n\\n\"\n        )\n\n    # -----------------------------\n    # Write iteration_details to disk as .txt\n    # -----------------------------\n    timestamp_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    txt_filename = Path(\n        self.workspace,\n        f\"iteration_details_{timestamp_str}_chat_history.txt\",\n    )\n    with open(txt_filename, \"w\", encoding=\"utf-8\") as f:\n        f.write(iteration_details)\n\n    print(f\"Wrote iteration details to {txt_filename}.\")\n\n    # Prompt the LLM to produce a LaTeX doc\n    # We'll just pass it as a single string to the LLM;\n    # you could also do system+human messages if you prefer.\n    prompt = f\"\"\"\\\n        You are a system that produces a FULL LaTeX document.\n        Here is information about a multi-iteration process:\n\n        Original question: {state[\"question\"]}\n\n        Below are the solutions, critiques, and competitor perspectives from each iteration:\n\n        {iteration_details}\n\n        The solution we arrived at was:\n\n        {state[\"solution\"]}\n\n        Now produce a valid LaTeX document.  Be sure to use a table of contents.\n        It must start with an Executive Summary (that may be multiple pages) which summarizes\n        the entire iterative process.  Following that, we should include the solution in full,\n        not summarized, but reformatted for appropriate LaTeX.  And then, finally (and this will be\n        quite long), we must take all the steps - solutions, critiques, and competitor perspectives\n        and *NOT SUMMARIZE THEM* but merely reformat them for the reader.  This will be in an Appendix\n        of the full content of the steps.  Finally, include a listing of all of the websites we\n        used in our research.\n\n        You must ONLY RETURN LaTeX, nothing else.  It must be valid LaTeX syntax!\n\n        Your output should start with:\n        \\\\documentclass{{article}}\n        \\\\usepackage[margin=1in]{{geometry}}\n        etc.\n\n        It must compile without errors under pdflatex.\n    \"\"\"\n\n    # Now produce a valid LaTeX document that nicely summarizes this entire iterative process.\n    # It must include the overall solution in full, not summarized, but reformatted for appropriate\n    # LaTeX. The summarization is for the other steps.\n\n    # all_visited_sites = list(state[\"visited_sites\"])\n    # (Optional) remove duplicates by converting to a set, then back to a list\n    # visited_sites_unique = list(set(all_visited_sites))\n    # if visited_sites_unique:\n    #     websites_latex = \"\\\\section*{Websites Visited}\\\\begin{itemize}\\n\"\n    #     for url in visited_sites_unique:\n    #         print(f\"We visited: {url}\")\n    #         # Use \\url{} to handle special characters in URLs\n    #         websites_latex += f\"\\\\item \\\\url{{{url}}}\\n\"\n    #     websites_latex += \"\\\\end{itemize}\\n\\n\"\n    # else:\n    #     # If no sites visited, or the list is empty\n    #     websites_latex = (\n    #         \"\\\\section*{Websites Visited}\\nNo sites were visited.\\n\\n\"\n    #     )\n    # print(websites_latex)\n    websites_latex = \"\"\n\n    # Ask the LLM to produce *only* LaTeX content\n    latex_response = self.strllm.invoke(prompt)\n\n    latex_doc = latex_response\n\n    def inject_into_latex(original_tex: str, injection: str) -&gt; str:\n        \"\"\"\n        Find the last occurrence of '\\\\end{document}' in 'original_tex'\n        and insert 'injection' right before it.\n        If '\\\\end{document}' is not found, just append the injection at the end.\n        \"\"\"\n        injection_index = original_tex.rfind(r\"\\end{document}\")\n        if injection_index == -1:\n            # If the LLM didn't include \\end{document}, just append\n            return original_tex + \"\\n\" + injection\n        else:\n            # Insert right before \\end{document}\n            return (\n                original_tex[:injection_index]\n                + \"\\n\"\n                + injection\n                + \"\\n\"\n                + original_tex[injection_index:]\n            )\n\n    final_latex = inject_into_latex(latex_doc, websites_latex)\n\n    print(\n        f\"[iteration {state['current_iteration']}] Received LaTeX from LLM. Preview:\"\n    )\n    print(latex_response[:300], \"...\")\n    print(\n        f\"[iteration {state['current_iteration']}] Exiting summarize_process_as_latex.\"\n    )\n    return {\"summary_report\": final_latex}\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.lammps_agent","title":"<code>lammps_agent</code>","text":""},{"location":"api_reference/agents/#ursa.agents.lammps_agent.LammpsAgent","title":"<code>LammpsAgent</code>","text":"<p>               Bases: <code>BaseAgent[LammpsState]</code></p> Source code in <code>src/ursa/agents/lammps_agent.py</code> <pre><code>class LammpsAgent(BaseAgent[LammpsState]):\n    state_type = LammpsState\n\n    def __init__(\n        self,\n        llm: BaseChatModel,\n        potential_files: Optional[list[str]] = None,\n        pair_style: Optional[str] = None,\n        pair_coeff: Optional[str] = None,\n        max_potentials: int = 5,\n        max_fix_attempts: int = 10,\n        find_potential_only: bool = False,\n        data_file: str = None,\n        data_max_lines: int = 50,\n        ngpus: int = -1,\n        mpi_procs: int = 8,\n        workspace: str = \"./workspace\",\n        lammps_cmd: str = \"lmp_mpi\",\n        mpirun_cmd: str = \"mpirun\",\n        tiktoken_model: str = \"gpt-5-mini\",\n        max_tokens: int = 200000,\n        summarize_results: bool = True,\n        **kwargs,\n    ):\n        if not working:\n            raise ImportError(\n                \"LAMMPS agent requires the atomman and trafilatura dependencies. These can be installed using 'pip install ursa-ai[lammps]' or, if working from a local installation, 'pip install -e .[lammps]' .\"\n            )\n\n        super().__init__(llm, **kwargs)\n\n        self.user_potential_files = potential_files\n        self.user_pair_style = pair_style\n        self.user_pair_coeff = pair_coeff\n        self.use_user_potential = (\n            potential_files is not None\n            and pair_style is not None\n            and pair_coeff is not None\n        )\n\n        self.max_potentials = max_potentials\n        self.max_fix_attempts = max_fix_attempts\n        self.find_potential_only = find_potential_only\n        self.data_file = data_file\n        self.data_max_lines = data_max_lines\n        self.ngpus = ngpus\n        self.mpi_procs = mpi_procs\n        self.lammps_cmd = lammps_cmd\n        self.mpirun_cmd = mpirun_cmd\n        self.tiktoken_model = tiktoken_model\n        self.max_tokens = max_tokens\n        self.summarize_results = summarize_results\n\n        self.console = Console()\n\n        self.pair_styles = [\n            \"eam\",\n            \"eam/alloy\",\n            \"eam/fs\",\n            \"meam\",\n            \"adp\",\n            \"kim\",\n            \"snap\",\n            \"quip\",\n            \"mlip\",\n            \"pace\",\n            \"nep\",\n        ]\n\n        self.workspace = workspace\n        os.makedirs(self.workspace, exist_ok=True)\n\n        self.str_parser = StrOutputParser()\n\n        self.summ_chain = (\n            ChatPromptTemplate.from_template(\n                \"Here is some data about an interatomic potential: {metadata}\\n\\n\"\n                \"Briefly summarize why it could be useful for this task: {simulation_task}.\"\n            )\n            | self.llm\n            | self.str_parser\n        )\n\n        self.choose_chain = (\n            ChatPromptTemplate.from_template(\n                \"Here are the summaries of a certain number of interatomic potentials: {summaries_combined}\\n\\n\"\n                \"Pick one potential which would be most useful for this task: {simulation_task}.\\n\\n\"\n                \"Return your answer **only** as valid JSON, with no extra text or formatting.\\n\\n\"\n                \"Use this exact schema:\\n\"\n                \"{{\\n\"\n                '  \"Chosen index\": &lt;int&gt;,\\n'\n                '  \"rationale\": \"&lt;string&gt;\",\\n'\n                '  \"Potential name\": \"&lt;string&gt;\"\\n'\n                \"}}\\n\"\n            )\n            | self.llm\n            | self.str_parser\n        )\n\n        self.author_chain = (\n            ChatPromptTemplate.from_template(\n                \"Your task is to write a LAMMPS input file for this purpose: {simulation_task}.\\n\"\n                \"Note that all potential files are in the './' directory.\\n\"\n                \"Here is some information about the pair_style and pair_coeff that might be useful in writing the input file: {pair_info}.\\n\"\n                \"If a template for the input file is provided, you should adapt it appropriately to meet the task requirements.\\n\"\n                \"Template provided (if any): {template}\\n\"\n                \"If a data file is provided, use it in the input script via the 'read_data' command.\\n\"\n                \"Name of data file (if any): {data_file}\\n\"\n                \"First few lines of data file (if any):\\n{data_content}\\n\"\n                \"Ensure that all logs are recorded in a './log.lammps' file.\\n\"\n                \"To create the log file, use may use the 'log ./log.lammps' command. \\n\"\n                \"Return your answer **only** as valid JSON, with no extra text or formatting.\\n\"\n                \"IMPORTANT: Properly escape all special characters in the input_script string (use \\\\n for newlines, \\\\\\\\ for backslashes, etc.).\\n\"\n                \"Use this exact schema:\\n\"\n                \"{{\\n\"\n                '  \"input_script\": \"&lt;string&gt;\"\\n'\n                \"}}\\n\"\n            )\n            | self.llm\n            | self.str_parser\n        )\n\n        self.fix_chain = (\n            ChatPromptTemplate.from_template(\n                \"You are part of a larger scientific workflow whose purpose is to accomplish this task: {simulation_task}\\n\"\n                \"Multiple attempts at writing and running a LAMMPS input file have been made.\\n\"\n                \"Here is the run history across attempts (each includes the input script and its stdout/stderr):{err_message}\\n\"\n                \"Use the history to identify what changed between attempts and avoid repeating failed approaches.\\n\"\n                \"Your task is to write a new input file that resolves the latest error.\\n\"\n                \"Note that all potential files are in the './' directory.\\n\"\n                \"Here is some information about the pair_style and pair_coeff that might be useful in writing the input file: {pair_info}.\\n\"\n                \"If a template for the input file is provided, you should adapt it appropriately to meet the task requirements.\\n\"\n                \"Template provided (if any): {template}\\n\"\n                \"If a data file is provided, use it in the input script via the 'read_data' command.\\n\"\n                \"Name of data file (if any): {data_file}\\n\"\n                \"First few lines of data file (if any):\\n{data_content}\\n\"\n                \"Ensure that all logs are recorded in a './log.lammps' file.\\n\"\n                \"To create the log file, use may use the 'log ./log.lammps' command. \\n\"\n                \"Return your answer **only** as valid JSON, with no extra text or formatting.\\n\"\n                \"IMPORTANT: Properly escape all special characters in the input_script string (use \\\\n for newlines, \\\\\\\\ for backslashes, etc.).\\n\"\n                \"Use this exact schema:\\n\"\n                \"{{\\n\"\n                '  \"input_script\": \"&lt;string&gt;\"\\n'\n                \"}}\\n\"\n            )\n            | self.llm\n            | self.str_parser\n        )\n\n    def _section(self, title: str):\n        self.console.print(Rule(f\"[bold cyan]{title}[/bold cyan]\"))\n\n    def _panel(self, title: str, body: str, style: str = \"cyan\"):\n        self.console.print(\n            Panel(body, title=f\"[bold]{title}[/bold]\", border_style=style)\n        )\n\n    def _code_panel(\n        self,\n        title: str,\n        code: str,\n        language: str = \"bash\",\n        style: str = \"magenta\",\n    ):\n        syn = Syntax(\n            code, language, theme=\"monokai\", line_numbers=True, word_wrap=True\n        )\n        self.console.print(\n            Panel(syn, title=f\"[bold]{title}[/bold]\", border_style=style)\n        )\n\n    def _diff_panel(self, old: str, new: str, title: str = \"LAMMPS input diff\"):\n        diff = \"\\n\".join(\n            difflib.unified_diff(\n                old.splitlines(),\n                new.splitlines(),\n                fromfile=\"in.lammps (before)\",\n                tofile=\"in.lammps (after)\",\n                lineterm=\"\",\n            )\n        )\n        if not diff.strip():\n            diff = \"(no changes)\"\n        syn = Syntax(\n            diff, \"diff\", theme=\"monokai\", line_numbers=False, word_wrap=True\n        )\n        self.console.print(\n            Panel(syn, title=f\"[bold]{title}[/bold]\", border_style=\"cyan\")\n        )\n\n    @staticmethod\n    def _safe_json_loads(s: str) -&gt; dict[str, Any]:\n        s = s.strip()\n        if s.startswith(\"```\"):\n            s = s.strip(\"`\")\n            i = s.find(\"\\n\")\n            if i != -1:\n                s = s[i + 1 :].strip()\n        return json.loads(s)\n\n    def _read_and_trim_data_file(self, data_file_path: str) -&gt; str:\n        \"\"\"Read LAMMPS data file and trim to token limit for LLM context.\"\"\"\n        if os.path.exists(data_file_path):\n            with open(data_file_path, \"r\") as f:\n                content = f.read()\n            lines = content.splitlines()\n            if len(lines) &gt; self.data_max_lines:\n                content = \"\\n\".join(lines[: self.data_max_lines])\n                print(\n                    f\"Data file trimmed from {len(lines)} to {self.data_max_lines} lines\"\n                )\n            return content\n        else:\n            return \"Could not read data file.\"\n\n    def _copy_data_file(self, data_file_path: str) -&gt; str:\n        \"\"\"Copy data file to workspace and return new path.\"\"\"\n        if not os.path.exists(data_file_path):\n            raise FileNotFoundError(f\"Data file not found: {data_file_path}\")\n\n        filename = os.path.basename(data_file_path)\n        dest_path = os.path.join(self.workspace, filename)\n        os.system(f\"cp {data_file_path} {dest_path}\")\n        print(f\"Data file copied to workspace: {dest_path}\")\n        return dest_path\n\n    def _copy_user_potential_files(self):\n        \"\"\"Copy user-provided potential files to workspace.\"\"\"\n        print(\"Copying user-provided potential files to workspace...\")\n        for pot_file in self.user_potential_files:\n            if not os.path.exists(pot_file):\n                raise FileNotFoundError(f\"Potential file not found: {pot_file}\")\n\n            filename = os.path.basename(pot_file)\n            dest_path = os.path.join(self.workspace, filename)\n\n            try:\n                os.system(f\"cp {pot_file} {dest_path}\")\n                print(f\"Potential files copied to workspace: {dest_path}\")\n            except Exception as e:\n                print(f\"Error copying {filename}: {e}\")\n                raise\n\n    def _create_user_potential_wrapper(self, state: LammpsState) -&gt; LammpsState:\n        \"\"\"Create a wrapper object for user-provided potential to match atomman interface.\"\"\"\n        self._copy_user_potential_files()\n\n        # Create a simple object that mimics the atomman potential interface\n        class UserPotential:\n            def __init__(self, pair_style, pair_coeff):\n                self._pair_style = pair_style\n                self._pair_coeff = pair_coeff\n\n            def pair_info(self):\n                return f\"pair_style {self._pair_style}\\npair_coeff {self._pair_coeff}\"\n\n        user_potential = UserPotential(\n            self.user_pair_style, self.user_pair_coeff\n        )\n\n        return {\n            **state,\n            \"chosen_potential\": user_potential,\n            \"fix_attempts\": 0,\n            \"run_history\": [],\n        }\n\n    def _fetch_and_trim_text(self, url: str) -&gt; str:\n        downloaded = trafilatura.fetch_url(url)\n        if not downloaded:\n            return \"No metadata available\"\n        text = trafilatura.extract(\n            downloaded,\n            include_comments=False,\n            include_tables=True,\n            include_links=False,\n            favor_recall=True,\n        )\n        if not text:\n            return \"No metadata available\"\n        text = text.strip()\n        try:\n            enc = tiktoken.encoding_for_model(self.tiktoken_model)\n            toks = enc.encode(text)\n            if len(toks) &gt; self.max_tokens:\n                toks = toks[: self.max_tokens]\n                text = enc.decode(toks)\n        except Exception:\n            pass\n        return text\n\n    def _entry_router(self, state: LammpsState) -&gt; dict:\n        # Check if using user-provided potential\n        if self.use_user_potential:\n            if self.find_potential_only:\n                raise Exception(\n                    \"Cannot set find_potential_only=True when providing your own potential!\"\n                )\n            print(\"Using user-provided potential files\")\n\n        if self.find_potential_only and state.get(\"chosen_potential\"):\n            raise Exception(\n                \"You cannot set find_potential_only=True and also specify your own potential!\"\n            )\n\n        if self.data_file:\n            try:\n                self._copy_data_file(self.data_file)\n            except Exception as e:\n                print(f\"Warning: Could not process data file: {e}\")\n\n        if not state.get(\"chosen_potential\"):\n            self.potential_summaries_dir = os.path.join(\n                self.workspace, \"potential_summaries\"\n            )\n            os.makedirs(self.potential_summaries_dir, exist_ok=True)\n        return {}\n\n    def _find_potentials(self, state: LammpsState) -&gt; LammpsState:\n        db = am.library.Database(remote=True)\n        matches = db.get_lammps_potentials(\n            pair_style=self.pair_styles, elements=state[\"elements\"]\n        )\n\n        return {\n            **state,\n            \"matches\": list(matches),\n            \"idx\": 0,\n            \"summaries\": [],\n            \"full_texts\": [],\n            \"fix_attempts\": 0,\n            \"run_history\": [],\n        }\n\n    def _should_summarize(self, state: LammpsState) -&gt; str:\n        matches = state.get(\"matches\", [])\n        i = state.get(\"idx\", 0)\n        if not matches:\n            print(\"No potentials found in NIST for this task. Exiting....\")\n            return \"done_no_matches\"\n        if i &lt; min(self.max_potentials, len(matches)):\n            return \"summarize_one\"\n        return \"summarize_done\"\n\n    def _summarize_one(self, state: LammpsState) -&gt; LammpsState:\n        i = state[\"idx\"]\n        self._section(f\"Summarizing potential #{i}\")\n        match = state[\"matches\"][i]\n        md = match.metadata()\n\n        if md.get(\"comments\") is None:\n            text = \"No metadata available\"\n            summary = \"No summary available\"\n        else:\n            lines = md[\"comments\"].split(\"\\n\")\n            url = lines[1] if len(lines) &gt; 1 else \"\"\n            text = (\n                self._fetch_and_trim_text(url)\n                if url\n                else \"No metadata available\"\n            )\n            summary = self.summ_chain.invoke({\n                \"metadata\": text,\n                \"simulation_task\": state[\"simulation_task\"],\n            })\n\n        summary_file = os.path.join(\n            self.potential_summaries_dir, \"potential_\" + str(i) + \".txt\"\n        )\n        with open(summary_file, \"w\") as f:\n            f.write(summary)\n\n        return {\n            **state,\n            \"idx\": i + 1,\n            \"summaries\": [*state[\"summaries\"], summary],\n            \"full_texts\": [*state[\"full_texts\"], text],\n        }\n\n    def _build_summaries(self, state: LammpsState) -&gt; LammpsState:\n        parts = []\n        for i, s in enumerate(state[\"summaries\"]):\n            rec = state[\"matches\"][i]\n            parts.append(f\"\\nSummary of potential #{i}: {rec.id}\\n{s}\\n\")\n        return {**state, \"summaries_combined\": \"\".join(parts)}\n\n    def _choose(self, state: LammpsState) -&gt; LammpsState:\n        self._section(\"Choosing potential\")\n        choice = self.choose_chain.invoke({\n            \"summaries_combined\": state[\"summaries_combined\"],\n            \"simulation_task\": state[\"simulation_task\"],\n        })\n        choice_dict = self._safe_json_loads(choice)\n        chosen_index = int(choice_dict[\"Chosen index\"])\n\n        chosen_potential = state[\"matches\"][chosen_index]\n\n        self._panel(\n            \"Chosen Potential\",\n            f\"[bold]Index:[/bold] {chosen_index}\\n[bold]ID:[/bold] {chosen_potential.id}\\n\\n[bold]Rationale:[/bold]\\n{choice_dict['rationale']}\",\n            style=\"green\",\n        )\n\n        out_file = os.path.join(self.potential_summaries_dir, \"Rationale.txt\")\n        with open(out_file, \"w\") as f:\n            f.write(f\"Chosen potential #{chosen_index}\")\n            f.write(\"\\n\")\n            f.write(\"Rationale for choosing this potential:\")\n            f.write(\"\\n\")\n            f.write(choice_dict[\"rationale\"])\n\n        return {**state, \"chosen_potential\": chosen_potential}\n\n    def _route_after_summarization(self, state: LammpsState) -&gt; str:\n        if self.find_potential_only:\n            return \"Exit\"\n        return \"continue_author\"\n\n    def _author(self, state: LammpsState) -&gt; LammpsState:\n        self._section(\"First attempt at writing LAMMPS input file\")\n\n        if not self.use_user_potential:\n            state[\"chosen_potential\"].download_files(self.workspace)\n        pair_info = state[\"chosen_potential\"].pair_info()\n\n        data_content = \"\"\n        if self.data_file:\n            data_content = self._read_and_trim_data_file(self.data_file)\n\n        authored_json = self.author_chain.invoke({\n            \"simulation_task\": state[\"simulation_task\"],\n            \"pair_info\": pair_info,\n            \"template\": state[\"template\"],\n            \"data_file\": self.data_file,\n            \"data_content\": data_content,\n        })\n        script_dict = self._safe_json_loads(authored_json)\n        input_script = script_dict[\"input_script\"]\n        with open(os.path.join(self.workspace, \"in.lammps\"), \"w\") as f:\n            f.write(input_script)\n\n        self._section(\"Authored LAMMPS input\")\n        self._code_panel(\n            \"in.lammps\", input_script, language=\"bash\", style=\"magenta\"\n        )\n\n        return {**state, \"input_script\": input_script}\n\n    def _run_lammps(self, state: LammpsState) -&gt; LammpsState:\n        self._section(\"Running LAMMPS\")\n\n        if self.ngpus &gt;= 0:\n            result = subprocess.run(\n                [\n                    self.mpirun_cmd,\n                    \"-np\",\n                    str(self.mpi_procs),\n                    self.lammps_cmd,\n                    \"-in\",\n                    \"in.lammps\",\n                    \"-k\",\n                    \"on\",\n                    \"g\",\n                    str(self.ngpus),\n                    \"-sf\",\n                    \"kk\",\n                    \"-pk\",\n                    \"kokkos\",\n                    \"neigh\",\n                    \"half\",\n                    \"newton\",\n                    \"on\",\n                ],\n                cwd=self.workspace,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                text=True,\n                check=False,\n            )\n            print(result)\n        else:\n            result = subprocess.run(\n                [\n                    self.mpirun_cmd,\n                    \"-np\",\n                    str(self.mpi_procs),\n                    self.lammps_cmd,\n                    \"-in\",\n                    \"in.lammps\",\n                ],\n                cwd=self.workspace,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                text=True,\n                check=False,\n            )\n\n        status_style = \"green\" if result.returncode == 0 else \"red\"\n        self._panel(\n            \"Run Result\",\n            f\"returncode = {result.returncode}\",\n            style=status_style,\n        )\n\n        if result.returncode != 0:\n            err_view = (\n                result.stderr.strip() + \"\\n\" + result.stdout.strip()\n            ).strip() or \"(no output captured)\"\n            self._panel(\"Run error/output\", err_view[-6000:], style=\"red\")\n\n        hist = list(state.get(\"run_history\", []))\n        hist.append({\n            \"attempt\": state.get(\"fix_attempts\", 0),\n            \"input_script\": state.get(\"input_script\", \"\"),\n            \"returncode\": result.returncode,\n            \"stdout\": result.stdout,\n            \"stderr\": result.stderr,\n        })\n\n        return {\n            **state,\n            \"run_returncode\": result.returncode,\n            \"run_stdout\": result.stdout,\n            \"run_stderr\": result.stderr,\n            \"run_history\": hist,\n        }\n\n    def _route_run(self, state: LammpsState) -&gt; str:\n        rc = state.get(\"run_returncode\", 0)\n        attempts = state.get(\"fix_attempts\", 0)\n        if rc == 0:\n            self._section(\"LAMMPS run successful! Exiting...\")\n            return \"done_success\"\n        if attempts &lt; self.max_fix_attempts:\n            self._section(\n                \"LAMMPS run Failed. Attempting to rewrite input file...\"\n            )\n            return \"need_fix\"\n        self._section(\n            \"LAMMPS run Failed and maximum fix attempts reached. Exiting..\"\n        )\n        return \"done_failed\"\n\n    def _fix(self, state: LammpsState) -&gt; LammpsState:\n        pair_info = state[\"chosen_potential\"].pair_info()\n\n        hist = state.get(\"run_history\", [])\n        if not hist:\n            hist = [\n                {\n                    \"attempt\": state.get(\"fix_attempts\", 0),\n                    \"input_script\": state.get(\"input_script\", \"\"),\n                    \"returncode\": state.get(\"run_returncode\"),\n                    \"stdout\": state.get(\"run_stdout\", \"\"),\n                    \"stderr\": state.get(\"run_stderr\", \"\"),\n                }\n            ]\n\n        parts = []\n        for h in hist:\n            parts.append(\n                \"=== Attempt {attempt} | returncode={returncode} ===\\n\"\n                \"--- input_script ---\\n{input_script}\\n\"\n                \"--- stdout ---\\n{stdout}\\n\"\n                \"--- stderr ---\\n{stderr}\\n\".format(**h)\n            )\n        err_blob = \"\\n\".join(parts)\n\n        data_content = \"\"\n        if self.data_file:\n            data_content = self._read_and_trim_data_file(self.data_file)\n\n        fixed_json = self.fix_chain.invoke({\n            \"simulation_task\": state[\"simulation_task\"],\n            \"err_message\": err_blob,\n            \"pair_info\": pair_info,\n            \"template\": state[\"template\"],\n            \"data_file\": self.data_file,\n            \"data_content\": data_content,\n        })\n        script_dict = self._safe_json_loads(fixed_json)\n\n        new_input = script_dict[\"input_script\"]\n        old_input = state[\"input_script\"]\n        self._diff_panel(old_input, new_input)\n\n        with open(os.path.join(self.workspace, \"in.lammps\"), \"w\") as f:\n            f.write(new_input)\n\n        return {\n            **state,\n            \"input_script\": new_input,\n            \"fix_attempts\": state.get(\"fix_attempts\", 0) + 1,\n        }\n\n    def _summarize(self, state: LammpsState) -&gt; LammpsState:\n        self._section(\n            \"Now handing things off to execution agent for summarization/visualization\"\n        )\n\n        executor = ExecutionAgent(llm=self.llm)\n\n        exe_plan = f\"\"\"\n        You are part of a larger scientific workflow whose purpose is to accomplish this task: {state[\"simulation_task\"]}\n        A LAMMPS simulation has been done and the output is located in the file 'log.lammps'.\n        Summarize the contents of this file in a markdown document. Include a plot, if relevent.\n        \"\"\"\n\n        exe_results = executor.invoke({\n            \"messages\": [HumanMessage(content=exe_plan)],\n            \"workspace\": self.workspace,\n        })\n\n        for x in exe_results[\"messages\"]:\n            print(x.content)\n\n        return state\n\n    def _post_run(self, state: LammpsState) -&gt; LammpsState:\n        return state\n\n    def _build_graph(self):\n        self.add_node(self._entry_router)\n        self.add_node(self._find_potentials)\n        self.add_node(self._summarize_one)\n        self.add_node(self._build_summaries)\n        self.add_node(self._choose)\n        self.add_node(self._create_user_potential_wrapper)\n        self.add_node(self._author)\n        self.add_node(self._run_lammps)\n        self.add_node(self._fix)\n        self.add_node(self._post_run)\n        self.add_node(self._summarize)\n\n        self.graph.set_entry_point(\"_entry_router\")\n\n        self.graph.add_conditional_edges(\n            \"_entry_router\",\n            lambda state: \"user_potential\"\n            if self.use_user_potential\n            else (\n                \"user_choice\"\n                if state.get(\"chosen_potential\")\n                else \"agent_choice\"\n            ),\n            {\n                \"user_potential\": \"_create_user_potential_wrapper\",\n                \"user_choice\": \"_author\",\n                \"agent_choice\": \"_find_potentials\",\n            },\n        )\n\n        self.graph.add_conditional_edges(\n            \"_find_potentials\",\n            self._should_summarize,\n            {\n                \"summarize_one\": \"_summarize_one\",\n                \"summarize_done\": \"_build_summaries\",\n                \"done_no_matches\": END,\n            },\n        )\n\n        self.graph.add_conditional_edges(\n            \"_summarize_one\",\n            self._should_summarize,\n            {\n                \"summarize_one\": \"_summarize_one\",\n                \"summarize_done\": \"_build_summaries\",\n            },\n        )\n\n        self.graph.add_edge(\"_build_summaries\", \"_choose\")\n\n        self.graph.add_conditional_edges(\n            \"_choose\",\n            self._route_after_summarization,\n            {\n                \"continue_author\": \"_author\",\n                \"Exit\": END,\n            },\n        )\n\n        self.graph.add_edge(\"_create_user_potential_wrapper\", \"_author\")\n        self.graph.add_edge(\"_author\", \"_run_lammps\")\n\n        self.graph.add_conditional_edges(\n            \"_run_lammps\",\n            self._route_run,\n            {\n                \"need_fix\": \"_fix\",\n                \"done_success\": \"_post_run\",\n                \"done_failed\": END,\n            },\n        )\n\n        self.graph.add_edge(\"_fix\", \"_run_lammps\")\n\n        self.graph.add_conditional_edges(\n            \"_post_run\",\n            lambda _: \"summarize\" if self.summarize_results else \"skip\",\n            {\n                \"summarize\": \"_summarize\",\n                \"skip\": END,\n            },\n        )\n\n        self.graph.add_edge(\"_summarize\", END)\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.mp_agent","title":"<code>mp_agent</code>","text":""},{"location":"api_reference/agents/#ursa.agents.mp_agent.MaterialsProjectAgent","title":"<code>MaterialsProjectAgent</code>","text":"<p>               Bases: <code>BaseAgent</code></p> Source code in <code>src/ursa/agents/mp_agent.py</code> <pre><code>class MaterialsProjectAgent(BaseAgent):\n    def __init__(\n        self,\n        llm: BaseChatModel,\n        summarize: bool = True,\n        max_results: int = 3,\n        database_path: str = \"mp_database\",\n        summaries_path: str = \"mp_summaries\",\n        **kwargs,\n    ):\n        super().__init__(llm, **kwargs)\n        self.summarize = summarize\n        self.max_results = max_results\n        self.database_path = self.workspace.joinpath(database_path)\n        self.summaries_path = self.workspace.joinpath(summaries_path)\n\n        self.database_path.mkdir(parents=True, exist_ok=True)\n        self.summaries_path.mkdir(parents=True, exist_ok=True)\n\n    def _fetch_node(self, state: dict) -&gt; dict:\n        f = state[\"query\"]\n        els = f[\"elements\"]  # e.g. [\"Ga\",\"In\"]\n        bg = (f[\"band_gap_min\"], f[\"band_gap_max\"])\n        e_above_hull = (0, 0)  # only on-hull (stable)\n        mats = []\n        with MPRester() as mpr:\n            # get ALL matching materials\u2026\n            all_results = mpr.materials.summary.search(\n                elements=els,\n                band_gap=bg,\n                energy_above_hull=e_above_hull,\n                is_stable=True,  # equivalent filter\n            )\n            # \u2026then take only the first `max_results`\n            for doc in all_results[: self.max_results]:\n                mid = doc.material_id\n                data = doc.dict()\n                # cache to disk\n                path = os.path.join(self.database_path, f\"{mid}.json\")\n                if not os.path.exists(path):\n                    with open(path, \"w\") as f:\n                        json.dump(data, f, indent=2)\n                mats.append({\"material_id\": mid, \"metadata\": data})\n\n        return {**state, \"materials\": mats}\n\n    def _summarize_node(self, state: dict) -&gt; dict:\n        \"\"\"Summarize each material via LLM over its metadata.\"\"\"\n        # prompt template\n        prompt = ChatPromptTemplate.from_template(\"\"\"\nYou are a materials-science assistant. Given the following metadata about a material, produce a concise summary focusing on its key properties:\n\n{metadata}\n        \"\"\")\n        chain = prompt | self.llm | StrOutputParser()\n\n        summaries = [None] * len(state[\"materials\"])\n\n        def process(i, mat):\n            mid = mat[\"material_id\"]\n            meta = mat[\"metadata\"]\n            # flatten metadata to text\n            text = \"\\n\".join(f\"{k}: {v}\" for k, v in meta.items())\n            # build or load summary\n            summary_file = os.path.join(\n                self.summaries_path, f\"{mid}_summary.txt\"\n            )\n            if os.path.exists(summary_file):\n                with open(summary_file) as f:\n                    return i, f.read()\n            # optional: vectorize &amp; retrieve, but here we just summarize full text\n            result = chain.invoke({\"metadata\": text})\n            with open(summary_file, \"w\") as f:\n                f.write(result)\n            return i, result\n\n        with ThreadPoolExecutor(\n            max_workers=min(8, len(state[\"materials\"]))\n        ) as exe:\n            futures = [\n                exe.submit(process, i, m)\n                for i, m in enumerate(state[\"materials\"])\n            ]\n            for future in tqdm(futures, desc=\"Summarizing materials\"):\n                i, summ = future.result()\n                summaries[i] = summ\n\n        return {**state, \"summaries\": summaries}\n\n    def _aggregate_node(self, state: dict) -&gt; dict:\n        \"\"\"Combine all summaries into a single, coherent answer.\"\"\"\n        combined = \"\\n\\n----\\n\\n\".join(\n            f\"[{i + 1}] {m['material_id']}\\n\\n{summary}\"\n            for i, (m, summary) in enumerate(\n                zip(state[\"materials\"], state[\"summaries\"])\n            )\n        )\n\n        prompt = ChatPromptTemplate.from_template(\"\"\"\n        You are a materials informatics assistant. Below are brief summaries of several materials:\n\n        {summaries}\n\n        Answer the user\u2019s question in context:\n\n        {context}\n                \"\"\")\n        chain = prompt | self.llm | StrOutputParser()\n        final = chain.invoke({\n            \"summaries\": combined,\n            \"context\": state[\"context\"],\n        })\n        return {**state, \"final_summary\": final}\n\n    def _build_graph(self):\n        self.add_node(self._fetch_node)\n        if self.summarize:\n            self.add_node(self._summarize_node)\n            self.add_node(self._aggregate_node)\n\n            self.graph.set_entry_point(\"_fetch_node\")\n            self.graph.add_edge(\"_fetch_node\", \"_summarize_node\")\n            self.graph.add_edge(\"_summarize_node\", \"_aggregate_node\")\n            self.graph.set_finish_point(\"_aggregate_node\")\n        else:\n            self.graph.set_entry_point(\"_fetch_node\")\n            self.graph.set_finish_point(\"_fetch_node\")\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.optimization_agent","title":"<code>optimization_agent</code>","text":""},{"location":"api_reference/agents/#ursa.agents.optimization_agent.run_cmd","title":"<code>run_cmd(query, state)</code>","text":"<p>Run a commandline command from using the subprocess package in python</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>commandline command to be run as a string given to the subprocess.run command.</p> required Source code in <code>src/ursa/agents/optimization_agent.py</code> <pre><code>@tool\ndef run_cmd(query: str, state: Annotated[dict, InjectedState]) -&gt; str:\n    \"\"\"\n    Run a commandline command from using the subprocess package in python\n\n    Args:\n        query: commandline command to be run as a string given to the subprocess.run command.\n    \"\"\"\n    workspace_dir = state[\"workspace\"]\n    print(\"RUNNING: \", query)\n    try:\n        process = subprocess.Popen(\n            query.split(\" \"),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n            cwd=workspace_dir,\n        )\n\n        stdout, stderr = process.communicate(timeout=60000)\n    except KeyboardInterrupt:\n        print(\"Keyboard Interrupt of command: \", query)\n        stdout, stderr = \"\", \"KeyboardInterrupt:\"\n\n    print(\"STDOUT: \", stdout)\n    print(\"STDERR: \", stderr)\n\n    return f\"STDOUT: {stdout} and STDERR: {stderr}\"\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.optimization_agent.write_code","title":"<code>write_code(code, filename, state)</code>","text":"<p>Writes python or Julia code to a file in the given workspace as requested.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>The code to write</p> required <code>filename</code> <code>str</code> <p>the filename with an appropriate extension for programming language (.py for python, .jl for Julia, etc.)</p> required <p>Returns:</p> Type Description <code>str</code> <p>Execution results</p> Source code in <code>src/ursa/agents/optimization_agent.py</code> <pre><code>@tool\ndef write_code(\n    code: str, filename: str, state: Annotated[dict, InjectedState]\n) -&gt; str:\n    \"\"\"\n    Writes python or Julia code to a file in the given workspace as requested.\n\n    Args:\n        code: The code to write\n        filename: the filename with an appropriate extension for programming language (.py for python, .jl for Julia, etc.)\n\n    Returns:\n        Execution results\n    \"\"\"\n    workspace_dir = state[\"workspace\"]\n    print(\"Writing filename \", filename)\n    try:\n        # Extract code if wrapped in markdown code blocks\n        if \"```\" in code:\n            code_parts = code.split(\"```\")\n            if len(code_parts) &gt;= 3:\n                # Extract the actual code\n                if \"\\n\" in code_parts[1]:\n                    code = \"\\n\".join(code_parts[1].strip().split(\"\\n\")[1:])\n                else:\n                    code = code_parts[2].strip()\n\n        # Write code to a file\n        code_file = os.path.join(workspace_dir, filename)\n\n        with open(code_file, \"w\") as f:\n            f.write(code)\n        print(f\"Written code to file: {code_file}\")\n\n        return f\"File {filename} written successfully.\"\n\n    except Exception as e:\n        print(f\"Error generating code: {str(e)}\")\n        # Return minimal code that prints the error\n        return f\"Failed to write {filename} successfully.\"\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.planning_agent","title":"<code>planning_agent</code>","text":""},{"location":"api_reference/agents/#ursa.agents.planning_agent.PlanningAgent","title":"<code>PlanningAgent</code>","text":"<p>               Bases: <code>BaseAgent[PlanningState]</code></p> Source code in <code>src/ursa/agents/planning_agent.py</code> <pre><code>class PlanningAgent(BaseAgent[PlanningState]):\n    agent_state = PlanningState\n\n    def __init__(\n        self,\n        llm: BaseChatModel,\n        max_reflection_steps: int = 1,\n        **kwargs,\n    ):\n        super().__init__(llm, **kwargs)\n        self.planner_prompt = planner_prompt\n        self.reflection_prompt = reflection_prompt\n        self.max_reflection_steps = max_reflection_steps\n\n    def format_result(self, state: PlanningState) -&gt; str:\n        return str(state[\"plan\"])\n\n    def generation_node(self, state: PlanningState) -&gt; PlanningState:\n        \"\"\"\n        Plan generation with structured output. Produces a JSON string in messages\n        and a parsed list of steps in state[\"plan_steps\"].\n        \"\"\"\n\n        print(\"PlanningAgent: generating . . .\")\n        messages = cast(list, state.get(\"messages\"))\n        if isinstance(messages[0], SystemMessage):\n            messages[0] = SystemMessage(content=self.planner_prompt)\n        else:\n            messages = [SystemMessage(content=self.planner_prompt)] + messages\n\n        structured_llm = self.llm.with_structured_output(Plan)\n        plan = cast(Plan, structured_llm.invoke(messages))\n\n        return {\n            \"plan\": plan,\n            \"messages\": [AIMessage(content=plan.model_dump_json())],\n            \"reflection_steps\": state.get(\n                \"reflection_steps\", self.max_reflection_steps\n            ),\n        }\n\n    def reflection_node(self, state: PlanningState) -&gt; PlanningState:\n        print(\"PlanningAgent: reflecting . . .\")\n\n        cls_map = {\"ai\": HumanMessage, \"human\": AIMessage}\n        translated = [state[\"messages\"][0]] + [\n            cls_map[msg.type](content=msg.content)\n            for msg in state[\"messages\"][1:]\n        ]\n        translated = [SystemMessage(content=reflection_prompt)] + translated\n        res = StrOutputParser().invoke(\n            self.llm.invoke(\n                translated,\n                self.build_config(tags=[\"planner\", \"reflect\"]),\n            )\n        )\n        return {\n            \"plan\": state[\"plan\"],\n            \"messages\": [HumanMessage(content=res)],\n            \"reflection_steps\": state[\"reflection_steps\"] - 1,\n        }\n\n    def _build_graph(self):\n        self.add_node(self.generation_node, \"generate\")\n        self.add_node(self.reflection_node, \"reflect\")\n        self.graph.set_entry_point(\"generate\")\n        self.graph.add_conditional_edges(\n            \"generate\",\n            self._wrap_cond(\n                _should_reflect, \"should_reflect\", \"planning_agent\"\n            ),\n            {\"reflect\": \"reflect\", \"END\": END},\n        )\n        self.graph.add_conditional_edges(\n            \"reflect\",\n            self._wrap_cond(\n                _should_regenerate, \"should_regenerate\", \"planning_agent\"\n            ),\n            {\"generate\": \"generate\", \"END\": END},\n        )\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.planning_agent.PlanningAgent.generation_node","title":"<code>generation_node(state)</code>","text":"<p>Plan generation with structured output. Produces a JSON string in messages and a parsed list of steps in state[\"plan_steps\"].</p> Source code in <code>src/ursa/agents/planning_agent.py</code> <pre><code>def generation_node(self, state: PlanningState) -&gt; PlanningState:\n    \"\"\"\n    Plan generation with structured output. Produces a JSON string in messages\n    and a parsed list of steps in state[\"plan_steps\"].\n    \"\"\"\n\n    print(\"PlanningAgent: generating . . .\")\n    messages = cast(list, state.get(\"messages\"))\n    if isinstance(messages[0], SystemMessage):\n        messages[0] = SystemMessage(content=self.planner_prompt)\n    else:\n        messages = [SystemMessage(content=self.planner_prompt)] + messages\n\n    structured_llm = self.llm.with_structured_output(Plan)\n    plan = cast(Plan, structured_llm.invoke(messages))\n\n    return {\n        \"plan\": plan,\n        \"messages\": [AIMessage(content=plan.model_dump_json())],\n        \"reflection_steps\": state.get(\n            \"reflection_steps\", self.max_reflection_steps\n        ),\n    }\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.planning_agent.PlanningState","title":"<code>PlanningState</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>State dictionary for planning agent</p> Source code in <code>src/ursa/agents/planning_agent.py</code> <pre><code>class PlanningState(TypedDict, total=False):\n    \"\"\"State dictionary for planning agent\"\"\"\n\n    plan: Plan\n    messages: Annotated[list, add_messages]\n    reflection_steps: int\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.websearch_agent","title":"<code>websearch_agent</code>","text":""},{"location":"api_reference/agents/#ursa.agents.websearch_agent.WebSearchAgentLegacy","title":"<code>WebSearchAgentLegacy</code>","text":"<p>               Bases: <code>BaseAgent</code></p> Source code in <code>src/ursa/agents/websearch_agent.py</code> <pre><code>class WebSearchAgentLegacy(BaseAgent):\n    def __init__(self, llm: BaseChatModel, **kwargs):\n        super().__init__(llm, **kwargs)\n        self.websearch_prompt = websearch_prompt\n        self.reflection_prompt = reflection_prompt\n        self.tools = [search_tool, process_content]  # + cb_tools\n        self.has_internet = self._check_for_internet(\n            kwargs.get(\"url\", \"http://www.lanl.gov\")\n        )\n\n    def _review_node(self, state: WebSearchState) -&gt; WebSearchState:\n        if not self.has_internet:\n            return {\n                \"messages\": [\n                    HumanMessage(\n                        content=\"No internet for WebSearch Agent so no research to review.\"\n                    )\n                ],\n                \"urls_visited\": [],\n            }\n\n        translated = [SystemMessage(content=reflection_prompt)] + state[\n            \"messages\"\n        ]\n        res = StrOutputParser().invoke(\n            self.llm.invoke(\n                translated, {\"configurable\": {\"thread_id\": self.thread_id}}\n            )\n        )\n        return {\"messages\": [HumanMessage(content=res)]}\n\n    def _response_node(self, state: WebSearchState) -&gt; WebSearchState:\n        if not self.has_internet:\n            return {\n                \"messages\": [\n                    HumanMessage(\n                        content=\"No internet for WebSearch Agent. No research carried out.\"\n                    )\n                ],\n                \"urls_visited\": [],\n            }\n\n        messages = state[\"messages\"] + [SystemMessage(content=summarize_prompt)]\n        response = StrOutputParser().invoke(\n            self.llm.invoke(\n                messages, {\"configurable\": {\"thread_id\": self.thread_id}}\n            )\n        )\n\n        urls_visited = []\n        for message in messages:\n            if message.model_dump().get(\"tool_calls\", []):\n                if \"url\" in message.tool_calls[0][\"args\"]:\n                    urls_visited.append(message.tool_calls[0][\"args\"][\"url\"])\n        return {\"messages\": [response], \"urls_visited\": urls_visited}\n\n    def _check_for_internet(self, url, timeout=2):\n        \"\"\"\n        Checks for internet connectivity by attempting an HTTP GET request.\n        \"\"\"\n        try:\n            requests.get(url, timeout=timeout)\n            return True\n        except (requests.ConnectionError, requests.Timeout):\n            return False\n\n    def _state_store_node(self, state: WebSearchState) -&gt; WebSearchState:\n        state[\"thread_id\"] = self.thread_id\n        return state\n        # return dict(**state, thread_id=self.thread_id)\n\n    def _create_react(self, state: WebSearchState) -&gt; WebSearchState:\n        react_agent = create_agent(\n            self.llm,\n            self.tools,\n            state_schema=WebSearchState,\n            system_prompt=self.websearch_prompt,\n        )\n        return react_agent.invoke(state)\n\n    def _build_graph(self):\n        self.add_node(self._state_store_node)\n        self.add_node(self._create_react)\n        self.add_node(self._review_node)\n        self.add_node(self._response_node)\n\n        self.graph.set_entry_point(\"_state_store_node\")\n        self.graph.add_edge(\"_state_store_node\", \"_create_react\")\n        self.graph.add_edge(\"_create_react\", \"_review_node\")\n        self.graph.set_finish_point(\"_response_node\")\n\n        self.graph.add_conditional_edges(\n            \"_review_node\",\n            should_continue,\n            {\n                \"_create_react\": \"_create_react\",\n                \"_response_node\": \"_response_node\",\n            },\n        )\n</code></pre>"},{"location":"api_reference/agents/#ursa.agents.websearch_agent.process_content","title":"<code>process_content(url, context, state)</code>","text":"<p>Processes content from a given webpage.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>string with the url to obtain text content from.</p> required <code>context</code> <code>str</code> <p>string summary of the information the agent wants from the url for summarizing salient information.</p> required Source code in <code>src/ursa/agents/websearch_agent.py</code> <pre><code>def process_content(\n    url: str, context: str, state: Annotated[dict, InjectedState]\n) -&gt; str:\n    \"\"\"\n    Processes content from a given webpage.\n\n    Args:\n        url: string with the url to obtain text content from.\n        context: string summary of the information the agent wants from the url for summarizing salient information.\n    \"\"\"\n    print(\"Parsing information from \", url)\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, \"html.parser\")\n\n    content_prompt = f\"\"\"\n    Here is the full content:\n    {soup.get_text()}\n\n    Carefully summarize the content in full detail, given the following context:\n    {context}\n    \"\"\"\n    summarized_information = StrOutputParser().invoke(\n        state[\"model\"].invoke(\n            content_prompt, {\"configurable\": {\"thread_id\": state[\"thread_id\"]}}\n        )\n    )\n    return summarized_information\n</code></pre>"},{"location":"api_reference/prompt_library/","title":"prompt_library","text":""},{"location":"api_reference/tools/","title":"tools","text":""},{"location":"api_reference/tools/#ursa.tools.feasibility_checker","title":"<code>feasibility_checker</code>","text":""},{"location":"api_reference/tools/#ursa.tools.feasibility_checker.heuristic_feasibility_check","title":"<code>heuristic_feasibility_check(constraints, variable_name, variable_type, variable_bounds, samples=10000)</code>","text":"<p>A tool for checking feasibility of the constraints.</p> <p>Parameters:</p> Name Type Description Default <code>constraints</code> <code>Annotated[list[str], \"List of strings like 'x0+x1&lt;=5'\"]</code> <p>list of strings like 'x0 + x1 &lt;= 5', etc.</p> required <code>variable_name</code> <code>Annotated[list[str], \"List of strings like 'x0', 'x1', etc.\"]</code> <p>list of strings containing variable names used in constraint expressions.</p> required <code>variable_type</code> <code>Annotated[list[str], \"List of strings like 'real', 'integer', 'boolean', etc.\"]</code> <p>list of strings like 'real', 'integer', 'boolean', etc.</p> required <code>variable_bounds</code> <code>Annotated[list[list[float]], \"List of (lower bound, upper bound) tuples for x0, x1, ...'\"]</code> <p>list of (lower, upper) tuples for x0, x1, etc.</p> required <code>samples</code> <code>Annotated[int, 'Number of random sample. Default 10000']</code> <p>number of random samples, default value 10000</p> <code>10000</code> <p>Returns:</p> Type Description <code>tuple[str]</code> <p>A string indicating whether a feasible solution was found.</p> Source code in <code>src/ursa/tools/feasibility_checker.py</code> <pre><code>@tool(parse_docstring=True)\ndef heuristic_feasibility_check(\n    constraints: Annotated[list[str], \"List of strings like 'x0+x1&lt;=5'\"],\n    variable_name: Annotated[\n        list[str], \"List of strings like 'x0', 'x1', etc.\"\n    ],\n    variable_type: Annotated[\n        list[str], \"List of strings like 'real', 'integer', 'boolean', etc.\"\n    ],\n    variable_bounds: Annotated[\n        list[list[float]],\n        \"List of (lower bound, upper bound) tuples for x0, x1, ...'\",\n    ],\n    samples: Annotated[int, \"Number of random sample. Default 10000\"] = 10000,\n) -&gt; tuple[str]:\n    \"\"\"\n    A tool for checking feasibility of the constraints.\n\n    Args:\n        constraints: list of strings like 'x0 + x1 &lt;= 5', etc.\n        variable_name: list of strings containing variable names used in constraint expressions.\n        variable_type: list of strings like 'real', 'integer', 'boolean', etc.\n        variable_bounds: list of (lower, upper) tuples for x0, x1, etc.\n        samples: number of random samples, default value 10000\n\n    Returns:\n        A string indicating whether a feasible solution was found.\n    \"\"\"\n\n    symbols = sp.symbols(variable_name)\n\n    # Build a dict mapping each name to its Symbol, for parsing\n    locals_map = {name: sym for name, sym in zip(variable_name, symbols)}\n\n    # Parse constraints into Sympy Boolean expressions\n    parsed_constraints = []\n    try:\n        for expr in constraints:\n            parsed = parse_expr(\n                expr,\n                local_dict=locals_map,\n                transformations=standard_transformations,\n                evaluate=False,\n            )\n            parsed_constraints.append(parsed)\n    except Exception as e:\n        return f\"Error parsing constraints: {e}\"\n\n    # Sampling loop\n    n = len(parsed_constraints)\n    funcs = [\n        sp.lambdify(symbols, c, modules=[\"math\", \"numpy\"])\n        for c in parsed_constraints\n    ]\n    constraint_satisfied = np.zeros(n, dtype=int)\n    for _ in range(samples):\n        point = {}\n        for i, sym in enumerate(symbols):\n            typ = variable_type[i].lower()\n            low, high = variable_bounds[i]\n            if typ == \"integer\":\n                value = random.randint(int(low), int(high))\n            elif typ in (\"real\", \"continuous\"):\n                value = random.uniform(low, high)\n            elif typ in (\"boolean\", \"logical\"):\n                value = random.choice([False, True])\n            else:\n                raise ValueError(\n                    f\"Unknown type {variable_type[i]} for variable {variable_name[i]}\"\n                )\n            point[sym] = value\n\n        # Evaluate all constraints at this point\n        try:\n            vals = [point[s] for s in symbols]\n            cons_satisfaction = [\n                bool(np.asarray(f(*vals)).all()) for f in funcs\n            ]\n            if all(cons_satisfaction):\n                # Found a feasible point\n                readable = {str(k): round(v, 3) for k, v in point.items()}\n                return f\"Feasible solution found: {readable}\"\n            else:\n                constraint_satisfied += np.array(cons_satisfaction)\n        except Exception as e:\n            return f\"Error evaluating constraint at point {point}: {e}\"\n\n    rates = constraint_satisfied / samples  # fraction satisfied per constraint\n    order = np.argsort(rates)  # lowest (most violated) first\n\n    lines = []\n    for rank, idx in enumerate(order, start=1):\n        expr_text = constraints[\n            idx\n        ]  # use the original string; easier to read than str(sympy_expr)\n        sat = constraint_satisfied[idx]\n        lines.append(\n            f\"[C{idx + 1}] {expr_text} \u2014 satisfied {sat:,}/{samples:,} ({sat / samples:.1%}), \"\n            f\"violated {1 - sat / samples:.1%}\"\n        )\n\n    return (\n        f\"No feasible solution found after {samples:,} samples. Most violated constraints (low\u2192high satisfaction):\\n \"\n        + \"\\n  \".join(lines)\n    )\n</code></pre>"},{"location":"api_reference/tools/#ursa.tools.feasibility_tools","title":"<code>feasibility_tools</code>","text":"<p>Unified feasibility checker with heuristic pre-check and exact auto-routing.</p> <p>Backends (imported lazily and used only if available): - PySMT (cvc5/msat/yices/z3) for SMT-style logic, disjunctions, and nonlinear constructs. - OR-Tools CP-SAT for strictly linear integer/boolean instances with integer coefficients. - OR-Tools CBC (pywraplp) for linear MILP/LP (mixed real + integer, or pure LP). - SciPy HiGHS (linprog) for pure continuous LP feasibility.</p> Install any subset you need <p>pip install pysmt &amp;&amp; pysmt-install --cvc5        # or --z3/--msat/--yices pip install ortools pip install scipy pip install numpy</p> <p>This file exposes a single LangChain tool: <code>feasibility_check_auto</code>.</p>"},{"location":"api_reference/tools/#ursa.tools.feasibility_tools.feasibility_check_auto","title":"<code>feasibility_check_auto(constraints, variable_name, variable_type, variable_bounds, prefer_smt_solver='cvc5', heuristic_enabled=True, heuristic_first=True, heuristic_samples=2000, heuristic_seed=None, heuristic_unbounded_radius_real=1000.0, heuristic_unbounded_radius_int=10 ** 6, numeric_tolerance=1e-08)</code>","text":"<p>Unified feasibility checker with heuristic pre-check and exact auto-routing.</p> <p>Performs an optional randomized feasibility search. If no witness is found (or the heuristic is disabled), the function auto-routes to an exact backend based on the detected problem structure (PySMT for SMT/logic/nonlinear, OR-Tools CP-SAT for linear integer/boolean, OR-Tools CBC for MILP/LP, or SciPy HiGHS for pure LP).</p> <p>Parameters:</p> Name Type Description Default <code>constraints</code> <code>Annotated[list[str], \"Constraint strings like 'x0 + 2*x1 &lt;= 5' or '(x0&lt;=3) | (x1&gt;=2)'\"]</code> <p>Constraint strings such as \"x0 + 2*x1 &lt;= 5\" or \"(x0&lt;=3) | (x1&gt;=2)\".</p> required <code>variable_name</code> <code>Annotated[list[str], ['x0', 'x1', ...]]</code> <p>Variable names, e.g., [\"x0\", \"x1\"].</p> required <code>variable_type</code> <code>Annotated[list[str], ['real' | 'integer' | 'boolean', ...]]</code> <p>Variable types aligned with <code>variable_name</code>. Each must be one of \"real\", \"integer\", or \"boolean\".</p> required <code>variable_bounds</code> <code>Annotated[list[list[Optional[float]]], '[(low, high), ...] (use None for unbounded)']</code> <p>Per-variable [low, high] bounds aligned with <code>variable_name</code>. Use None to denote an unbounded side.</p> required <code>prefer_smt_solver</code> <code>Annotated[str, \"SMT backend if needed: 'cvc5'|'msat'|'yices'|'z3'\"]</code> <p>SMT backend name used by PySMT (\"cvc5\", \"msat\", \"yices\", or \"z3\").</p> <code>'cvc5'</code> <code>heuristic_enabled</code> <code>Annotated[bool, 'Run a fast randomized search first?']</code> <p>Whether to run the heuristic sampler.</p> <code>True</code> <code>heuristic_first</code> <code>Annotated[bool, 'Try heuristic before exact routing']</code> <p>If True, run the heuristic before exact routing; if False, run it after.</p> <code>True</code> <code>heuristic_samples</code> <code>Annotated[int, 'Samples for heuristic search']</code> <p>Number of heuristic samples.</p> <code>2000</code> <code>heuristic_seed</code> <code>Annotated[Optional[int], 'Seed for reproducibility']</code> <p>Random seed for reproducibility.</p> <code>None</code> <code>heuristic_unbounded_radius_real</code> <code>Annotated[float, 'Sampling range for unbounded real vars']</code> <p>Sampling radius for unbounded real variables.</p> <code>1000.0</code> <code>heuristic_unbounded_radius_int</code> <code>Annotated[int, 'Sampling range for unbounded integer vars']</code> <p>Sampling radius for unbounded integer variables.</p> <code>10 ** 6</code> <code>numeric_tolerance</code> <code>Annotated[float, 'Tolerance for relational checks (Eq/Lt/Le/etc.)']</code> <p>Tolerance used in relational checks (e.g., Eq, Lt, Le).</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>str</code> <p>A message indicating the chosen backend and the feasibility result. On success,</p> <code>str</code> <p>includes an example model (assignment). On infeasibility, includes a short</p> <code>str</code> <p>diagnostic or solver status.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If constraints cannot be parsed or an unsupported variable type is provided.</p> Source code in <code>src/ursa/tools/feasibility_tools.py</code> <pre><code>@tool(parse_docstring=True)\ndef feasibility_check_auto(\n    constraints: Annotated[\n        list[str],\n        \"Constraint strings like 'x0 + 2*x1 &lt;= 5' or '(x0&lt;=3) | (x1&gt;=2)'\",\n    ],\n    variable_name: Annotated[list[str], \"['x0','x1',...]\"],\n    variable_type: Annotated[list[str], \"['real'|'integer'|'boolean', ...]\"],\n    variable_bounds: Annotated[\n        list[list[Optional[float]]],\n        \"[(low, high), ...] (use None for unbounded)\",\n    ],\n    prefer_smt_solver: Annotated[\n        str, \"SMT backend if needed: 'cvc5'|'msat'|'yices'|'z3'\"\n    ] = \"cvc5\",\n    heuristic_enabled: Annotated[\n        bool, \"Run a fast randomized search first?\"\n    ] = True,\n    heuristic_first: Annotated[\n        bool, \"Try heuristic before exact routing\"\n    ] = True,\n    heuristic_samples: Annotated[int, \"Samples for heuristic search\"] = 2000,\n    heuristic_seed: Annotated[Optional[int], \"Seed for reproducibility\"] = None,\n    heuristic_unbounded_radius_real: Annotated[\n        float, \"Sampling range for unbounded real vars\"\n    ] = 1e3,\n    heuristic_unbounded_radius_int: Annotated[\n        int, \"Sampling range for unbounded integer vars\"\n    ] = 10**6,\n    numeric_tolerance: Annotated[\n        float, \"Tolerance for relational checks (Eq/Lt/Le/etc.)\"\n    ] = 1e-8,\n) -&gt; str:\n    \"\"\"Unified feasibility checker with heuristic pre-check and exact auto-routing.\n\n    Performs an optional randomized feasibility search. If no witness is found (or the\n    heuristic is disabled), the function auto-routes to an exact backend based on the\n    detected problem structure (PySMT for SMT/logic/nonlinear, OR-Tools CP-SAT for\n    linear integer/boolean, OR-Tools CBC for MILP/LP, or SciPy HiGHS for pure LP).\n\n    Args:\n        constraints: Constraint strings such as \"x0 + 2*x1 &lt;= 5\" or \"(x0&lt;=3) | (x1&gt;=2)\".\n        variable_name: Variable names, e.g., [\"x0\", \"x1\"].\n        variable_type: Variable types aligned with `variable_name`. Each must be one of\n            \"real\", \"integer\", or \"boolean\".\n        variable_bounds: Per-variable [low, high] bounds aligned with `variable_name`.\n            Use None to denote an unbounded side.\n        prefer_smt_solver: SMT backend name used by PySMT (\"cvc5\", \"msat\", \"yices\", or \"z3\").\n        heuristic_enabled: Whether to run the heuristic sampler.\n        heuristic_first: If True, run the heuristic before exact routing; if False, run it after.\n        heuristic_samples: Number of heuristic samples.\n        heuristic_seed: Random seed for reproducibility.\n        heuristic_unbounded_radius_real: Sampling radius for unbounded real variables.\n        heuristic_unbounded_radius_int: Sampling radius for unbounded integer variables.\n        numeric_tolerance: Tolerance used in relational checks (e.g., Eq, Lt, Le).\n\n    Returns:\n        A message indicating the chosen backend and the feasibility result. On success,\n        includes an example model (assignment). On infeasibility, includes a short\n        diagnostic or solver status.\n\n    Raises:\n        ValueError: If constraints cannot be parsed or an unsupported variable type is provided.\n    \"\"\"\n    # 1) Parse\n    try:\n        symbols, sympy_cons = _parse_constraints(constraints, variable_name)\n    except Exception as e:\n        return f\"Parse error: {e}\"\n\n    # 2) Heuristic (optional)\n    if heuristic_enabled and heuristic_first:\n        try:\n            h_model = _heuristic_feasible(\n                sympy_cons,\n                symbols,\n                variable_name,\n                variable_type,\n                variable_bounds,\n                samples=heuristic_samples,\n                seed=heuristic_seed,\n                tol=numeric_tolerance,\n                unbounded_radius_real=heuristic_unbounded_radius_real,\n                unbounded_radius_int=heuristic_unbounded_radius_int,\n            )\n            if h_model is not None:\n                return f\"[backend=heuristic] Feasible (sampled witness). Example solution: {h_model}\"\n        except Exception:\n            # Ignore heuristic issues and continue to exact route\n            pass\n\n    # 3) Classify &amp; route\n    info = _classify(sympy_cons, symbols, variable_type)\n\n    # SMT needed or nonlinear / non-conj\n    if info[\"requires_smt\"] or not info[\"all_linear\"]:\n        res = _solve_with_pysmt(\n            sympy_cons,\n            symbols,\n            variable_name,\n            variable_type,\n            variable_bounds,\n            solver_name=prefer_smt_solver,\n        )\n        # Optional heuristic after exact if requested\n        if (\n            heuristic_enabled\n            and not heuristic_first\n            and any(\n                kw in res.lower()\n                for kw in (\"unknown\", \"not installed\", \"unsupported\", \"failed\")\n            )\n        ):\n            h_model = _heuristic_feasible(\n                sympy_cons,\n                symbols,\n                variable_name,\n                variable_type,\n                variable_bounds,\n                samples=heuristic_samples,\n                seed=heuristic_seed,\n                tol=numeric_tolerance,\n                unbounded_radius_real=heuristic_unbounded_radius_real,\n                unbounded_radius_int=heuristic_unbounded_radius_int,\n            )\n            if h_model is not None:\n                return f\"[backend=heuristic] Feasible (sampled witness). Example solution: {h_model}\"\n        return res\n\n    # Linear-only path: collect atomic conjuncts\n    conjuncts: list[sp.Expr] = []\n    for c in sympy_cons:\n        atoms, _ = _flatten_conjunction(c)\n        conjuncts.extend(atoms)\n\n    has_int, has_bool, has_real = (\n        info[\"has_int\"],\n        info[\"has_bool\"],\n        info[\"has_real\"],\n    )\n\n    # Pure LP (continuous only)\n    if not has_int and not has_bool and has_real:\n        res = _solve_with_highs_lp(\n            conjuncts, symbols, variable_name, variable_bounds\n        )\n        if \"not installed\" in res.lower():\n            res = _solve_with_cbc_milp(\n                conjuncts,\n                symbols,\n                variable_name,\n                variable_type,\n                variable_bounds,\n            )\n        if (\n            heuristic_enabled\n            and not heuristic_first\n            and any(kw in res.lower() for kw in (\"failed\", \"unknown\"))\n        ):\n            h_model = _heuristic_feasible(\n                sympy_cons,\n                symbols,\n                variable_name,\n                variable_type,\n                variable_bounds,\n                samples=heuristic_samples,\n                seed=heuristic_seed,\n                tol=numeric_tolerance,\n                unbounded_radius_real=heuristic_unbounded_radius_real,\n                unbounded_radius_int=heuristic_unbounded_radius_int,\n            )\n            if h_model is not None:\n                return f\"[backend=heuristic] Feasible (sampled witness). Example solution: {h_model}\"\n        return res\n\n    # All integer/boolean \u2192 CP-SAT first (if integer coefficients), else CBC MILP\n    if (has_int or has_bool) and not has_real:\n        res = _solve_with_cpsat_integer_boolean(\n            conjuncts, symbols, variable_name, variable_type, variable_bounds\n        )\n        if (\n            any(\n                kw in res\n                for kw in (\n                    \"routing to MILP/LP\",\n                    \"handles linear conjunctions only\",\n                )\n            )\n            or \"not installed\" in res.lower()\n        ):\n            res = _solve_with_cbc_milp(\n                conjuncts,\n                symbols,\n                variable_name,\n                variable_type,\n                variable_bounds,\n            )\n        return res\n\n    # Mixed reals + integers \u2192 CBC MILP\n    res = _solve_with_cbc_milp(\n        conjuncts, symbols, variable_name, variable_type, variable_bounds\n    )\n\n    # Optional heuristic after exact (if backend missing/failing)\n    if (\n        heuristic_enabled\n        and not heuristic_first\n        and any(\n            kw in res.lower() for kw in (\"not installed\", \"failed\", \"status:\")\n        )\n    ):\n        h_model = _heuristic_feasible(\n            sympy_cons,\n            symbols,\n            variable_name,\n            variable_type,\n            variable_bounds,\n            samples=heuristic_samples,\n            seed=heuristic_seed,\n            tol=numeric_tolerance,\n            unbounded_radius_real=heuristic_unbounded_radius_real,\n            unbounded_radius_int=heuristic_unbounded_radius_int,\n        )\n        if h_model is not None:\n            return f\"[backend=heuristic] Feasible (sampled witness). Example solution: {h_model}\"\n\n    return res\n</code></pre>"},{"location":"api_reference/tools/#ursa.tools.fm_base_tool","title":"<code>fm_base_tool</code>","text":""},{"location":"api_reference/tools/#ursa.tools.fm_base_tool.TorchModuleTool","title":"<code>TorchModuleTool</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[Input, Output, ModelInput, ModelOutput]</code></p> <p>A helper class for exposing a PyTorch model as an MCP tool for inference. Provides default methods for running the following pipeline:</p> <ol> <li>Preprocess a sequence of <code>Inputs</code> into a <code>ModelInput</code></li> <li>Pass <code>ModelInput</code> through the PyTorch model getting <code>ModelOutput</code></li> <li>Postprocess <code>ModelOutput</code> into a suitable sequence of <code>Outputs</code></li> </ol> <p>Complex models (i.e. multi-GPU) may not be fully supported by this class.</p> Source code in <code>src/ursa/tools/fm_base_tool.py</code> <pre><code>class TorchModuleTool(\n    BaseModel, Generic[Input, Output, ModelInput, ModelOutput]\n):\n    \"\"\"\n    A helper class for exposing a PyTorch model as an MCP tool for inference.\n    Provides default methods for running the following pipeline:\n\n    1. Preprocess a sequence of `Inputs` into a `ModelInput`\n    2. Pass `ModelInput` through the PyTorch model getting `ModelOutput`\n    3. Postprocess `ModelOutput` into a suitable sequence of `Outputs`\n\n    Complex models (i.e. multi-GPU) may not be fully supported by this class.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    fm: torch.nn.Module\n    \"\"\" The underlying PyTorch model used for inference \"\"\"\n\n    name: str = None\n    \"\"\" A short name for the foundation model \"\"\"\n\n    description: str\n    \"\"\" What the foundation model does / how to use it \"\"\"\n\n    args_schema: type[Input]\n    \"\"\" The input schema for the model \"\"\"\n\n    output_schema: type[Output]\n    \"\"\" The output_schema for the model \"\"\"\n\n    batch_size: int = 1\n    \"\"\" Inputs to the model will be batched into set of at most this size \"\"\"\n\n    device: torch.device = Field(default_factory=default_device)\n    \"\"\" The accelerator on which the model is placed \"\"\"\n\n    def preprocess(self, input: Sequence[Input]) -&gt; ModelInput:\n        \"\"\"\n        Convert tool input into the form accepted by the model\n        The input will be of type `list[args_schema]` with a length\n        of `batch_size`\n\n        Defaults to `torch.data.default_collate`\n        \"\"\"\n\n        return default_collate(list(input))\n\n    def _forward(self, model_inputs: ModelInput) -&gt; ModelOutput:\n        \"\"\"Process a batch of observations with the model\"\"\"\n        return self.fm(model_inputs).to(\"cpu\")\n\n    def postprocess(self, model_output: ModelOutput) -&gt; Iterable[Output]:\n        \"\"\"Postprocess the model's raw output into a relevant tool output format\"\"\"\n        yield from model_output\n\n    @classmethod\n    def from_pretrained(\n        cls, pretrained_model_name_or_path: str, **kwargs\n    ) -&gt; \"TorchModuleTool\":\n        \"\"\"Instantiate tool from a pretrained checkpoint either on disk,\n        or automatically downloaded from a repository (i.e. HuggingFace)\n        \"\"\"\n        raise NotImplementedError()\n\n    def model_post_init(self, __context) -&gt; None:\n        # Move the model to the indicated device\n        self.fm = self.fm.to(self.device)\n\n        # Default to the class name\n        if self.name is None:\n            self.name = self.__class__.__name__\n\n    @final\n    def batch(self, inputs: list[Input], **kwargs) -&gt; list[Output]:\n        return list(self.batch_as_completed(inputs, **kwargs))\n\n    @final\n    def batch_as_completed(\n        self,\n        inputs: list[Input],\n        max_concurency: int | None = None,\n    ) -&gt; Iterable[Output]:\n        n = max_concurency or self.batch_size\n        for batch in batched(inputs, n=n):\n            from torch import inference_mode\n\n            with inference_mode():\n                batch = self.preprocess(batch)\n                y = self._forward(batch)\n                yield from self.postprocess(y)\n\n    @final\n    def __call__(self, input: Input):\n        with torch.inference_mode():\n            batch = self.preprocess([input])\n            y = self._forward(batch)\n            return next(iter(self.postprocess(y)))\n\n    @final\n    def __to_fastmcp(self) -&gt; FastMCPTool:\n        field_definitions = {\n            field: (field_info.annotation, field_info)\n            for field, field_info in self.args_schema.model_fields.items()\n        }\n        arg_model = create_model(\n            f\"{self.name}arguments\",\n            **field_definitions,\n            __base__=ArgModelBase,\n        )\n        fn_metadata = FuncMetadata(\n            arg_model=arg_model,\n            output_model=self.output_schema,\n            output_schema=self.output_schema.model_json_schema(),\n        )\n\n        async def fn(**input) -&gt; self.output_schema:\n            x = self.args_schema(**input)\n            return self(x)\n\n        return FastMCPTool(\n            fn=fn,\n            name=self.name,\n            description=self.description,\n            parameters=self.args_schema.model_json_schema(),\n            fn_metadata=fn_metadata,\n            is_async=True,\n        )\n\n    @final\n    def add_to_fastmcp(self, server: FastMCP) -&gt; FastMCPTool:\n        \"\"\"Add `self` as a tool to `server`\"\"\"\n        fasttool = self.__to_fastmcp()\n\n        if fasttool.name not in server._tool_manager._tools:\n            server._tool_manager._tools[fasttool.name] = fasttool\n\n        elif server._tool_manager.warn_on_duplicate_tools:\n            logging.warning(f\"Tool already exists: {fasttool.name}\")\n\n        return fasttool\n</code></pre>"},{"location":"api_reference/tools/#ursa.tools.fm_base_tool.TorchModuleTool.args_schema","title":"<code>args_schema</code>  <code>instance-attribute</code>","text":"<p>The input schema for the model</p>"},{"location":"api_reference/tools/#ursa.tools.fm_base_tool.TorchModuleTool.batch_size","title":"<code>batch_size = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Inputs to the model will be batched into set of at most this size</p>"},{"location":"api_reference/tools/#ursa.tools.fm_base_tool.TorchModuleTool.description","title":"<code>description</code>  <code>instance-attribute</code>","text":"<p>What the foundation model does / how to use it</p>"},{"location":"api_reference/tools/#ursa.tools.fm_base_tool.TorchModuleTool.device","title":"<code>device = Field(default_factory=default_device)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The accelerator on which the model is placed</p>"},{"location":"api_reference/tools/#ursa.tools.fm_base_tool.TorchModuleTool.fm","title":"<code>fm</code>  <code>instance-attribute</code>","text":"<p>The underlying PyTorch model used for inference</p>"},{"location":"api_reference/tools/#ursa.tools.fm_base_tool.TorchModuleTool.name","title":"<code>name = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>A short name for the foundation model</p>"},{"location":"api_reference/tools/#ursa.tools.fm_base_tool.TorchModuleTool.output_schema","title":"<code>output_schema</code>  <code>instance-attribute</code>","text":"<p>The output_schema for the model</p>"},{"location":"api_reference/tools/#ursa.tools.fm_base_tool.TorchModuleTool.add_to_fastmcp","title":"<code>add_to_fastmcp(server)</code>","text":"<p>Add <code>self</code> as a tool to <code>server</code></p> Source code in <code>src/ursa/tools/fm_base_tool.py</code> <pre><code>@final\ndef add_to_fastmcp(self, server: FastMCP) -&gt; FastMCPTool:\n    \"\"\"Add `self` as a tool to `server`\"\"\"\n    fasttool = self.__to_fastmcp()\n\n    if fasttool.name not in server._tool_manager._tools:\n        server._tool_manager._tools[fasttool.name] = fasttool\n\n    elif server._tool_manager.warn_on_duplicate_tools:\n        logging.warning(f\"Tool already exists: {fasttool.name}\")\n\n    return fasttool\n</code></pre>"},{"location":"api_reference/tools/#ursa.tools.fm_base_tool.TorchModuleTool.from_pretrained","title":"<code>from_pretrained(pretrained_model_name_or_path, **kwargs)</code>  <code>classmethod</code>","text":"<p>Instantiate tool from a pretrained checkpoint either on disk, or automatically downloaded from a repository (i.e. HuggingFace)</p> Source code in <code>src/ursa/tools/fm_base_tool.py</code> <pre><code>@classmethod\ndef from_pretrained(\n    cls, pretrained_model_name_or_path: str, **kwargs\n) -&gt; \"TorchModuleTool\":\n    \"\"\"Instantiate tool from a pretrained checkpoint either on disk,\n    or automatically downloaded from a repository (i.e. HuggingFace)\n    \"\"\"\n    raise NotImplementedError()\n</code></pre>"},{"location":"api_reference/tools/#ursa.tools.fm_base_tool.TorchModuleTool.postprocess","title":"<code>postprocess(model_output)</code>","text":"<p>Postprocess the model's raw output into a relevant tool output format</p> Source code in <code>src/ursa/tools/fm_base_tool.py</code> <pre><code>def postprocess(self, model_output: ModelOutput) -&gt; Iterable[Output]:\n    \"\"\"Postprocess the model's raw output into a relevant tool output format\"\"\"\n    yield from model_output\n</code></pre>"},{"location":"api_reference/tools/#ursa.tools.fm_base_tool.TorchModuleTool.preprocess","title":"<code>preprocess(input)</code>","text":"<p>Convert tool input into the form accepted by the model The input will be of type <code>list[args_schema]</code> with a length of <code>batch_size</code></p> <p>Defaults to <code>torch.data.default_collate</code></p> Source code in <code>src/ursa/tools/fm_base_tool.py</code> <pre><code>def preprocess(self, input: Sequence[Input]) -&gt; ModelInput:\n    \"\"\"\n    Convert tool input into the form accepted by the model\n    The input will be of type `list[args_schema]` with a length\n    of `batch_size`\n\n    Defaults to `torch.data.default_collate`\n    \"\"\"\n\n    return default_collate(list(input))\n</code></pre>"},{"location":"api_reference/tools/#ursa.tools.read_file_tool","title":"<code>read_file_tool</code>","text":""},{"location":"api_reference/tools/#ursa.tools.read_file_tool.read_file","title":"<code>read_file(filename, runtime)</code>","text":"<p>Read a file from the workspace.</p> <ul> <li>If filename ends with .pdf, extract text from the PDF.</li> <li>If extracted text is very small (likely scanned), optionally run OCR to add a text layer.</li> <li>Otherwise read as UTF-8 text.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>File name relative to the workspace directory.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Extracted text content.</p> Source code in <code>src/ursa/tools/read_file_tool.py</code> <pre><code>@tool\ndef read_file(filename: str, runtime: ToolRuntime[AgentContext]) -&gt; str:\n    \"\"\"Read a file from the workspace.\n\n    - If filename ends with .pdf, extract text from the PDF.\n    - If extracted text is very small (likely scanned), optionally run OCR to add a text layer.\n    - Otherwise read as UTF-8 text.\n\n    Args:\n        filename: File name relative to the workspace directory.\n\n    Returns:\n        Extracted text content.\n    \"\"\"\n    full_filename = runtime.context.workspace.joinpath(filename)\n\n    print(\"[READING]:\", full_filename)\n    # Move all the reading to a function in the parse util\n    text = read_text_from_file(full_filename)\n    return text\n</code></pre>"},{"location":"api_reference/tools/#ursa.tools.run_command_tool","title":"<code>run_command_tool</code>","text":""},{"location":"api_reference/tools/#ursa.tools.run_command_tool.run_command","title":"<code>run_command(query, runtime)</code>","text":"<p>Execute a shell command in the workspace and return its combined output.</p> <p>Runs the specified command using subprocess.run in the given workspace directory, captures stdout and stderr, enforces a maximum character budget, and formats both streams into a single string. KeyboardInterrupt during execution is caught and reported.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>AsciiStr</code> <p>The shell command to execute.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A formatted string with \"STDOUT:\" followed by the truncated stdout and</p> <code>str</code> <p>\"STDERR:\" followed by the truncated stderr.</p> Source code in <code>src/ursa/tools/run_command_tool.py</code> <pre><code>@tool\ndef run_command(query: AsciiStr, runtime: ToolRuntime[AgentContext]) -&gt; str:\n    \"\"\"Execute a shell command in the workspace and return its combined output.\n\n    Runs the specified command using subprocess.run in the given workspace\n    directory, captures stdout and stderr, enforces a maximum character budget,\n    and formats both streams into a single string. KeyboardInterrupt during\n    execution is caught and reported.\n\n    Args:\n        query: The shell command to execute.\n\n    Returns:\n        A formatted string with \"STDOUT:\" followed by the truncated stdout and\n        \"STDERR:\" followed by the truncated stderr.\n    \"\"\"\n    workspace_dir = Path(runtime.context.workspace)\n    if runtime.store is not None:\n        search_results = runtime.store.search(\n            (\"workspace\", \"file_edit\"), limit=1000\n        )\n        edited_files = [item.key for item in search_results]\n    else:\n        edited_files = []\n\n    if runtime.store is not None:\n        search_results = runtime.store.search(\n            (\"workspace\", \"safe_codes\"), limit=1000\n        )\n        safe_codes = [item.key for item in search_results]\n    else:\n        safe_codes = []\n\n    llm = runtime.context.llm\n    safety_result = llm.with_structured_output(SafetyAssessment).invoke(\n        get_safety_prompt(query, safe_codes, edited_files)\n    )\n\n    if not safety_result[\"is_safe\"]:\n        tool_response = f\"[UNSAFE] That command `{query}` was deemed unsafe and cannot be run.\\nFor reason: {safety_result['reason']}\"\n        console.print(\n            \"[bold red][WARNING][/bold red] Command deemed unsafe:\",\n            query,\n        )\n        # Also surface the model's rationale for transparency.\n        console.print(\"[bold red][WARNING][/bold red] REASON:\", tool_response)\n        return tool_response\n    else:\n        console.print(\n            f\"[green]Command passed safety check:[/green] {query}\\nFor reason: {safety_result['reason']}\"\n        )\n\n    print(\"RUNNING: \", query)\n\n    try:\n        result = subprocess.run(\n            query,\n            text=True,\n            shell=True,\n            timeout=60000,\n            capture_output=True,\n            cwd=workspace_dir,\n        )\n        stdout, stderr = result.stdout, result.stderr\n    except KeyboardInterrupt:\n        print(\"Keyboard Interrupt of command: \", query)\n        stdout, stderr = \"\", \"KeyboardInterrupt:\"\n\n    # Fit BOTH streams under a single overall cap\n    stdout_fit, stderr_fit = _fit_streams_to_budget(\n        stdout or \"\", stderr or \"\", runtime.context.tool_character_limit\n    )\n\n    print(\"STDOUT: \", stdout_fit)\n    print(\"STDERR: \", stderr_fit)\n\n    return f\"STDOUT:\\n{stdout_fit}\\nSTDERR:\\n{stderr_fit}\"\n</code></pre>"},{"location":"api_reference/tools/#ursa.tools.search_tools","title":"<code>search_tools</code>","text":""},{"location":"api_reference/tools/#ursa.tools.search_tools.run_arxiv_search","title":"<code>run_arxiv_search(prompt, query, runtime, max_results=3)</code>","text":"<p>Search ArXiv for the first 'max_results' papers and summarize them in the context of the user prompt</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>string describing the information the agent is interested in from arxiv papers</p> required <code>query</code> <code>str</code> <p>1 and 8 word search query for the Arxiv search API to find papers relevant to the prompt</p> required <code>max_results</code> <code>int</code> <p>integer number of papers to return (defaults 3). Request fewer if searching for something very specific or a larger number if broadly searching for information. Do not exceeed 10.</p> <code>3</code> Source code in <code>src/ursa/tools/search_tools.py</code> <pre><code>@tool\ndef run_arxiv_search(\n    prompt: str, query: str, runtime: ToolRuntime, max_results: int = 3\n):\n    \"\"\"\n    Search ArXiv for the first 'max_results' papers and summarize them in the context\n    of the user prompt\n\n    Arguments:\n        prompt:\n            string describing the information the agent is interested in from arxiv papers\n        query:\n            1 and 8 word search query for the Arxiv search API to find papers relevant to the prompt\n        max_results:\n            integer number of papers to return (defaults 3). Request fewer if searching for something\n            very specific or a larger number if broadly searching for information. Do not exceeed 10.\n    \"\"\"\n    try:\n        agent = ArxivAgent(\n            llm=runtime.context.llm,\n            summarize=True,\n            process_images=False,\n            max_results=max_results,\n            workspace=runtime.context.workspace,\n            # rag_embedding=self.embedding,\n            database_path=Path(\"./arxiv_downloaded\"),\n            summaries_path=Path(\"./arxiv_summaries\"),\n            download=True,\n        )\n        console.print(f\"[bold cyan]Searching ArXiv for: [default]{query}\")\n        assert isinstance(query, str)\n\n        arxiv_result = asyncio.run(\n            agent.ainvoke(\n                arxiv_search_query=query,\n                context=prompt,\n            )\n        )[\"final_summary\"]\n\n        console.print(\n            Panel(\n                f\"{arxiv_result}\",\n                title=f\"[bold cyan on black]ArXiv summary for {query}\",\n                border_style=\"cyan on black\",\n                style=\"cyan on black\",\n            )\n        )\n        return f\"[ArXiv Agent Output]:\\n {arxiv_result}\"\n    except Exception as e:\n        return f\"Unexpected error while running ArxivAgent: {e}\"\n</code></pre>"},{"location":"api_reference/tools/#ursa.tools.search_tools.run_osti_search","title":"<code>run_osti_search(prompt, query, runtime, max_results=3)</code>","text":"<p>Search OSTI.gov for the first 'max_results' papers and summarize them in the context of the user prompt</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>string describing the information the agent is interested in from arxiv papers</p> required <code>query</code> <code>str</code> <p>1 and 8 word search query for the OSTI.gov search API to find papers relevant to the prompt</p> required <code>max_results</code> <code>int</code> <p>integer number of papers to return (defaults 3). Request fewer if searching for something very specific or a larger number if broadly searching for information. Do not exceeed 10.</p> <code>3</code> Source code in <code>src/ursa/tools/search_tools.py</code> <pre><code>@tool\ndef run_osti_search(\n    prompt: str,\n    query: str,\n    runtime: ToolRuntime,\n    max_results: int = 3,\n):\n    \"\"\"\n    Search OSTI.gov for the first 'max_results' papers and summarize them in the context\n    of the user prompt\n\n    Arguments:\n        prompt:\n            string describing the information the agent is interested in from arxiv papers\n        query:\n            1 and 8 word search query for the OSTI.gov search API to find papers relevant to the prompt\n        max_results:\n            integer number of papers to return (defaults 3). Request fewer if searching for something\n            very specific or a larger number if broadly searching for information. Do not exceeed 10.\n    \"\"\"\n    max_results\n    try:\n        agent = OSTIAgent(\n            llm=runtime.context.llm,\n            summarize=True,\n            process_images=False,\n            max_results=max_results,\n            workspace=runtime.context.workspace,\n            # rag_embedding=self.embedding,\n            database_path=Path(\"./osti_downloaded_papers\"),\n            summaries_path=Path(\"./osti_generated_summaries\"),\n            vectorstore_path=Path(\"./osti_vectorstores\"),\n            download=True,\n        )\n        console.print(f\"[bold cyan]Searching OSTI.gov for: [default]{query}\")\n        assert isinstance(query, str)\n\n        osti_result = asyncio.run(\n            agent.ainvoke(\n                query=query,\n                context=prompt,\n            )\n        )[\"final_summary\"]\n\n        console.print(\n            Panel(\n                f\"[cyan on black]{osti_result}\",\n                title=f\"[bold cyan on black]OSTI.gov summary for {query}\",\n                border_style=\"cyan on black\",\n                style=\"cyan on black\",\n            )\n        )\n        return f\"[OSTI Agent Output]:\\n {osti_result}\"\n    except Exception as e:\n        return f\"Unexpected error while running OSTIAgent: {e}\"\n</code></pre>"},{"location":"api_reference/tools/#ursa.tools.search_tools.run_web_search","title":"<code>run_web_search(prompt, query, runtime, max_results=3)</code>","text":"<p>Search the internet for the first 'max_results' pages and summarize them in the context of the user prompt</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>string describing the information the agent is interested in from websites</p> required <code>query</code> <code>str</code> <p>1 and 8 word search query for the web search engines to find papers relevant to the prompt</p> required <code>max_results</code> <code>int</code> <p>integer number of pages to return (defaults 3). Request fewer if searching for something very specific or a larger number if broadly searching for information. Do not exceeed 10.</p> <code>3</code> Source code in <code>src/ursa/tools/search_tools.py</code> <pre><code>@tool\ndef run_web_search(\n    prompt: str,\n    query: str,\n    runtime: ToolRuntime,\n    max_results: int = 3,\n):\n    \"\"\"\n    Search the internet for the first 'max_results' pages and summarize them in the context\n    of the user prompt\n\n    Arguments:\n        prompt:\n            string describing the information the agent is interested in from websites\n        query:\n            1 and 8 word search query for the web search engines to find papers relevant to the prompt\n        max_results:\n            integer number of pages to return (defaults 3). Request fewer if searching for something\n            very specific or a larger number if broadly searching for information. Do not exceeed 10.\n    \"\"\"\n    try:\n        agent = WebSearchAgent(\n            llm=runtime.context.llm,\n            summarize=True,\n            process_images=False,\n            max_results=max_results,\n            workspace=runtime.context.workspace,\n            # rag_embedding=self.embedding,\n            database_path=Path(\"./web_downloads\"),\n            summaries_path=Path(\"./web_summaries\"),\n            download=True,\n        )\n        console.print(f\"[bold cyan]Searching Web for: [default]{query}\")\n        assert isinstance(query, str)\n\n        web_result = asyncio.run(\n            agent.ainvoke(\n                query=query,\n                context=prompt,\n            )\n        )[\"final_summary\"]\n\n        console.print(\n            Panel(\n                f\"{web_result}\",\n                title=f\"[bold cyan on black]Web summary for {query}\",\n                border_style=\"cyan on black\",\n                style=\"cyan on black\",\n            )\n        )\n        return f\"[Web Search Agent Output]:\\n {web_result}\"\n    except Exception as e:\n        return f\"Unexpected error while running WebSearchAgent: {e}\"\n</code></pre>"},{"location":"api_reference/tools/#ursa.tools.write_code_tool","title":"<code>write_code_tool</code>","text":""},{"location":"api_reference/tools/#ursa.tools.write_code_tool.edit_code","title":"<code>edit_code(old_code, new_code, filename, runtime)</code>","text":"<p>Replace the first occurrence of old_code with new_code in filename.</p> <p>Parameters:</p> Name Type Description Default <code>old_code</code> <code>str</code> <p>Code fragment to search for.</p> required <code>new_code</code> <code>str</code> <p>Replacement fragment.</p> required <code>filename</code> <code>AsciiStr</code> <p>Target file inside the workspace.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Success / failure message.</p> Source code in <code>src/ursa/tools/write_code_tool.py</code> <pre><code>@tool\ndef edit_code(\n    old_code: str,\n    new_code: str,\n    filename: AsciiStr,\n    runtime: ToolRuntime[AgentContext],\n) -&gt; str:\n    \"\"\"Replace the **first** occurrence of *old_code* with *new_code* in *filename*.\n\n    Args:\n        old_code: Code fragment to search for.\n        new_code: Replacement fragment.\n        filename: Target file inside the workspace.\n\n    Returns:\n        Success / failure message.\n    \"\"\"\n    workspace_dir = runtime.context.workspace\n    console.print(\"[cyan]Editing file:[/cyan]\", filename)\n\n    code_file = Path(workspace_dir, filename)\n    try:\n        content = read_text_file(code_file)\n    except FileNotFoundError:\n        console.print(\n            \"[bold bright_white on red] :heavy_multiplication_x: [/] \"\n            \"[red]File not found:[/]\",\n        )\n        return f\"Failed: {filename} not found.\"\n\n    # Clean up markdown fences\n    old_code_clean = old_code\n    new_code_clean = new_code\n\n    if old_code_clean not in content:\n        console.print(\n            \"[yellow] \u26a0\ufe0f 'old_code' not found in file'; no changes made.[/]\"\n        )\n        return f\"No changes made to {filename}: 'old_code' not found in file.\"\n\n    updated = content.replace(old_code_clean, new_code_clean, 1)\n\n    console.print(\n        Panel(\n            DiffRenderer(content, updated, filename),\n            title=\"Diff Preview\",\n            border_style=\"cyan\",\n        )\n    )\n\n    try:\n        with open(code_file, \"w\", encoding=\"utf-8\") as f:\n            f.write(updated)\n    except Exception as exc:\n        console.print(\n            \"[bold bright_white on red] :heavy_multiplication_x: [/] \"\n            \"[red]Failed to write file:[/]\",\n            exc,\n        )\n        return f\"Failed to edit {filename}.\"\n\n    console.print(\n        f\"[bold bright_white on green] :heavy_check_mark: [/] \"\n        f\"[green]File updated:[/] {code_file}\"\n    )\n\n    # Record the edit operation\n    if (store := runtime.store) is not None:\n        store.put(\n            (\"workspace\", \"file_edit\"),\n            filename,\n            {\n                \"modified\": time.time(),\n                \"tool_call_id\": runtime.tool_call_id,\n                \"thread_id\": runtime.config.get(\"metadata\", {}).get(\n                    \"thread_id\", None\n                ),\n            },\n        )\n    return f\"File {filename} updated successfully.\"\n</code></pre>"},{"location":"api_reference/tools/#ursa.tools.write_code_tool.write_code","title":"<code>write_code(code, filename, runtime)</code>","text":"<p>Write source code to a file</p> <p>Records successful file edits to the graph's store</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>The source code content to be written to disk.</p> required <code>filename</code> <code>AsciiStr</code> <p>Name of the target file (including its extension).</p> required Source code in <code>src/ursa/tools/write_code_tool.py</code> <pre><code>@tool(description=\"Write source code to a file\")\ndef write_code(\n    code: str,\n    filename: AsciiStr,\n    runtime: ToolRuntime[AgentContext],\n) -&gt; str:\n    \"\"\"Write source code to a file\n\n    Records successful file edits to the graph's store\n\n    Args:\n        code: The source code content to be written to disk.\n        filename: Name of the target file (including its extension).\n\n    \"\"\"\n    # Determine the full path to the target file\n    workspace_dir = runtime.context.workspace\n    console.print(\"[cyan]Writing file:[/]\", filename)\n\n    # Show syntax-highlighted preview before writing to file\n    try:\n        lexer_name = Syntax.guess_lexer(filename, code)\n    except Exception:\n        lexer_name = \"text\"\n\n    console.print(\n        Panel(\n            Syntax(code, lexer_name, line_numbers=True),\n            title=\"File Preview\",\n            border_style=\"cyan\",\n        )\n    )\n\n    # Write cleaned code to disk\n    code_file = workspace_dir.joinpath(filename)\n    try:\n        with open(code_file, \"w\", encoding=\"utf-8\") as f:\n            f.write(code)\n    except Exception as exc:\n        console.print(\n            \"[bold bright_white on red] :heavy_multiplication_x: [/] \"\n            \"[red]Failed to write file:[/]\",\n            exc,\n        )\n        return f\"Failed to write {filename}.\"\n\n    console.print(\n        f\"[bold bright_white on green] :heavy_check_mark: [/] \"\n        f\"[green]File written:[/] {code_file}\"\n    )\n\n    # Record the edit operation\n    if (store := runtime.store) is not None:\n        store.put(\n            (\"workspace\", \"file_edit\"),\n            filename,\n            {\n                \"modified\": time.time(),\n                \"tool_call_id\": runtime.tool_call_id,\n                \"thread_id\": runtime.config.get(\"metadata\", {}).get(\n                    \"thread_id\", None\n                ),\n            },\n        )\n    return f\"File {filename} written successfully.\"\n</code></pre>"},{"location":"api_reference/util/","title":"util","text":""},{"location":"api_reference/util/#ursa.util.Checkpointer","title":"<code>Checkpointer</code>","text":"Source code in <code>src/ursa/util/__init__.py</code> <pre><code>class Checkpointer:\n    @classmethod\n    def from_workspace(\n        cls,\n        workspace: Path,\n        db_dir: str = \"db\",\n        db_name: str = \"checkpointer.db\",\n    ) -&gt; SqliteSaver:\n        (db_path := workspace / db_dir).mkdir(parents=True, exist_ok=True)\n        conn = sqlite3.connect(str(db_path / db_name), check_same_thread=False)\n        return SqliteSaver(conn)\n\n    @classmethod\n    def from_path(\n        cls, db_path: Path, db_name: str = \"checkpointer.db\"\n    ) -&gt; SqliteSaver:\n        \"\"\"Make checkpointer sqlite db.\n\n        Args\n        ====\n        * db_path: The path to the SQLite database file (e.g. ./checkpoint.db) to be created.\n        \"\"\"\n\n        db_path.parent.mkdir(parents=True, exist_ok=True)\n        conn = sqlite3.connect(str(db_path / db_name), check_same_thread=False)\n        return SqliteSaver(conn)\n</code></pre>"},{"location":"api_reference/util/#ursa.util.Checkpointer.from_path","title":"<code>from_path(db_path, db_name='checkpointer.db')</code>  <code>classmethod</code>","text":"<p>Make checkpointer sqlite db.</p>"},{"location":"api_reference/util/#ursa.util.Checkpointer.from_path--args","title":"Args","text":"<ul> <li>db_path: The path to the SQLite database file (e.g. ./checkpoint.db) to be created.</li> </ul> Source code in <code>src/ursa/util/__init__.py</code> <pre><code>@classmethod\ndef from_path(\n    cls, db_path: Path, db_name: str = \"checkpointer.db\"\n) -&gt; SqliteSaver:\n    \"\"\"Make checkpointer sqlite db.\n\n    Args\n    ====\n    * db_path: The path to the SQLite database file (e.g. ./checkpoint.db) to be created.\n    \"\"\"\n\n    db_path.parent.mkdir(parents=True, exist_ok=True)\n    conn = sqlite3.connect(str(db_path / db_name), check_same_thread=False)\n    return SqliteSaver(conn)\n</code></pre>"},{"location":"api_reference/util/#ursa.util.diff_renderer","title":"<code>diff_renderer</code>","text":""},{"location":"api_reference/util/#ursa.util.diff_renderer.DiffRenderer","title":"<code>DiffRenderer</code>","text":"<p>Renderable diff\u2014<code>console.print(DiffRenderer(...))</code></p> Source code in <code>src/ursa/util/diff_renderer.py</code> <pre><code>class DiffRenderer:\n    \"\"\"Renderable diff\u2014`console.print(DiffRenderer(...))`\"\"\"\n\n    def __init__(self, content: str, updated: str, filename: str):\n        # total lines in each version\n        self._old_total = len(content.splitlines())\n        self._new_total = len(updated.splitlines())\n\n        # number of digits in the largest count\n        self._num_width = len(str(max(self._old_total, self._new_total))) + 2\n\n        # get the diff\n        self._diff_lines = list(\n            difflib.unified_diff(\n                content.splitlines(),\n                updated.splitlines(),\n                fromfile=f\"{filename} (original)\",\n                tofile=f\"{filename} (modified)\",\n                lineterm=\"\",\n            )\n        )\n\n        # get syntax style\n        try:\n            self._lexer_name = Syntax.guess_lexer(filename, updated)\n        except Exception:\n            self._lexer_name = \"text\"\n\n    def __rich_console__(\n        self, console: Console, opts: ConsoleOptions\n    ) -&gt; RenderResult:\n        old_line = new_line = None\n        width = console.width\n\n        for raw in self._diff_lines:\n            # grab line numbers from hunk header\n            if m := _HUNK_RE.match(raw):\n                old_line, new_line = map(int, m.groups())\n                # build a marker\n                n = self._num_width\n                tick_col = \".\" * (n - 1)\n                indent_ticks = f\" {tick_col} {tick_col}\"\n                # pad to the indent width\n                full_indent = indent_ticks.ljust(2 * n + 3)\n                yield Text(\n                    f\"{full_indent}{raw}\".ljust(width), style=\"white on grey30\"\n                )\n                continue\n\n            # skip header lines\n            if raw.startswith((\"---\", \"+++\")):\n                continue\n\n            # split the line\n            if raw.startswith(\"+\"):\n                style = _STYLE[\"add\"]\n                code = raw[1:]\n            elif raw.startswith(\"-\"):\n                style = _STYLE[\"del\"]\n                code = raw[1:]\n            else:\n                style = _STYLE[\"ctx\"]\n                code = raw[1:] if raw.startswith(\" \") else raw\n\n            # compute line numbers\n            if raw.startswith(\"+\"):\n                old_num, new_num = None, new_line\n                new_line += 1\n            elif raw.startswith(\"-\"):\n                old_num, new_num = old_line, None\n                old_line += 1\n            else:\n                old_num, new_num = old_line, new_line\n                old_line += 1\n                new_line += 1\n\n            old_str = str(old_num) if old_num is not None else \" \"\n            new_str = str(new_num) if new_num is not None else \" \"\n\n            # Syntax-highlight the code part\n            syntax = Syntax(\n                code, self._lexer_name, line_numbers=False, word_wrap=False\n            )\n            text_code: Text = syntax.highlight(code)\n            if text_code.plain.endswith(\"\\n\"):\n                text_code = text_code[:-1]\n            # apply background\n            text_code.stylize(style.bg)\n\n            # line numbers + code\n            nums = Text(\n                f\"{old_str:&gt;{self._num_width}}{new_str:&gt;{self._num_width}} \",\n                style=f\"white {style.bg}\",\n            )\n            diff_mark = Text(style.prefix, style=f\"bright_white {style.bg}\")\n            line_text = nums + diff_mark + text_code\n\n            # pad to console width\n            pad_len = width - line_text.cell_len\n            if pad_len &gt; 0:\n                line_text.append(\" \" * pad_len, style=style.bg)\n\n            yield line_text\n</code></pre>"},{"location":"api_reference/util/#ursa.util.helperFunctions","title":"<code>helperFunctions</code>","text":""},{"location":"api_reference/util/#ursa.util.helperFunctions.run_tool_calls","title":"<code>run_tool_calls(ai_msg, tools)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>ai_msg</code> <code>AIMessage</code> <p>The LLM's AIMessage containing tool calls.</p> required <code>tools</code> <code>ToolRegistry | Iterable[Runnable | Callable[..., Any]]</code> <p>Either a dict {name: tool} or an iterable of tools (must have <code>.name</code>    for mapping). Each tool can be a Runnable or a plain callable.</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>list[BaseMessage]</code> <p>list[BaseMessage] to feed back to the model</p> Source code in <code>src/ursa/util/helperFunctions.py</code> <pre><code>def run_tool_calls(\n    ai_msg: AIMessage,\n    tools: ToolRegistry | Iterable[Runnable | Callable[..., Any]],\n) -&gt; list[BaseMessage]:\n    \"\"\"\n    Args:\n        ai_msg: The LLM's AIMessage containing tool calls.\n        tools: Either a dict {name: tool} or an iterable of tools (must have `.name`\n               for mapping). Each tool can be a Runnable or a plain callable.\n\n    Returns:\n        out: list[BaseMessage] to feed back to the model\n    \"\"\"\n    # Build a name-&gt;tool map\n    if isinstance(tools, dict):\n        registry: ToolRegistry = tools  # type: ignore\n    else:\n        registry = {}\n        for t in tools:\n            name = getattr(t, \"name\", None) or getattr(t, \"__name__\", None)\n            if not name:\n                raise ValueError(f\"Tool {t!r} has no discoverable name.\")\n            registry[name] = t  # type: ignore\n\n    calls = extract_tool_calls(ai_msg)\n\n    if not calls:\n        return []\n\n    out: list[BaseMessage] = []\n    for call in calls:\n        name = call.get(\"name\")\n        args = call.get(\"args\", {}) or {}\n        call_id = call.get(\"id\") or f\"call_{uuid.uuid4().hex}\"\n\n        # 1) the AIMessage that generated the call\n        out.append(ai_msg)\n\n        # 2) the ToolMessage with the execution result (or error)\n        if name not in registry:\n            content = f\"ERROR: unknown tool '{name}'.\"\n        else:\n            try:\n                result = _invoke_tool(registry[name], args)\n                content = _stringify_output(result)\n            except Exception as e:\n                content = f\"ERROR: {type(e).__name__}: {e}\"\n\n        out.append(\n            ToolMessage(content=content, tool_call_id=call_id, name=name)\n        )\n\n    return out\n</code></pre>"},{"location":"api_reference/util/#ursa.util.logo_generator","title":"<code>logo_generator</code>","text":""},{"location":"api_reference/util/#ursa.util.logo_generator.generate_logo_sync","title":"<code>generate_logo_sync(*, problem_text, workspace, out_dir, filename=None, model=DEFAULT_IMAGE_MODEL, size=None, background='opaque', quality='high', n=1, overwrite=False, style='sticker', allow_text=False, palette=None, mode='logo', aspect='square', style_intensity='overt', aperture=DEFAULT_APERTURE, console=None, image_model_provider='openai', image_provider_kwargs=None)</code>","text":"<p>Generate images.</p> <p>Key change (diversity):   - We no longer rely on a single prompt with n&gt;1 siblings for scenes.   - If mode='scene' and style='random' and n&gt;1, we pick n distinct scene styles     (horror/fantasy/etc) and generate 1 image per style/prompt.</p> Return value <ul> <li>Returns the \"main\" path (first generated image). Additional variants are saved alongside it.</li> </ul> Source code in <code>src/ursa/util/logo_generator.py</code> <pre><code>def generate_logo_sync(\n    *,\n    problem_text: str,\n    workspace: str,\n    out_dir: str | Path,\n    filename: str | None = None,\n    model: str = DEFAULT_IMAGE_MODEL,\n    size: str | None = None,\n    background: str = \"opaque\",\n    quality: str = \"high\",\n    n: int = 1,\n    overwrite: bool = False,\n    style: str = \"sticker\",\n    allow_text: bool = False,\n    palette: str | None = None,\n    mode: str = \"logo\",\n    aspect: str = \"square\",\n    style_intensity: str = \"overt\",\n    aperture: float = DEFAULT_APERTURE,\n    console: Optional[Console] = None,\n    image_model_provider: str = \"openai\",\n    image_provider_kwargs: Optional[dict] = None,\n) -&gt; Path:\n    \"\"\"\n    Generate images.\n\n    Key change (diversity):\n      - We no longer rely on a single prompt with n&gt;1 siblings for scenes.\n      - If mode='scene' and style='random' and n&gt;1, we pick n distinct scene styles\n        (horror/fantasy/etc) and generate 1 image per style/prompt.\n\n    Return value:\n      - Returns the \"main\" path (first generated image). Additional variants are saved alongside it.\n    \"\"\"\n    out_dir = Path(out_dir)\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    # this is how we'll pass through a vision model and provider/url/endpoint\n    client_kwargs = {}\n    if image_provider_kwargs:\n        # Only pass through safe/known kwargs\n        for k in (\"api_key\", \"base_url\", \"organization\"):\n            if k in image_provider_kwargs and image_provider_kwargs[k]:\n                client_kwargs[k] = image_provider_kwargs[k]\n    client = OpenAI(**client_kwargs)\n\n    final_size = _normalize_size(size, aspect, mode)\n    # Scenes tend to look odd with transparent backgrounds; force opaque.\n    final_background = \"opaque\" if mode == \"scene\" else background\n\n    # -------------------------\n    # Multi-style scene generation (requested change)\n    # -------------------------\n    if (\n        mode == \"scene\"\n        and n &gt; 1\n        and SCENE_MULTI_STYLE_DEFAULT\n        and (style is None or style.strip().lower() == \"random\")\n        and filename\n        is None  # filename implies \"single series\"; keep single-style naming\n    ):\n        style_slugs = _choose_n_distinct_styles(n)\n        out_paths = _compose_multi_scene_paths(out_dir, workspace, style_slugs)\n\n        # If everything already exists and overwrite is False, skip regeneration.\n        if not overwrite and all(p.exists() for p in out_paths):\n            return out_paths[0]\n\n        for idx, (style_slug, path) in enumerate(\n            zip(style_slugs, out_paths), start=1\n        ):\n            prompt, _ = _craft_logo_prompt(\n                problem_text,\n                workspace,\n                style=style_slug,\n                allow_text=allow_text,\n                palette=palette,\n                mode=\"scene\",\n                style_intensity=style_intensity,\n                aperture=aperture,\n            )\n\n            extra_title = (\n                f\"[bold magenta]mode: scene[/bold magenta] [dim]\u2022[/dim] \"\n                f\"aspect: {aspect} [dim]\u2022[/dim] variant {idx}/{n}\"\n            )\n            _render_prompt_panel(\n                console=console,\n                style_slug=style_slug,\n                workspace=workspace,\n                prompt=prompt,\n                extra_title=extra_title,\n            )\n\n            if path.exists() and not overwrite:\n                continue\n\n            kwargs = dict(\n                model=model,\n                prompt=prompt,\n                size=final_size,\n                n=1,\n                quality=quality,\n                background=final_background,\n            )\n            try:\n                resp = client.images.generate(**kwargs)\n            except Exception:\n                # Some models ignore/forbid background=; retry without it\n                kwargs.pop(\"background\", None)\n                resp = client.images.generate(**kwargs)\n\n            path.write_bytes(base64.b64decode(resp.data[0].b64_json))\n\n        return out_paths[0]\n\n    # -------------------------\n    # Default behavior (single-style series)\n    #   - Also improved diversity: when n&gt;1, we do n separate prompts (not siblings).\n    # -------------------------\n    # Build filenames for this series\n    prompt0, style_slug0 = _craft_logo_prompt(\n        problem_text,\n        workspace,\n        style=style,\n        allow_text=allow_text,\n        palette=palette,\n        mode=mode,\n        style_intensity=style_intensity,\n        aperture=aperture,\n    )\n\n    main_path, alt_paths = _compose_filenames(\n        out_dir, style_slug0, filename, n, mode=mode\n    )\n\n    # If everything exists and overwrite is False, return main\n    if (\n        not overwrite\n        and main_path.exists()\n        and all(p.exists() for p in alt_paths)\n    ):\n        return main_path\n\n    # Generate 1 image per prompt (more divergence than n&gt;1 siblings)\n    paths = [main_path] + alt_paths\n    for idx, path in enumerate(paths, start=1):\n        # For n&gt;1, rebuild prompt each time so pools + aperture actually matter\n        prompt_i, style_slug_i = (\n            (prompt0, style_slug0)\n            if idx == 1\n            else _craft_logo_prompt(\n                problem_text,\n                workspace,\n                style=style,\n                allow_text=allow_text,\n                palette=palette,\n                mode=mode,\n                style_intensity=style_intensity,\n                aperture=aperture,\n            )\n        )\n\n        extra_title = (\n            f\"[bold magenta]mode: {mode}[/bold magenta] [dim]\u2022[/dim] \"\n            f\"aspect: {aspect} [dim]\u2022[/dim] variant {idx}/{len(paths)}\"\n        )\n        _render_prompt_panel(\n            console=console,\n            style_slug=style_slug_i,\n            workspace=workspace,\n            prompt=prompt_i,\n            extra_title=extra_title,\n        )\n\n        if path.exists() and not overwrite:\n            continue\n\n        kwargs = dict(\n            model=model,\n            prompt=prompt_i,\n            size=final_size,\n            n=1,\n            quality=quality,\n            background=final_background,\n        )\n        try:\n            resp = client.images.generate(**kwargs)\n        except Exception:\n            kwargs.pop(\"background\", None)\n            resp = client.images.generate(**kwargs)\n\n        path.write_bytes(base64.b64decode(resp.data[0].b64_json))\n\n    return main_path\n</code></pre>"},{"location":"api_reference/util/#ursa.util.mcp","title":"<code>mcp</code>","text":""},{"location":"api_reference/util/#ursa.util.memory_logger","title":"<code>memory_logger</code>","text":""},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory","title":"<code>AgentMemory</code>","text":"<p>Simple wrapper around a persistent Chroma vector-store for agent-conversation memory.</p>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory--parameters","title":"Parameters","text":"<p>path : str | Path | None     Where to keep the on-disk Chroma DB.  If None, a folder called     <code>agent_memory_db</code> is created in the package\u2019s base directory. collection_name : str     Name of the Chroma collection. embedding_model :  | None     the embedding model"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory--notes","title":"Notes","text":"<ul> <li>Requires <code>langchain-chroma</code>, and <code>chromadb</code>.</li> </ul> Source code in <code>src/ursa/util/memory_logger.py</code> <pre><code>class AgentMemory:\n    \"\"\"\n    Simple wrapper around a persistent Chroma vector-store for agent-conversation memory.\n\n    Parameters\n    ----------\n    path : str | Path | None\n        Where to keep the on-disk Chroma DB.  If *None*, a folder called\n        ``agent_memory_db`` is created in the package\u2019s base directory.\n    collection_name : str\n        Name of the Chroma collection.\n    embedding_model : &lt;TODO&gt; | None\n        the embedding model\n\n    Notes\n    -----\n    * Requires `langchain-chroma`, and `chromadb`.\n    \"\"\"\n\n    @classmethod\n    def get_db_path(cls, path: Optional[str | Path]) -&gt; Path:\n        match path:\n            case None:\n                return Path.home() / \".cache\" / \"ursa\" / \"rag\" / \"db\"\n            case str():\n                return Path(path)\n            case Path():\n                return path\n            case _:\n                raise TypeError(\n                    f\"Type of path is `{type(path)}` \"\n                    \"but `Optional[str | Path]` was expected.\"\n                )\n\n    def __init__(\n        self,\n        embedding_model,\n        path: Optional[str | Path] = None,\n        collection_name: str = \"agent_memory\",\n    ) -&gt; None:\n        self.path = self.get_db_path(path)\n        self.collection_name = collection_name\n        self.path.mkdir(parents=True, exist_ok=True)\n        self.embeddings = embedding_model\n\n        # If a DB already exists, load it; otherwise defer creation until `build_index`.\n        self.vectorstore: Optional[Chroma] = None\n        if any(self.path.iterdir()):\n            self.vectorstore = Chroma(\n                collection_name=self.collection_name,\n                embedding_function=self.embeddings,\n                persist_directory=str(self.path),\n            )\n\n    # --------------------------------------------------------------------- #\n    # \u2776 Build &amp; index a brand-new database                                   #\n    # --------------------------------------------------------------------- #\n    def build_index(\n        self,\n        chunks: Sequence[str],\n        metadatas: Optional[Sequence[dict[str, Any]]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Create a fresh vector store from ``chunks``.  Existing data (if any)\n        are overwritten.\n\n        Parameters\n        ----------\n        chunks : Sequence[str]\n            Text snippets (already chunked) to embed.\n        metadatas : Sequence[dict] | None\n            Optional metadata dict for each chunk, same length as ``chunks``.\n        \"\"\"\n        docs = [\n            Document(\n                page_content=text, metadata=metadatas[i] if metadatas else {}\n            )\n            for i, text in enumerate(chunks)\n        ]\n\n        # Create (or overwrite) the collection\n        self.vectorstore = Chroma.from_documents(\n            documents=docs,\n            embedding=self.embeddings,\n            collection_name=self.collection_name,\n            persist_directory=str(self.path),\n        )\n\n    # --------------------------------------------------------------------- #\n    # \u2777 Add new chunks and re-index                                          #\n    # --------------------------------------------------------------------- #\n    def add_memories(\n        self,\n        new_chunks: Sequence[str],\n        metadatas: Optional[Sequence[dict[str, Any]]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Append new text chunks to the existing store (must call `build_index`\n        first if the DB is empty).\n\n        Raises\n        ------\n        RuntimeError\n            If the vector store is not yet initialised.\n        \"\"\"\n        if self.vectorstore is None:\n            self.build_index(new_chunks, metadatas)\n            print(\"----- Vector store initialised -----\")\n\n        docs = []\n        for i, text in enumerate(new_chunks):\n            if len(text) &gt; 0:  # only add non-empty documents\n                docs.append(\n                    Document(\n                        page_content=text,\n                        metadata=metadatas[i] if metadatas else {},\n                    )\n                )\n        self.vectorstore.add_documents(docs)\n\n    # --------------------------------------------------------------------- #\n    # \u2778 Retrieve relevant chunks (RAG query)                                 #\n    # --------------------------------------------------------------------- #\n    def retrieve(\n        self,\n        query: str,\n        k: int = 4,\n        with_scores: bool = False,\n        **search_kwargs,\n    ):\n        \"\"\"\n        Return the *k* most similar chunks for `query`.\n\n        Parameters\n        ----------\n        query : str\n            Natural-language question or statement.\n        k : int\n            How many results to return.\n        with_scores : bool\n            If True, also return similarity scores.\n        **search_kwargs\n            Extra kwargs forwarded to Chroma\u2019s ``similarity_search*`` helpers.\n\n        Returns\n        -------\n        list[Document] | list[tuple[Document, float]]\n        \"\"\"\n        if self.vectorstore is None:\n            return [\"None\"]\n\n        if with_scores:\n            return self.vectorstore.similarity_search_with_score(\n                query, k=k, **search_kwargs\n            )\n        return self.vectorstore.similarity_search(query, k=k, **search_kwargs)\n</code></pre>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory.add_memories","title":"<code>add_memories(new_chunks, metadatas=None)</code>","text":"<p>Append new text chunks to the existing store (must call <code>build_index</code> first if the DB is empty).</p>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory.add_memories--raises","title":"Raises","text":"<p>RuntimeError     If the vector store is not yet initialised.</p> Source code in <code>src/ursa/util/memory_logger.py</code> <pre><code>def add_memories(\n    self,\n    new_chunks: Sequence[str],\n    metadatas: Optional[Sequence[dict[str, Any]]] = None,\n) -&gt; None:\n    \"\"\"\n    Append new text chunks to the existing store (must call `build_index`\n    first if the DB is empty).\n\n    Raises\n    ------\n    RuntimeError\n        If the vector store is not yet initialised.\n    \"\"\"\n    if self.vectorstore is None:\n        self.build_index(new_chunks, metadatas)\n        print(\"----- Vector store initialised -----\")\n\n    docs = []\n    for i, text in enumerate(new_chunks):\n        if len(text) &gt; 0:  # only add non-empty documents\n            docs.append(\n                Document(\n                    page_content=text,\n                    metadata=metadatas[i] if metadatas else {},\n                )\n            )\n    self.vectorstore.add_documents(docs)\n</code></pre>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory.build_index","title":"<code>build_index(chunks, metadatas=None)</code>","text":"<p>Create a fresh vector store from <code>chunks</code>.  Existing data (if any) are overwritten.</p>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory.build_index--parameters","title":"Parameters","text":"<p>chunks : Sequence[str]     Text snippets (already chunked) to embed. metadatas : Sequence[dict] | None     Optional metadata dict for each chunk, same length as <code>chunks</code>.</p> Source code in <code>src/ursa/util/memory_logger.py</code> <pre><code>def build_index(\n    self,\n    chunks: Sequence[str],\n    metadatas: Optional[Sequence[dict[str, Any]]] = None,\n) -&gt; None:\n    \"\"\"\n    Create a fresh vector store from ``chunks``.  Existing data (if any)\n    are overwritten.\n\n    Parameters\n    ----------\n    chunks : Sequence[str]\n        Text snippets (already chunked) to embed.\n    metadatas : Sequence[dict] | None\n        Optional metadata dict for each chunk, same length as ``chunks``.\n    \"\"\"\n    docs = [\n        Document(\n            page_content=text, metadata=metadatas[i] if metadatas else {}\n        )\n        for i, text in enumerate(chunks)\n    ]\n\n    # Create (or overwrite) the collection\n    self.vectorstore = Chroma.from_documents(\n        documents=docs,\n        embedding=self.embeddings,\n        collection_name=self.collection_name,\n        persist_directory=str(self.path),\n    )\n</code></pre>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory.retrieve","title":"<code>retrieve(query, k=4, with_scores=False, **search_kwargs)</code>","text":"<p>Return the k most similar chunks for <code>query</code>.</p>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory.retrieve--parameters","title":"Parameters","text":"<p>query : str     Natural-language question or statement. k : int     How many results to return. with_scores : bool     If True, also return similarity scores. **search_kwargs     Extra kwargs forwarded to Chroma\u2019s <code>similarity_search*</code> helpers.</p>"},{"location":"api_reference/util/#ursa.util.memory_logger.AgentMemory.retrieve--returns","title":"Returns","text":"<p>list[Document] | list[tuple[Document, float]]</p> Source code in <code>src/ursa/util/memory_logger.py</code> <pre><code>def retrieve(\n    self,\n    query: str,\n    k: int = 4,\n    with_scores: bool = False,\n    **search_kwargs,\n):\n    \"\"\"\n    Return the *k* most similar chunks for `query`.\n\n    Parameters\n    ----------\n    query : str\n        Natural-language question or statement.\n    k : int\n        How many results to return.\n    with_scores : bool\n        If True, also return similarity scores.\n    **search_kwargs\n        Extra kwargs forwarded to Chroma\u2019s ``similarity_search*`` helpers.\n\n    Returns\n    -------\n    list[Document] | list[tuple[Document, float]]\n    \"\"\"\n    if self.vectorstore is None:\n        return [\"None\"]\n\n    if with_scores:\n        return self.vectorstore.similarity_search_with_score(\n            query, k=k, **search_kwargs\n        )\n    return self.vectorstore.similarity_search(query, k=k, **search_kwargs)\n</code></pre>"},{"location":"api_reference/util/#ursa.util.memory_logger.delete_database","title":"<code>delete_database(path=None)</code>","text":"<p>Simple wrapper around a persistent Chroma vector-store for agent-conversation memory.</p>"},{"location":"api_reference/util/#ursa.util.memory_logger.delete_database--parameters","title":"Parameters","text":"<p>path : str | Path | None     Where the on-disk Chroma DB is for deleting.  If None, a folder called     <code>agent_memory_db</code> is created in the package\u2019s base directory.</p> Source code in <code>src/ursa/util/memory_logger.py</code> <pre><code>def delete_database(path: Optional[str | Path] = None):\n    \"\"\"\n    Simple wrapper around a persistent Chroma vector-store for agent-conversation memory.\n\n    Parameters\n    ----------\n    path : str | Path | None\n        Where the on-disk Chroma DB is for deleting.  If *None*, a folder called\n        ``agent_memory_db`` is created in the package\u2019s base directory.\n    \"\"\"\n    db_path = AgentMemory.get_db_path(path)\n    if os.path.exists(db_path):\n        shutil.rmtree(db_path)\n        print(f\"Database: {db_path} has been deleted.\")\n    else:\n        print(\"No database found to delete.\")\n</code></pre>"},{"location":"api_reference/util/#ursa.util.parse","title":"<code>parse</code>","text":""},{"location":"api_reference/util/#ursa.util.parse.extract_json","title":"<code>extract_json(text)</code>","text":"<p>Extract a JSON object or array from text that might contain markdown or other content.</p> The function attempts three strategies <ol> <li>Extract JSON from a markdown code block labeled as JSON.</li> <li>Extract JSON from any markdown code block.</li> <li>Use bracket matching to extract a JSON substring starting with '{' or '['.</li> </ol> <p>Returns:</p> Type Description <code>list[dict]</code> <p>A Python object parsed from the JSON string (dict or list).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no valid JSON is found.</p> Source code in <code>src/ursa/util/parse.py</code> <pre><code>def extract_json(text: str) -&gt; list[dict]:\n    \"\"\"\n    Extract a JSON object or array from text that might contain markdown or other content.\n\n    The function attempts three strategies:\n        1. Extract JSON from a markdown code block labeled as JSON.\n        2. Extract JSON from any markdown code block.\n        3. Use bracket matching to extract a JSON substring starting with '{' or '['.\n\n    Returns:\n        A Python object parsed from the JSON string (dict or list).\n\n    Raises:\n        ValueError: If no valid JSON is found.\n    \"\"\"\n    # Approach 1: Look for a markdown code block specifically labeled as JSON.\n    labeled_block = re.search(\n        r\"```json\\s*([\\[{].*?[\\]}])\\s*```\", text, re.DOTALL\n    )\n    if labeled_block:\n        json_str = labeled_block.group(1).strip()\n        try:\n            return json.loads(json_str)\n        except json.JSONDecodeError:\n            # Fall back to the next approach if parsing fails.\n            pass\n\n    # Approach 2: Look for any code block delimited by triple backticks.\n    generic_block = re.search(r\"```(.*?)```\", text, re.DOTALL)\n    if generic_block:\n        json_str = generic_block.group(1).strip()\n        if json_str.startswith(\"{\") or json_str.startswith(\"[\"):\n            try:\n                return json.loads(json_str)\n            except json.JSONDecodeError:\n                pass\n\n    # Approach 3: Attempt to extract JSON using bracket matching.\n    # Find the first occurrence of either '{' or '['.\n    first_obj = text.find(\"{\")\n    first_arr = text.find(\"[\")\n    if first_obj == -1 and first_arr == -1:\n        raise ValueError(\"No JSON object or array found in the text.\")\n\n    # Determine which bracket comes first.\n    if first_obj == -1:\n        start = first_arr\n        open_bracket = \"[\"\n        close_bracket = \"]\"\n    elif first_arr == -1:\n        start = first_obj\n        open_bracket = \"{\"\n        close_bracket = \"}\"\n    else:\n        if first_obj &lt; first_arr:\n            start = first_obj\n            open_bracket = \"{\"\n            close_bracket = \"}\"\n        else:\n            start = first_arr\n            open_bracket = \"[\"\n            close_bracket = \"]\"\n\n    # Bracket matching: find the matching closing bracket.\n    depth = 0\n    end = None\n    for i in range(start, len(text)):\n        if text[i] == open_bracket:\n            depth += 1\n        elif text[i] == close_bracket:\n            depth -= 1\n            if depth == 0:\n                end = i\n                break\n\n    if end is None:\n        raise ValueError(\n            \"Could not find matching closing bracket for JSON content.\"\n        )\n\n    json_str = text[start : end + 1]\n    try:\n        return json.loads(json_str)\n    except json.JSONDecodeError as e:\n        raise ValueError(\"Extracted content is not valid JSON.\") from e\n</code></pre>"},{"location":"api_reference/util/#ursa.util.parse.extract_main_text_only","title":"<code>extract_main_text_only(html, *, max_chars=250000)</code>","text":"<p>Returns plain text with navigation/ads/scripts removed. Prefers trafilatura -&gt; jusText -&gt; BS4 paragraphs.</p> Source code in <code>src/ursa/util/parse.py</code> <pre><code>def extract_main_text_only(html: str, *, max_chars: int = 250_000) -&gt; str:\n    \"\"\"\n    Returns plain text with navigation/ads/scripts removed.\n    Prefers trafilatura -&gt; jusText -&gt; BS4 paragraphs.\n    \"\"\"\n    # 1) Trafilatura\n    # You can tune config: with_metadata, include_comments, include_images, favor_recall, etc.\n    cfg = trafilatura.settings.use_config()\n    cfg.set(\"DEFAULT\", \"include_comments\", \"false\")\n    cfg.set(\"DEFAULT\", \"include_tables\", \"false\")\n    cfg.set(\"DEFAULT\", \"favor_recall\", \"false\")  # be stricter; less noise\n    try:\n        # If you fetched HTML already, use extract() on string; otherwise, fetch_url(url)\n        txt = trafilatura.extract(\n            html,\n            config=cfg,\n            include_comments=False,\n            include_tables=False,\n            favor_recall=False,\n        )\n        if txt and txt.strip():\n            txt = _normalize_ws(txt)\n            txt = _dedupe_lines(txt)\n            return txt[:max_chars]\n    except Exception:\n        pass\n\n    # 2) jusText\n    try:\n        paragraphs = justext.justext(html, justext.get_stoplist(\"English\"))\n        body_paras = [p.text for p in paragraphs if not p.is_boilerplate]\n        if body_paras:\n            txt = _normalize_ws(\"\\n\\n\".join(body_paras))\n            txt = _dedupe_lines(txt)\n            return txt[:max_chars]\n    except Exception:\n        pass\n\n    # 4) last-resort: BS4 paragraphs/headings only\n    from bs4 import BeautifulSoup\n\n    soup = BeautifulSoup(html, \"html.parser\")\n    for tag in soup([\n        \"script\",\n        \"style\",\n        \"noscript\",\n        \"header\",\n        \"footer\",\n        \"nav\",\n        \"form\",\n        \"aside\",\n    ]):\n        tag.decompose()\n    chunks = []\n    for el in soup.find_all([\"h1\", \"h2\", \"h3\", \"p\", \"li\", \"figcaption\"]):\n        t = el.get_text(\" \", strip=True)\n        if t:\n            chunks.append(t)\n    txt = _normalize_ws(\"\\n\\n\".join(chunks))\n    txt = _dedupe_lines(txt)\n    return txt[:max_chars]\n</code></pre>"},{"location":"api_reference/util/#ursa.util.parse.read_text_file","title":"<code>read_text_file(path)</code>","text":"<p>Reads in a file at a given path into a string</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>string filename, with path, to read in</p> required Source code in <code>src/ursa/util/parse.py</code> <pre><code>def read_text_file(path: str | Path) -&gt; str:\n    \"\"\"\n    Reads in a file at a given path into a string\n\n    Args:\n        path: string filename, with path, to read in\n    \"\"\"\n    with open(path, \"r\", encoding=\"utf-8\") as file:\n        file_contents = file.read()\n    return file_contents\n</code></pre>"},{"location":"api_reference/util/#ursa.util.parse.resolve_pdf_from_osti_record","title":"<code>resolve_pdf_from_osti_record(rec, *, headers=None, unpaywall_email=None, timeout=25)</code>","text":"<p>Returns (pdf_url, landing_used, note)   - pdf_url: direct downloadable PDF URL if found (or a strong candidate)   - landing_used: landing page URL we parsed (if any)   - note: brief trace of how we found it</p> Source code in <code>src/ursa/util/parse.py</code> <pre><code>def resolve_pdf_from_osti_record(\n    rec: dict[str, Any],\n    *,\n    headers: Optional[dict[str, str]] = None,\n    unpaywall_email: Optional[str] = None,\n    timeout: int = 25,\n) -&gt; tuple[Optional[str], Optional[str], str]:\n    \"\"\"\n    Returns (pdf_url, landing_used, note)\n      - pdf_url: direct downloadable PDF URL if found (or a strong candidate)\n      - landing_used: landing page URL we parsed (if any)\n      - note: brief trace of how we found it\n    \"\"\"\n    headers = headers or {\"User-Agent\": \"Mozilla/5.0\"}\n    note_parts: list[str] = []\n\n    links = rec.get(\"links\", []) or []\n    # doi = rec.get(\"doi\")\n\n    # 1) Try 'fulltext' first (OSTI purl)\n    fulltext = None\n    for link in links:\n        if link.get(\"rel\") == \"fulltext\":\n            fulltext = link.get(\"href\")\n            break\n\n    if fulltext:\n        note_parts.append(\"Tried links[fulltext] purl\")\n        try:\n            # Follow redirects; stream to peek headers without loading whole body\n            r = requests.get(\n                fulltext,\n                headers=headers,\n                timeout=timeout,\n                allow_redirects=True,\n                stream=True,\n            )\n            r.raise_for_status()\n\n            if _is_pdf_response(r):\n                note_parts.append(\"fulltext resolved directly to PDF\")\n                return (r.url, None, \" | \".join(note_parts))\n\n            # Not a PDF: parse page HTML for meta or obvious PDF anchors\n            # (If server sent binary but CT lied, _is_pdf_response would have caught via CD or ext)\n            r.close()\n            soup = _get_soup(fulltext, timeout=timeout, headers=headers)\n            candidate = _find_pdf_on_landing(soup, fulltext)\n            if candidate:\n                note_parts.append(\n                    \"found PDF via meta/anchor on fulltext landing\"\n                )\n                return (candidate, fulltext, \" | \".join(note_parts))\n        except Exception as e:\n            note_parts.append(f\"fulltext failed: {e}\")\n\n    # 2) Try DOE PAGES landing (citation_doe_pages)\n    doe_pages = None\n    for link in links:\n        if link.get(\"rel\") == \"citation_doe_pages\":\n            doe_pages = link.get(\"href\")\n            break\n\n    if doe_pages:\n        note_parts.append(\"Tried links[citation_doe_pages] landing\")\n        try:\n            soup = _get_soup(doe_pages, timeout=timeout, headers=headers)\n            candidate = _find_pdf_on_landing(soup, doe_pages)\n            if candidate:\n                # Candidate may itself be a landing\u2014check if it serves PDF\n                try:\n                    r2 = requests.get(\n                        candidate,\n                        headers=headers,\n                        timeout=timeout,\n                        allow_redirects=True,\n                        stream=True,\n                    )\n                    r2.raise_for_status()\n                    if _is_pdf_response(r2):\n                        note_parts.append(\"citation_doe_pages \u2192 direct PDF\")\n                        return (r2.url, doe_pages, \" | \".join(note_parts))\n                    r2.close()\n                except Exception:\n                    pass\n                # If not clearly PDF, still return as a candidate (agent will fetch &amp; parse)\n                note_parts.append(\n                    \"citation_doe_pages \u2192 PDF-like candidate (not confirmed by headers)\"\n                )\n                return (candidate, doe_pages, \" | \".join(note_parts))\n        except Exception as e:\n            note_parts.append(f\"citation_doe_pages failed: {e}\")\n\n    # # 3) Optional: DOI \u2192 Unpaywall OA\n    # if doi and unpaywall_email:\n    #     note_parts.append(\"Tried Unpaywall via DOI\")\n    #     pdf_from_ua = _resolve_pdf_via_unpaywall(doi, unpaywall_email)\n    #     if pdf_from_ua:\n    #         # May be direct PDF or landing; the caller will validate headers during download\n    #         note_parts.append(\"Unpaywall returned candidate\")\n    #         return (pdf_from_ua, None, \" | \".join(note_parts))\n\n    # 4) Give up\n    note_parts.append(\"No PDF found\")\n    return (None, None, \" | \".join(note_parts))\n</code></pre>"},{"location":"api_reference/util/#ursa.util.plan_renderer","title":"<code>plan_renderer</code>","text":""},{"location":"api_reference/util/#ursa.util.plan_renderer.render_plan_steps_rich","title":"<code>render_plan_steps_rich(plan_steps, highlight_index=None)</code>","text":"<p>Pretty table for a list of plan steps (strings or dicts), with an optional highlighted row.</p> Source code in <code>src/ursa/util/plan_renderer.py</code> <pre><code>def render_plan_steps_rich(plan_steps, highlight_index: int | None = None):\n    \"\"\"Pretty table for a list of plan steps (strings or dicts), with an optional highlighted row.\"\"\"\n\n    console = get_console()\n\n    if not plan_steps:\n        return\n\n    table = Table(\n        title=\"Planned Steps\",\n        box=box.ROUNDED,\n        show_lines=False,\n        header_style=\"bold magenta\",\n        expand=True,\n        row_styles=None,  # we'll control per-row styles manually\n    )\n    table.add_column(\"#\", style=\"bold cyan\", no_wrap=True)\n    table.add_column(\"Name\", style=\"bold\", overflow=\"fold\")\n    table.add_column(\"Description\", overflow=\"fold\")\n    table.add_column(\"Outputs\", overflow=\"fold\")\n    table.add_column(\"Criteria\", overflow=\"fold\")\n    table.add_column(\"Code?\", justify=\"center\", no_wrap=True)\n\n    def bullets(items):\n        if not items:\n            return \"\"\n        return \"\\n\".join(f\"\u2022 {x}\" for x in items)\n\n    def code_badge(needs_code: bool):\n        return Text.from_markup(\n            \":hammer_and_wrench: [bold green]Yes[/]\"\n            if needs_code\n            else \"[bold red]No[/]\"\n        )\n\n    for i, step in enumerate(plan_steps, 1):\n        # build cells\n        if isinstance(step, PlanStep):\n            name = step.name\n            desc = step.description\n            outs = bullets(step.expected_outputs)\n            crit = bullets(step.success_criteria)\n            needs_code = bool(step.requires_code)\n        elif isinstance(step, dict):\n            name = step.get(\"name\", \"No Name\")\n            desc = step.get(\"description\", \"No Description\")\n            outs = bullets(step.get(\"expected_outputs\", \"None\"))\n            crit = bullets(step.get(\"success_criteria\", \"None listed\"))\n            needs_code = bool(step.get(\"requires_code\", False))\n        else:\n            name, desc, outs, crit, needs_code = (\n                f\"Step {i}\",\n                str(step),\n                \"\",\n                \"\",\n                False,\n            )\n\n        # style logic\n        row_style = None\n        idx0 = i - 1\n        step_label = str(i)\n\n        if highlight_index is not None:\n            if idx0 &lt; highlight_index:\n                row_style = \"dim\"\n            elif idx0 == highlight_index:\n                row_style = \"bold white on grey50\"  # light gray\n                # row_style = \"bold black on bright_green\"\n                step_label = f\"\u25b6 {i}\"  # pointer on current row\n\n        table.add_row(\n            step_label,\n            str(name),\n            str(desc),\n            outs,\n            crit,\n            code_badge(needs_code),\n            style=row_style,\n        )\n\n    console.print(table)\n</code></pre>"},{"location":"api_reference/util/#ursa.util.types","title":"<code>types</code>","text":""},{"location":"api_reference/util/#ursa.util.types.AsciiStr","title":"<code>AsciiStr = Annotated[str, StringConstraints(strip_whitespace=True, strict=True, pattern='^[\\\\x20-\\\\x7E\\\\t\\\\n\\\\r\\\\f\\\\v]+$')]</code>  <code>module-attribute</code>","text":"<p>Limit strings to \"text\" ASCII characters (letters, digits, symbols, whitespace)</p>"}]}